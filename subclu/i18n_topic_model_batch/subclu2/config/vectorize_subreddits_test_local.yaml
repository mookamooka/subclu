config_description: "Test data loaders & vectorizing classes on subreddit descriptions"
defaults:
  - data_text: v0.5.0_test.yaml

# Paths to cache & upload
local_cache_path: "/home/jupyter/subreddit_clustering_i18n/data/local_cache/"
# Note: this model path is used ONLY if we run model OUTSIDE of hydra
local_model_path: "/home/jupyter/subreddit_clustering_i18n/data/models/embeddings"

output_bucket: 'gazette-models-temp'


# Values for data loader
# GCS_path is the key we'll use from the data_text config
#  get full value hydra call instead of having to write it twice.
# We'll also use this key to write embeddings in a subfolder of text file
#  to make data lineage easier (until we use mlflow).
gcs_path_text_key: folder_subreddits_text_and_meta

data_loader_name: 'LoadSubredditsGCS'
data_loader_kwargs:
  columns:
    - subreddit_id
    - subreddit_name
    - subreddit_name_title_related_subs_and_clean_descriptions
  df_format: 'pandas'
  unique_check: false
  verbose: true

# These sampling values should also get passed to the data loader
n_sample_files: null
n_files_slice_start: null
n_files_slice_end: null
process_individual_files: true

# Values for vectorizing methods/functions
# TODO(djb): how to handle LIST of columns to concat
col_text_for_embeddings: subreddit_name_title_related_subs_and_clean_descriptions

model_name: use_multilingual_3
batch_inference_rows: 1600
limit_first_n_chars: 1900
limit_first_n_chars_retry: 700
get_embeddings_verbose: true
cols_index:  # subreddit_default_
  - subreddit_id
  - subreddit_name



# Change n_jobs(parallel jobs) & logging for hydra itself
hydra:
  # launcher:
    # override the number of jobs for joblib
    # n_jobs: 11
  job_logging:
    formatters:
      simple:
        # format: '`%(asctime)s` | `%(name)s` | `%(levelname)s` | `%(message)s`'
        format: '`%(asctime)s` | `%(levelname)s` | `%(message)s`'
  # Change location of logging - by default it might try to create logs in
  #  home/david.bermejo instead of home/jupyter
  #  https://hydra.cc/docs/configure_hydra/workdir/
  run:
    dir: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: /home/jupyter/subreddit_clustering_i18n/hydra_runs/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
