{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af0500c",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-07-28\n",
    "Let's install `\"tensorflow-text == 2.3.0\"` and see if the GPU is still detected after installing it.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Check whether GPUs are available/usable in current python environment.\n",
    "\n",
    "**UPDATE:** With a fresh VM/notebook, we could see GPUs. This notebook runs the same code AFTER stopping and re-starting the VM. Maybe it's a problem with the way GCP handles the VM and not a problem created by installing new libraries?\n",
    "\n",
    "Provenance:\n",
    "- djb_01.03-test_gpus_available-instance-djb-subclu-inference-tf-2-3-20210630\n",
    "- djb_01.031-test_gpus_available_AFTER_RESTART-instance-djb-subclu-inference-tf-2-3-20210630\n",
    "\n",
    "\n",
    "# Debugging notes\n",
    "## Make sure to set the correct venv/kernel in your notebook\n",
    "The default `python 3` might not have the correct drivers.\n",
    "<br>Instead, might need to manually set it to:\n",
    "<br>`Python [conda env:root]`\n",
    "\n",
    "## Sometimes the best fix is `sudo reboot`\n",
    "When in doubt, open a terminal and do `sudo reboot`.\n",
    "\n",
    "For some reason, the NVIDIA drivers might not be loaded properly after shutting down a VM instance from the GUI:\n",
    "- https://console.cloud.google.com/ai-platform/notebooks/list/instances?project=data-prod-165221\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c976d6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ad049",
   "metadata": {},
   "source": [
    "# Check libraries, BEFORE installing `\"tensorflow-text == 2.3.0\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2210fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9081a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1bb40",
   "metadata": {},
   "source": [
    "# Install `\"tensorflow-text == 2.3.0\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0710872",
   "metadata": {},
   "source": [
    "We can't install `tensorflow-text==2.3.0` on the root environment because there is a numpy conflict *smh*...\n",
    "\n",
    "So we have to add the `--user` flag. Even then we get some errors/warnings about incompatible libraries.\n",
    "```bash\n",
    "!pip install \"tensorflow-text==2.3.0\" --user\n",
    "Installing collected packages: numpy, tensorflow-text\n",
    "  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
    "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "  \n",
    "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "\n",
    "tfx-bsl 0.26.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.10.0 which is incompatible.\n",
    "tfx-bsl 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 4.0.1 which is incompatible.\n",
    "tensorflow-transform 0.26.0 requires pyarrow<0.18,>=0.17, but you have pyarrow 4.0.1 which is incompatible.\n",
    "tensorflow-probability 0.11.0 requires cloudpickle==1.3, but you have cloudpickle 1.6.0 which is incompatible.\n",
    "tensorflow-model-analysis 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 4.0.1 which is incompatible.\n",
    "tensorflow-data-validation 0.26.1 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
    "tensorflow-data-validation 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 4.0.1 which is incompatible.\n",
    "apache-beam 2.28.0 requires httplib2<0.18.0,>=0.8, but you have httplib2 0.19.1 which is incompatible.\n",
    "apache-beam 2.28.0 requires pyarrow<3.0.0,>=0.15.1, but you have pyarrow 4.0.1 which is incompatible.\n",
    "apache-beam 2.28.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 3.10.0.0 which is incompatible.\n",
    "\n",
    "Successfully installed numpy-1.18.5 tensorflow-text-2.3.0\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Error w/o the --user flag:\n",
    "```bash\n",
    "!pip install \"tensorflow-text==2.3.0\"\n",
    "...\n",
    "Installing collected packages: numpy, tensorflow-text\n",
    "  Attempting uninstall: numpy\n",
    "    Found existing installation: numpy 1.19.5\n",
    "    Uninstalling numpy-1.19.5:\n",
    "      Successfully uninstalled numpy-1.19.5\n",
    "  Rolling back uninstall of numpy\n",
    "\n",
    "...\n",
    "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/opt/conda/lib/python3.7/site-packages/numpy-1.18.5.dist-info/LICENSE.txt'\n",
    "Consider using the `--user` option or check the permissions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01dacddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow-text==2.3.0\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac8f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea68635",
   "metadata": {},
   "source": [
    "# Imports & notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28acfe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd94b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\tv: 2.3.3\n",
      "tensorflow_text\tv: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# import subclu\n",
    "# from subclu.utils.eda import (\n",
    "#     setup_logging, notebook_display_config, print_lib_versions,\n",
    "# )\n",
    "\n",
    "for lib_ in [tf, tensorflow_text]:\n",
    "    sep_ = '\\t' if len(lib_.__name__) > 7 else '\\t\\t'\n",
    "    print(f\"{lib_.__name__}{sep_}v: {get_distribution(f'{lib_.__name__}').version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0907011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27b985",
   "metadata": {},
   "source": [
    "# Check GPUs/XLA_GPUs recognized by Tensorflow/python\n",
    "\n",
    "NOTE: `GPU`s and `XLA_GPU`s are recognized as two different device types.\n",
    "\n",
    "https://www.tensorflow.org/xla\n",
    "> **XLA: Optimizing Compiler for Machine Learning**\n",
    "> XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n",
    "> \n",
    "> The results are improvements in speed and memory usage: e.g. in BERT MLPerf submission using 8 Volta V100 GPUs using XLA has achieved a ~7x performance improvement and ~5x batch size improvement\n",
    "\n",
    "Other sources\n",
    "- https://stackoverflow.com/questions/52943489/what-is-xla-gpu-and-xla-cpu-for-tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c96f01",
   "metadata": {},
   "source": [
    "## What device gets used for calculations?\n",
    "\n",
    "It should be `GPU` or `XLA_GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "353f0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "CPU times: user 657 ms, sys: 383 ms, total: 1.04 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f82dd",
   "metadata": {},
   "source": [
    "## List devices\n",
    "\n",
    "Expected GPU output\n",
    "```\n",
    "Built with CUDA? True\n",
    "\n",
    "GPUs\n",
    "===\n",
    "Num GPUs Available: 2\n",
    "GPU details:\n",
    "[   PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
    "    PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n",
    "\n",
    "Built with CUDA? True\n",
    "\n",
    "All devices:\n",
    "===\n",
    "Num devices: 4\n",
    "Details:\n",
    "[   name: \"/device:CPU:0\"\n",
    "device_type: \"CPU\"\n",
    "\n",
    "...\n",
    "\n",
    ",\n",
    "    name: \"/device:XLA_GPU:0\"\n",
    "device_type: \"XLA_GPU\"\n",
    "memory_limit: 17179869184\n",
    "locality {\n",
    "}\n",
    "incarnation: 6215884038941287466\n",
    "physical_device_desc: \"device: XLA_GPU device\"\n",
    ",\n",
    "    name: \"/device:GPU:0\"\n",
    "device_type: \"GPU\"\n",
    "memory_limit: 14676252416\n",
    "locality {\n",
    "  bus_id: 1\n",
    "  links {\n",
    "  }\n",
    "}\n",
    "incarnation: 8485125904456880156\n",
    "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c35c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 2\n",
      "GPU details:\n",
      "[   PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
      "    PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = (\n",
    "    tf.config.list_physical_devices('GPU') +\n",
    "    tf.config.list_physical_devices('XLA_GPU')\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\n\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\"\n",
    ")\n",
    "pprint(l_phys_gpus, indent=4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "386a30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "\n",
      "All devices:\n",
      "===\n",
      "Num devices: 4\n",
      "Details:\n",
      "[   name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2031236102443331441\n",
      ",\n",
      "    name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4577649183693087916\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ",\n",
      "    name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12427452598662418691\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ",\n",
      "    name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 303824896\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8820588589459663090\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "l_all_local_devices = device_lib.list_local_devices()\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\n\\nAll devices:\\n===\"\n",
    "    f\"\\nNum devices: {len(l_all_local_devices)}\"\n",
    "    f\"\\nDetails:\"\n",
    ")\n",
    "pprint(l_all_local_devices, indent=4,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ce12a",
   "metadata": {},
   "source": [
    "# Check NVIDIA CLI\n",
    "\n",
    "First, do we even see the GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d05c0c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:04.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\n"
     ]
    }
   ],
   "source": [
    "!lspci | grep 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495389b",
   "metadata": {},
   "source": [
    "Then, are they recognized by the nvidia-smi tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f80a0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 29 04:20:20 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   75C    P0    33W /  70W |  15045MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     10380      C   /opt/conda/bin/python           14375MiB |\n",
      "|    0   N/A  N/A     10991      C   /opt/conda/bin/python             667MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf223d",
   "metadata": {},
   "source": [
    "## Debug nvidia drivers\n",
    "\n",
    "If `nvidia-smi` doesn't detect the drivers, we might need to reinstall them.\n",
    "\n",
    "- https://towardsdatascience.com/troubleshooting-gcp-cuda-nvidia-docker-and-keeping-it-running-d5c8b34b6a4c\n",
    "\n",
    "Getting nothing from `cuda` is bad... sigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd7001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libnvidia-container-tools             1.4.0-1                       amd64        NVIDIA container runtime library (command-line tools)\n",
      "ii  libnvidia-container1:amd64            1.4.0-1                       amd64        NVIDIA container runtime library\n",
      "ii  nvidia-container-runtime              3.5.0-1                       amd64        NVIDIA container runtime\n",
      "ii  nvidia-container-toolkit              1.5.1-1                       amd64        NVIDIA container runtime hook\n",
      "ii  nvidia-docker2                        2.6.0-1                       all          nvidia-docker CLI wrapper\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f7102",
   "metadata": {},
   "source": [
    "## CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "928eef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a400ec",
   "metadata": {},
   "source": [
    "### Check `cudnn` & `cudatoolkit` versions in conda\n",
    "\n",
    "This doens't work because we don't have the right permissions. We might not need it, though, because by default the VMs created by Google don't have these drivers installed via conda either. \n",
    "\n",
    "Unclear what's missing or what changes that breaks the driver detection when installing the requirements from my project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "314c74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda search cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7513fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda search cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91ed1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this call works on my laptop, but fails on VM because of permissions\n",
    "# !conda search cudnn --platform linux-64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2132a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
