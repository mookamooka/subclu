{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b82332",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "**2022-08-15: v0.6.0**\n",
    "<br>Test `dask.delayed` to run aggregation of multiple subreddits in parallel.\n",
    "With the new project we expect to aggregate posts for over 300k subreddits. For most of the process, each subreddit can be processed independently of other subreddits, so it makes sense to try to split up the work so we can speed things up.\n",
    "\n",
    "**2022-06-29: v0.5.0**\n",
    "<br>Because we embedded post & text as a single embedding and we didn't use MLflow to create those embeddings, it's easier to  run the embeddings in this notebook rather than to re-use or re-write the old `AggregateEmbeddings` class.\n",
    "\n",
    "Provenance:\n",
    "* `v0.4.1 / djb_03.01-2021-12-aggregate_v041_posts_and_comments_pandas.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6df85",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7127464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c677c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from logging import info\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "from subclu.models import aggregate_embeddings_pd\n",
    "\n",
    "from subclu.utils import set_working_directory, get_project_subfolder\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric,\n",
    "    elapsed_time,\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import LoadSubredditsGCS\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d877641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04636e5",
   "metadata": {},
   "source": [
    "# Set Local model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe61c379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/manual_v060_2022-08-16_084151')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_model_timestamp = datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')\n",
    "path_this_model = get_project_subfolder(\n",
    "    f\"data/models/aggregate_embeddings/manual_v060_{manual_model_timestamp}\"\n",
    ")\n",
    "Path.mkdir(path_this_model, parents=True, exist_ok=True)\n",
    "path_this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94959f6d",
   "metadata": {},
   "source": [
    "# Load config for embeddings aggregation\n",
    "\n",
    "For v0.6.0 embeddings I didn't use mlflow to track the embeddings inference. We'll need to get them from these folders in GCS:\n",
    "\n",
    "- [Subreddit metadata](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555`\n",
    "- [Post + Comment Text (already combined)](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13073cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data_text_and_metadata', 'data_embeddings_to_aggregate', 'aggregate_params', 'bucket_output', 'mlflow_tracking_uri', 'mlflow_experiment', 'n_sample_subreddits', 'n_sample_posts_files', 'n_sample_comments_files', 'agg_style'])\n"
     ]
    }
   ],
   "source": [
    "cfg_agg_embeddings = LoadHydraConfig(\n",
    "    config_name='aggregate_embeddings_v0.6.0.yaml',\n",
    "    config_path=\"../config\",\n",
    "    overrides=[\n",
    "        f\"agg_style=dask_delayed\",\n",
    "    ],\n",
    ")\n",
    "print(cfg_agg_embeddings.config_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe32d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb01cd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_text_and_metadata:\n",
      "    dataset_name: v0.6.0 inputs. ~110k seed subreddits, ~340k with 3+ posts, ~700k total subreddits\n",
      "    bucket_name: i18n-subreddit-clustering\n",
      "    folder_subreddits_text_and_meta: i18n_topic_model_batch/runs/20220811/subreddits/text\n",
      "    folder_posts_text_and_meta: i18n_topic_model_batch/runs/20220811/posts\n",
      "    folder_comments_text_and_meta: i18n_topic_model_batch/runs/20220811/comments\n",
      "    folder_post_and_comment_text_and_meta: i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all\n",
      "data_embeddings_to_aggregate:\n",
      "    bucket_embeddings: i18n-subreddit-clustering\n",
      "    post_and_comments_folder_embeddings: i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218\n",
      "    subreddit_desc_folder_embeddings: i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859\n",
      "    col_subreddit_id: subreddit_id\n",
      "aggregate_params:\n",
      "    min_post_and_comment_text_len: 3\n",
      "    agg_post_post_and_comment_weight: 85\n",
      "    agg_post_subreddit_desc_weight: 15\n",
      "bucket_output: i18n-subreddit-clustering\n",
      "mlflow_tracking_uri: sqlite\n",
      "mlflow_experiment: v0.6.0_mUSE_aggregates\n",
      "n_sample_subreddits: None\n",
      "n_sample_posts_files: None\n",
      "n_sample_comments_files: None\n",
      "agg_style: dask_delayed\n"
     ]
    }
   ],
   "source": [
    "for k_, v_ in cfg_agg_embeddings.config_dict.items():\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"{k_}:\")\n",
    "        for k2_, v2_ in v_.items():\n",
    "            print(f\"    {k2_}: {v2_}\")\n",
    "    else:\n",
    "        print(f\"{k_}: {v_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c53e80",
   "metadata": {},
   "source": [
    "# Start MLflow & Log base params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9218af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MlflowLogger(tracking_uri=cfg_agg_embeddings.config_dict['mlflow_tracking_uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37150c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:41:52 | INFO | \"== Start run_aggregation() method ==\"\n",
      "08:41:52 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "08:41:53 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "08:41:53 | INFO | \"cpu_count: 96\"\n",
      "08:41:53 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '5.98%', 'memory_total': '1,444,961', 'memory_used': '86,467', 'memory_free': '1,197,814'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_total': 1444961,\n",
       " 'memory_used_percent': 0.05984036939405285,\n",
       " 'memory_used': 86467,\n",
       " 'memory_free': 1197814}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_experiment = cfg_agg_embeddings.config_dict['mlflow_experiment']\n",
    "# 'v0.6.0_mUSE_aggregates', 'v0.6.0_mUSE_aggregates_test'\n",
    "\n",
    "\n",
    "t_start_agg_embed = datetime.utcnow()\n",
    "info(f\"== Start run_aggregation() method ==\")\n",
    "\n",
    "\n",
    "info(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "mlf.set_experiment(mlflow_experiment)\n",
    "mlflow.start_run()\n",
    "mlf.add_git_hash_to_active_run()\n",
    "mlf.set_tag_hostname(key='host_name')\n",
    "mlf.log_param_hostname(key='host_name')\n",
    "mlf.log_cpu_count()\n",
    "mlf.log_ram_stats(param=True, only_memory_used=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3fee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859\n",
      "i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218\n"
     ]
    }
   ],
   "source": [
    "# set weights\n",
    "# Normalize them by dividing by 100\n",
    "WEIGHT_POST_COMMENT = (\n",
    "    cfg_agg_embeddings.config_dict['aggregate_params']['agg_post_post_and_comment_weight'] / 100\n",
    ")\n",
    "WEIGHT_SUB_META = (\n",
    "    cfg_agg_embeddings.config_dict['aggregate_params']['agg_post_subreddit_desc_weight'] / 100\n",
    ")\n",
    "print(WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "assert(1.0 == WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "\n",
    "\n",
    "gcs_sub_embeddings = cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['subreddit_desc_folder_embeddings']\n",
    "print(gcs_sub_embeddings)\n",
    "gcs_post_comment_embeddings = cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['post_and_comments_folder_embeddings']\n",
    "print(gcs_post_comment_embeddings)\n",
    "\n",
    "mlflow.log_params(\n",
    "    {\n",
    "        'embeddings_bucket': cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "        'embeddings_subreddit_path': gcs_sub_embeddings,\n",
    "        'embeddings_post_and_comments_path': gcs_post_comment_embeddings,\n",
    "        'weight_post_and_comments': WEIGHT_POST_COMMENT,\n",
    "        'weight_subreddit_meta': WEIGHT_SUB_META,\n",
    "    }\n",
    ")\n",
    "for k_, v_ in cfg_agg_embeddings.config_dict.items():\n",
    "    if isinstance(v_, str):\n",
    "        try:\n",
    "            mlflow.log_param(k_, v_)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048e289",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cdecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:41:55 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859\"\n",
      "08:41:55 | INFO | \"  7 <- Files matching prefix\"\n",
      "08:41:55 | INFO | \"  7 <- Files to check\"\n",
      "08:41:55 | INFO | \"    000000000000-131971_by_514.parquet <- File already exists, not downloading\"\n",
      "08:41:55 | INFO | \"    000000000001-198630_by_514.parquet <- File already exists, not downloading\"\n",
      "08:41:55 | INFO | \"    000000000002-441159_by_514.parquet <- File already exists, not downloading\"\n",
      "08:41:55 | INFO | \"    2022-08-11_08-28-59_vectorize_text.log <- File already exists, not downloading\"\n",
      "08:41:55 | INFO | \"  Files already cached: 4\"\n",
      "08:41:55 | INFO | \"  Files already downloaded.\"\n",
      "08:41:55 | INFO | \"  df format: pandas\"\n",
      "08:42:00 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771,760 rows, 514 cols\n",
      "CPU times: user 5.38 s, sys: 5.19 s, total: 10.6 s\n",
      "Wall time: 7.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_start_data_load_ = datetime.utcnow()\n",
    "\n",
    "subs_v = LoadSubredditsGCS(\n",
    "    bucket_name=cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "    gcs_path=gcs_sub_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    ")\n",
    "subs_v.local_cache()\n",
    "\n",
    "df_v_subs = subs_v.read_as_one_df()\n",
    "r_subs, c_subs = df_v_subs.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_subs-rows\": r_subs,\n",
    "        f\"df_v_subs-cols\": c_subs,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_subs:,.0f} rows, {c_subs:,.0f} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91feaa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -m cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218 /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding\n"
     ]
    }
   ],
   "source": [
    "# gsutil is usually faster than the python library.\n",
    "remote_bucket_and_key = 'i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218'\n",
    "remote_gs_path = f'gs://{remote_bucket_and_key}'\n",
    "\n",
    "# Need to remove the last part of the local path otherwise we'll get duplicate subfolders:\n",
    "#. top/2021-12-14/2021-12-14 instead of top/2021-12-14\n",
    "local_f = f\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/{'/'.join(remote_bucket_and_key.split('/')[:-1])}\"\n",
    "Path(local_f).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(f\"Remote path:\\n  {remote_gs_path}\")\n",
    "# print(f\"Local path:\\n  {local_f}\")\n",
    "\n",
    "print(\n",
    "    f\"gsutil -m cp -r -n {remote_gs_path} {local_f}\"\n",
    ")\n",
    "# gsutil -m cp -r -n $remote_gs_path $local_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20878597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:42:03 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218\"\n",
      "08:42:03 | INFO | \"  197 <- Files matching prefix\"\n",
      "08:42:03 | INFO | \"  197 <- Files to check\"\n",
      "08:42:03 | INFO | \"    000000000000-264431_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000001-249532_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000002-308094_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000003-331082_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000004-356401_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000005-331679_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000006-253861_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000007-365498_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000008-271629_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000009-355342_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000010-356760_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000011-370490_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000012-357532_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000013-233247_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000014-134434_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000015-250143_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000016-136611_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000017-181862_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000018-140950_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000019-169025_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000020-153909_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000021-121734_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000022-149810_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000023-193707_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000024-157257_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000025-153696_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000026-178417_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000027-158698_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000028-166516_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000029-134767_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000030-167537_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000031-166943_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000032-148950_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000033-214139_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000034-145226_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000035-174913_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000036-129006_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000037-183050_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000038-228930_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000039-169420_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000040-190340_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000041-209568_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000042-166077_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000043-157511_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000044-176937_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000045-192161_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000046-246921_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000047-173822_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000048-185120_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000049-181449_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000050-189495_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000051-141399_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000052-214970_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000053-209653_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000054-227487_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000055-188764_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000056-166519_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000057-204426_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000058-135687_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000059-173364_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000060-171807_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000061-202668_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000062-196125_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000063-165010_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000064-190401_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000065-185782_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000066-158051_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000067-178195_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000068-232743_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000069-164717_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000070-187535_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000071-197794_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000072-188607_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000073-210410_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000074-230000_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000075-222524_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000076-166300_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000077-266372_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000078-209463_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000079-172924_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000080-186829_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000081-225345_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000082-229456_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000083-197983_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000084-219726_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000085-316505_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000086-172618_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000087-250852_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000088-201434_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000089-261433_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000090-223554_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000091-217039_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000092-302573_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000093-223954_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000094-227185_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000095-199144_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000096-176973_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000097-165848_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000098-267512_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000099-226861_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000100-206414_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000101-213130_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000102-247275_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000103-188984_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000104-283839_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000105-217584_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000106-267602_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000107-278886_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000108-265243_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000109-146745_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000110-330246_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000111-228777_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000112-281636_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000113-241766_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000114-245857_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000115-281145_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000116-230592_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000117-258417_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000118-337749_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000119-240860_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000120-271515_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000121-166452_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000122-294465_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000123-238877_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000124-299063_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000125-282082_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000126-205459_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000127-257803_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000128-229708_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000129-248835_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000130-279925_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000131-226903_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000132-315432_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000133-237756_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000134-310613_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000135-329852_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000136-236955_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000137-282993_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000138-258136_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000139-227795_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000140-420758_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000141-420500_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000142-356366_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000143-303307_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000144-227848_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000145-307717_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000146-307629_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000147-243582_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000148-301334_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000149-254682_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000150-301055_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000151-315099_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000152-360964_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000153-354388_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000154-380301_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000155-436827_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000156-334947_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000157-396313_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000158-444881_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000159-451416_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000160-462155_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000161-473842_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000162-407864_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000163-557506_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000164-410085_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000165-388272_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000166-372157_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000167-548366_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000168-423502_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000169-452732_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000170-522805_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000171-497492_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000172-424836_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000173-492358_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000174-693704_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000175-695091_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000176-455451_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000177-491107_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000178-584946_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000179-546528_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000180-411298_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000181-371464_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000182-275564_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000183-195271_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000184-285922_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000185-225947_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000186-360629_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000187-426063_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000188-357355_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000189-235049_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000190-318299_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000191-263350_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    000000000192-328908_by_515.parquet <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"    2022-08-11_08-42-18_vectorize_text.log <- File already exists, not downloading\"\n",
      "08:42:03 | INFO | \"  Files already cached: 194\"\n",
      "08:42:03 | INFO | \"  Files already downloaded.\"\n",
      "08:42:03 | INFO | \"  df format: pandas\"\n",
      "08:45:35 | INFO | \"  0:03:41.929324 <- Data Loading Time time elapsed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,906,348 rows, 515 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:45:38 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '29.48%', 'memory_used': '425,923'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 43s, sys: 18min 49s, total: 25min 32s\n",
      "Wall time: 3min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.2947643569618834, 'memory_used': 425923}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pc_v = LoadSubredditsGCS(\n",
    "    bucket_name=cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "    gcs_path=gcs_post_comment_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='post_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=cfg_agg_embeddings.config_dict['n_sample_posts_files'],  # None,\n",
    "    n_files_slice_start=None,  # None,\n",
    "    n_files_slice_end=None,  # None, \n",
    ")\n",
    "pc_v.local_cache()\n",
    "\n",
    "df_v_pc = pc_v.read_as_one_df()\n",
    "r_pc, c_pc = df_v_pc.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_post_comments-rows\": r_pc,\n",
    "        f\"df_v_post_comments-cols\": c_pc,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_pc:,.0f} rows, {c_pc:,.0f} cols\")\n",
    "\n",
    "t_data_load = elapsed_time(start_time=t_start_data_load_, log_label='Data Loading Time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-data_loading_time',\n",
    "                  t_data_load / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab857bd",
   "metadata": {},
   "source": [
    "# Set weights & create copy dfs for new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da65e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "l_ix_sub_level = ['subreddit_id', 'subreddit_name']\n",
    "l_ix_post_level = l_ix_sub_level + ['post_id']\n",
    "\n",
    "l_embedding_cols = [c for c in df_v_pc if c.startswith('embeddings_')]\n",
    "print(len(l_embedding_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "997cc121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:46:27 | INFO | \"Initializing weighted SUBS meta\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:46:30 | INFO | \"Initializing weighted POSTS embeddings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "CPU times: user 1min 55s, sys: 2min 6s, total: 4min 2s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_v_pc_weighted = df_v_pc.copy()\n",
    "\n",
    "df_v_subs_weighted = df_v_subs.copy()\n",
    "\n",
    "# should be True b/c they're copies\n",
    "print(np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515]))\n",
    "print(np.allclose(df_v_subs_weighted.iloc[:1000,2:515], df_v_subs.iloc[:1000,2:515]))\n",
    "\n",
    "# apply weight to all posts & subreddit meta at once (vectorized)\n",
    "info(f\"Initializing weighted SUBS meta\")\n",
    "df_v_subs_weighted[l_embedding_cols] = df_v_subs_weighted[l_embedding_cols] * WEIGHT_SUB_META\n",
    "\n",
    "info(f\"Initializing weighted POSTS embeddings\")\n",
    "df_v_pc_weighted[l_embedding_cols] = df_v_pc_weighted[l_embedding_cols] * WEIGHT_POST_COMMENT\n",
    "\n",
    "# NOW they shouldn't be equal (Should be False)\n",
    "print(np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515]))\n",
    "print(np.allclose(df_v_subs_weighted.iloc[:1000,2:515], df_v_subs.iloc[:1000,2:515]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f699dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_describe(df_v_pc[l_ix_post_level])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b1c9b",
   "metadata": {},
   "source": [
    "# Aggregate to Post-Level: Post&Comments + Subreddit Meta\n",
    "\n",
    "It's better to let pandas handle the interations with `.groupby('subreddit_id')`. Otherwise we have to create masks for each subreddit that can take much longer (17+ hours).\n",
    "\n",
    "- ETA with masks: +17.6 hours\n",
    "- ETA with groupby ~2.5 hours\n",
    "\n",
    "```\n",
    "# mask:\n",
    "0%  329/81973 [04:18<17:42:36, 1.28it/s]\n",
    "\n",
    "# .groupby()\n",
    "6% 4751/81973 [09:56<2:35:06, 8.30it/s]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Updates using `dask.delayed`:\n",
    "By combining .groupby() + `dask.delayed` we can process things ~3x faster:\n",
    "\n",
    "```\n",
    "# .groupby() + dask.delayed(....to_numpy())\n",
    "5%  34121/705963 [26:36<8:15:14, 22.61it/s]\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "# .groupby(), no dask delayed\n",
    "100% 3467/3467 [08:20<00:00, 6.97it/s]\n",
    "  0:08:21.661816 <- Total Agg fxn time time elapsed\n",
    "\n",
    "\n",
    "# .groupby() + dask.delayed(....to_numpy())\n",
    "100% 3467/3467 [02:31<00:00, 23.08it/s]\n",
    "  Wall time: 2min 38s\n",
    "\n",
    "\n",
    "# masks with dask.delayed():\n",
    "#  This is 2x faster than serial processing, but .groupby() + dask.delayed() is much faster\n",
    "100% 3467/3467 [00:11<00:00, 299.85it/s]\n",
    "05:44:20 | INFO | \"Define new C1 df DAG in dask\"\n",
    "05:44:20 | INFO | \"COMPUTE new C1 df START\"\n",
    "05:48:20 | INFO | \"COMPUTE new C1 df DONE\"\n",
    "05:48:20 | INFO | \"  0:04:11.393036 <- Total Agg fxn time time elapsed\"\n",
    "CPU times: user 4min 33s, sys: 24.3 s, total: 4min 57s\n",
    "Wall time: 4min 12s\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9a53d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:49:40 | INFO | \"Start C1 - posts + comments + sub descriptions with format: `dask_delayed`\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2346b5f8cb914970a21cd636b7f2dae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:29:25 | INFO | \"Define new posts C1 df DAG in dask\"\n",
      "17:29:38 | INFO | \"COMPUTE new C1 df START\"\n",
      "17:53:58 | INFO | \"COMPUTE new C1 df DONE\"\n",
      "17:53:58 | INFO | \"  9:04:17.319290 <- Total Agg fxn time time elapsed\"\n",
      "17:53:58 | INFO | \"C1 - post level complete\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,906,348 rows, 515 cols\n",
      "CPU times: user 8h 57min 19s, sys: 11min 58s, total: 9h 9min 17s\n",
      "Wall time: 9h 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set style so that we can try output & time in either format\n",
    "AGG_STYLE = cfg_agg_embeddings.config_dict['agg_style']  # serial v. dask.delayed\n",
    "\n",
    "info(f\"Start C1 - posts + comments + sub descriptions with format: `{AGG_STYLE}`\")\n",
    "t_start_agg_post_c1 = datetime.utcnow()\n",
    "\n",
    "l_df_c1_weights = list()\n",
    "\n",
    "if AGG_STYLE == 'serial':\n",
    "    for s_id, df_ in tqdm(\n",
    "        df_v_pc_weighted.groupby('subreddit_id'),\n",
    "        ascii=True, mininterval=5,\n",
    "    ):\n",
    "        df_.loc[:, l_embedding_cols] = np.add(\n",
    "            df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "            df_[l_embedding_cols]\n",
    "        )\n",
    "        l_df_c1_weights.append(df_)\n",
    "        del df_\n",
    "\n",
    "    info(f\"Create new C1 df\")\n",
    "    df_posts_agg_c1 = pd.concat(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "elif AGG_STYLE == 'dask_delayed':\n",
    "    for s_id, df_ in tqdm(\n",
    "        df_v_pc_weighted.groupby('subreddit_id'),\n",
    "        ascii=True, mininterval=5,\n",
    "    ):\n",
    "        df_pc_embeddings_ = dask.delayed(np.add)(\n",
    "            # df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "            dask.delayed(df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy()),\n",
    "            dask.delayed(df_[l_embedding_cols])\n",
    "        )\n",
    "        l_df_c1_weights.append(\n",
    "            dask.delayed(pd.concat)([dask.delayed(df_[l_ix_post_level]), df_pc_embeddings_], ignore_index=False, axis=1)\n",
    "        )\n",
    "\n",
    "    info(f\"Define new posts C1 df DAG in dask\")\n",
    "    df_posts_agg_c1_delayed = dask.delayed(pd.concat)(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "    info(f\"COMPUTE new C1 df START\")\n",
    "    df_posts_agg_c1 = df_posts_agg_c1_delayed.compute()\n",
    "    info(f\"COMPUTE new C1 df DONE\")\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(f'Other agg style not implemented: {AGG_STYLE}')\n",
    "\n",
    "    \n",
    "r_, c_ = df_posts_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_posts_agg_c1-rows\": r_,\n",
    "        f\"df_posts_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_:,.0f} rows, {c_:,.0f} cols\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_pc_c1 = elapsed_time(start_time=t_start_agg_post_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_posts_agg_c1_no_delay',\n",
    "                  t_agg_pc_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "info(f\"C1 - post level complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9f4484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51906348 entries, 0 to 51906347\n",
      "Columns: 515 entries, subreddit_id to embeddings_511\n",
      "dtypes: float32(512), object(3)\n",
      "memory usage: 100.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df_posts_agg_c1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8d8610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>post_id</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_1001tl</td>\n",
       "      <td>jewel_xo</td>\n",
       "      <td>t3_w6lnkt</td>\n",
       "      <td>-0.011266</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.035281</td>\n",
       "      <td>0.040452</td>\n",
       "      <td>-0.065908</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>-0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_10029e</td>\n",
       "      <td>milkyhentai</td>\n",
       "      <td>t3_wacyh8</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>0.038307</td>\n",
       "      <td>0.045457</td>\n",
       "      <td>-0.027101</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>0.040519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_1006k8</td>\n",
       "      <td>badwouldyourather</td>\n",
       "      <td>t3_v9i9a9</td>\n",
       "      <td>-0.008159</td>\n",
       "      <td>0.035251</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>0.036374</td>\n",
       "      <td>0.046088</td>\n",
       "      <td>0.031007</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5_100806</td>\n",
       "      <td>jojojosiah</td>\n",
       "      <td>t3_v49gii</td>\n",
       "      <td>0.029087</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.031094</td>\n",
       "      <td>-0.019099</td>\n",
       "      <td>-0.041052</td>\n",
       "      <td>0.058712</td>\n",
       "      <td>-0.029871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5_100806</td>\n",
       "      <td>jojojosiah</td>\n",
       "      <td>t3_v49tw9</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>0.040265</td>\n",
       "      <td>0.045831</td>\n",
       "      <td>0.049636</td>\n",
       "      <td>0.060570</td>\n",
       "      <td>-0.014866</td>\n",
       "      <td>0.047486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id     subreddit_name    post_id  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6\n",
       "0    t5_1001tl           jewel_xo  t3_w6lnkt     -0.011266      0.001246      0.035281      0.040452     -0.065908      0.009004     -0.000900\n",
       "1    t5_10029e        milkyhentai  t3_wacyh8     -0.039492      0.007736      0.038307      0.045457     -0.027101      0.033553      0.040519\n",
       "2    t5_1006k8  badwouldyourather  t3_v9i9a9     -0.008159      0.035251     -0.000912      0.036374      0.046088      0.031007      0.003537\n",
       "3    t5_100806         jojojosiah  t3_v49gii      0.029087      0.004141      0.031094     -0.019099     -0.041052      0.058712     -0.029871\n",
       "4    t5_100806         jojojosiah  t3_v49tw9     -0.035303      0.040265      0.045831      0.049636      0.060570     -0.014866      0.047486"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts_agg_c1.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24053688",
   "metadata": {},
   "source": [
    "### Save post-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c8583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dfs_to_save = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4364cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:59:30 | INFO | \"Converting pandas to dask...\"\n",
      "20:59:41 | INFO | \"  111,275.8 MB <- Memory usage\"\n",
      "20:59:41 | INFO | \"     203\t<- target Dask partitions\t  550.0 <- target MB partition size\"\n",
      "21:10:14 | INFO | \"  Logging df to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 43min 7s, sys: 22min 41s, total: 2h 5min 48s\n",
      "Wall time: 33min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_posts_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_posts_agg_c1_{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_posts_agg_c1,\n",
    "    d_dfs_to_save['df_posts_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_posts_agg_c1']['local'], artifact_path='df_posts_agg_c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d8167",
   "metadata": {},
   "source": [
    "# Aggregate to Subreddit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "125ec301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:36:17 | INFO | \"Count posts per subreddit...\"\n",
      "21:37:40 | INFO | \"SUBREDDIT-LEVEL C1 - posts + comments + sub descriptions\"\n",
      "21:37:40 | INFO | \"Mean for subs above threshold: 3\"\n",
      "21:53:11 | INFO | \"Calculating mean for subs BELOW post threshold...\"\n",
      "21:53:27 | INFO | \"Combining all subreddits...\"\n",
      "21:53:35 | INFO | \"  0:15:55.600336 <- Total Agg fxn time time elapsed\"\n",
      "21:53:35 | INFO | \"  <- df_subs_agg_c1.shape (posts + comments + sub description)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771,760 rows, 515 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:53:43 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '59.21%', 'memory_used': '855,520'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 27s, sys: 10min 54s, total: 17min 22s\n",
      "Wall time: 17min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.5920713431019937, 'memory_used': 855520}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# first, figure out how many posts each subreddit has\n",
    "info(f\"Count posts per subreddit...\")\n",
    "c_post_embedding_count = 'posts_for_embeddings_count'\n",
    "\n",
    "\n",
    "df_posts_for_embedding_count = (\n",
    "    df_posts_agg_c1\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .agg(**{c_post_embedding_count: ('post_id', 'nunique')})\n",
    ")\n",
    "# fill subs that have no posts\n",
    "df_posts_for_embedding_count = pd.concat(\n",
    "    [\n",
    "        df_posts_for_embedding_count, \n",
    "        df_v_subs[\n",
    "            ~df_v_subs['subreddit_id'].isin(df_posts_agg_c1['subreddit_id'])\n",
    "        ][l_ix_sub_level].assign(**{c_post_embedding_count: 0})\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "mlf.log_ram_stats(only_memory_used=True)\n",
    "\n",
    "# min_posts >= -> regular mean. If it's less than this, then mix in subreddit_description into average\n",
    "n_min_posts_for_regular_mean = 3\n",
    "subreddits_above_n_ = (\n",
    "    df_posts_for_embedding_count\n",
    "    [df_posts_for_embedding_count[c_post_embedding_count] >= n_min_posts_for_regular_mean]\n",
    "    ['subreddit_id']\n",
    ")\n",
    "subreddits_below_n_ = set(df_v_subs['subreddit_id']) - set(subreddits_above_n_)\n",
    "mask_min_posts_for_reg_mean = df_posts_agg_c1['subreddit_id'].isin(subreddits_above_n_)\n",
    "\n",
    "\n",
    "info(f\"SUBREDDIT-LEVEL C1 - posts + comments + sub descriptions\")\n",
    "t_start_agg_subs_c1 = datetime.utcnow()\n",
    "\n",
    "# 3+ posts: simple mean()\n",
    "info(f\"Mean for subs above threshold: {n_min_posts_for_regular_mean}\")\n",
    "df_subs_agg_c1_Nplus = (\n",
    "    df_posts_agg_c1[mask_min_posts_for_reg_mean]\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# calculate mean for all other subs: add UNWEIGHTED subreddit_description into averages\n",
    "info(f\"Calculating mean for subs BELOW post threshold...\")\n",
    "df_subs_agg_c1_Nbelow = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df_posts_agg_c1[~mask_min_posts_for_reg_mean],\n",
    "            df_v_subs[df_v_subs['subreddit_id'].isin(subreddits_below_n_)]\n",
    "        ]\n",
    "    )\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "mlf.log_ram_stats(only_memory_used=True)\n",
    "info(f\"Combining all subreddits...\")\n",
    "df_subs_agg_c1 = (\n",
    "    df_posts_for_embedding_count\n",
    "    .merge(\n",
    "        pd.concat([df_subs_agg_c1_Nplus, df_subs_agg_c1_Nbelow]),\n",
    "        how='outer',\n",
    "        on=l_ix_sub_level\n",
    "    )\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "\n",
    "# Check for dupes\n",
    "assert(len(df_subs_agg_c1) == df_subs_agg_c1['subreddit_id'].nunique()), f\"Found duplicate subreddit_ids\"\n",
    "\n",
    "r_, c_ = df_subs_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_subs_agg_c1-rows\": r_,\n",
    "        f\"df_subs_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "info(f\"{r_:,.0f} rows, {c_:,.0f} cols  <- df_subs_agg_c1.shape (posts + comments + sub description)\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_subs_c1 = elapsed_time(start_time=t_start_agg_subs_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_subs_agg_c1',\n",
    "                  t_agg_subs_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd9629ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>posts_for_embeddings_count</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>705955</th>\n",
       "      <td>t5_zzsmq</td>\n",
       "      <td>floridamanatheart</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027130</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.059026</td>\n",
       "      <td>0.050773</td>\n",
       "      <td>0.016815</td>\n",
       "      <td>0.003628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705956</th>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>275</td>\n",
       "      <td>-0.011574</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>-0.015716</td>\n",
       "      <td>0.034211</td>\n",
       "      <td>0.015709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705957</th>\n",
       "      <td>t5_zzw6f</td>\n",
       "      <td>missourisingles</td>\n",
       "      <td>32</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>-0.045115</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.030778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705958</th>\n",
       "      <td>t5_zzw7y</td>\n",
       "      <td>geofssim</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.038012</td>\n",
       "      <td>0.043682</td>\n",
       "      <td>-0.032269</td>\n",
       "      <td>-0.086736</td>\n",
       "      <td>-0.057276</td>\n",
       "      <td>-0.009810</td>\n",
       "      <td>0.032813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705959</th>\n",
       "      <td>t5_zzwrs</td>\n",
       "      <td>hypnosisisbs</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017816</td>\n",
       "      <td>-0.022911</td>\n",
       "      <td>-0.020059</td>\n",
       "      <td>-0.014588</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>-0.024723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705960</th>\n",
       "      <td>t5_zzyg0</td>\n",
       "      <td>creepyscarystories</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030785</td>\n",
       "      <td>-0.005454</td>\n",
       "      <td>0.021258</td>\n",
       "      <td>0.017853</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.039826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705961</th>\n",
       "      <td>t5_zzze9</td>\n",
       "      <td>demonmemes</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.047720</td>\n",
       "      <td>-0.064439</td>\n",
       "      <td>-0.059378</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>-0.015243</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>-0.014066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705962</th>\n",
       "      <td>t5_zzzyw</td>\n",
       "      <td>rachelnicki</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.038606</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>-0.022144</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.029157</td>\n",
       "      <td>-0.011033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_id      subreddit_name  posts_for_embeddings_count  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6\n",
       "705955     t5_zzsmq   floridamanatheart                           1      0.027130      0.002310     -0.000681      0.059026      0.050773      0.016815      0.003628\n",
       "705956     t5_zzszh   circumcisiongrief                         275     -0.011574      0.028891      0.004747      0.004758     -0.015716      0.034211      0.015709\n",
       "705957     t5_zzw6f     missourisingles                          32      0.009504      0.006715     -0.004402      0.019836     -0.045115      0.047648      0.030778\n",
       "705958     t5_zzw7y            geofssim                           1     -0.038012      0.043682     -0.032269     -0.086736     -0.057276     -0.009810      0.032813\n",
       "705959     t5_zzwrs        hypnosisisbs                           2      0.017816     -0.022911     -0.020059     -0.014588      0.004333      0.031112     -0.024723\n",
       "705960     t5_zzyg0  creepyscarystories                           2      0.030785     -0.005454      0.021258      0.017853      0.008426      0.050500      0.039826\n",
       "705961     t5_zzze9          demonmemes                           1     -0.047720     -0.064439     -0.059378      0.002949     -0.015243      0.047832     -0.014066\n",
       "705962     t5_zzzyw         rachelnicki                           5      0.003122      0.038606      0.004465     -0.022144      0.022109      0.029157     -0.011033"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs_agg_c1.iloc[-8:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4099e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>posts_for_embeddings_count</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t5_100mht</td>\n",
       "      <td>thelongestgameever2</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>-0.019218</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>-0.003276</td>\n",
       "      <td>-0.037487</td>\n",
       "      <td>0.038918</td>\n",
       "      <td>0.001788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t5_100mqi</td>\n",
       "      <td>shoelacecult</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.017796</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.031859</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.044953</td>\n",
       "      <td>0.008811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t5_100pg0</td>\n",
       "      <td>ru_animemes</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.052363</td>\n",
       "      <td>0.019555</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>-0.080538</td>\n",
       "      <td>-0.020567</td>\n",
       "      <td>0.021408</td>\n",
       "      <td>0.026688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714150</th>\n",
       "      <td>t5_100q7e</td>\n",
       "      <td>alahly</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002044</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>0.019453</td>\n",
       "      <td>-0.046617</td>\n",
       "      <td>0.039276</td>\n",
       "      <td>0.060532</td>\n",
       "      <td>-0.028324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714836</th>\n",
       "      <td>t5_100r30</td>\n",
       "      <td>comss</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066905</td>\n",
       "      <td>0.038777</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.012710</td>\n",
       "      <td>0.050384</td>\n",
       "      <td>-0.077518</td>\n",
       "      <td>-0.062838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t5_100sns</td>\n",
       "      <td>wapt</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.038230</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>-0.042922</td>\n",
       "      <td>-0.044318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t5_100tjt</td>\n",
       "      <td>jodi_huisentruit_case</td>\n",
       "      <td>6</td>\n",
       "      <td>0.008531</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>0.033053</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>-0.012744</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>-0.016081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t5_100uqq</td>\n",
       "      <td>thethinkingfox</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>-0.021833</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>-0.038427</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.044391</td>\n",
       "      <td>0.041226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_id         subreddit_name  posts_for_embeddings_count  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6\n",
       "10        t5_100mht    thelongestgameever2                          34     -0.004382     -0.019218      0.028417     -0.003276     -0.037487      0.038918      0.001788\n",
       "11        t5_100mqi           shoelacecult                           2     -0.017796      0.003162      0.031859      0.003582      0.002934      0.044953      0.008811\n",
       "12        t5_100pg0            ru_animemes                           1     -0.052363      0.019555      0.006061     -0.080538     -0.020567      0.021408      0.026688\n",
       "714150    t5_100q7e                 alahly                           0     -0.002044      0.064038      0.019453     -0.046617      0.039276      0.060532     -0.028324\n",
       "714836    t5_100r30                  comss                           0      0.066905      0.038777     -0.000221      0.012710      0.050384     -0.077518     -0.062838\n",
       "13        t5_100sns                   wapt                           6     -0.038230      0.004211      0.022296      0.012222      0.009444     -0.042922     -0.044318\n",
       "14        t5_100tjt  jodi_huisentruit_case                           6      0.008531      0.032689      0.033053      0.023155     -0.012744      0.020924     -0.016081\n",
       "15        t5_100uqq         thethinkingfox                           5      0.009767     -0.021833      0.016461     -0.038427      0.026392      0.044391      0.041226"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs_agg_c1.iloc[10:18, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6501d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:53:53 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '59.21%', 'memory_used': '855,526'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.5920754954631994, 'memory_used': 855526}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab790bd",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "This one we can save as a pandas df, no need to split it into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d0f85b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:53:53 | INFO | \"Converting pandas to dask...\"\n",
      "21:53:54 | INFO | \"  1,618.8 MB <- Memory usage\"\n",
      "21:53:54 | INFO | \"       4\t<- target Dask partitions\t  450.0 <- target MB partition size\"\n",
      "21:54:08 | INFO | \"  Logging df to mlflow...\"\n",
      "21:54:56 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '59.21%', 'memory_used': '855,595'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.3 s, sys: 18.6 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.5921232476170637, 'memory_used': 855595}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_subs_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_subs_agg_c1-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_subs_agg_c1,\n",
    "    d_dfs_to_save['df_subs_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_subs_agg_c1']['local'], artifact_path='df_subs_agg_c1')\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b882b9",
   "metadata": {},
   "source": [
    "## 2nd flow for subreddit level -- do not include additional weight from subreddit description\n",
    "Potentially, we might be skewing the embeddings too much by adding extra weight to subreddit description.\n",
    "\n",
    "So save embeddings WITHOUT additional weights so that we can compare the two approaches.\n",
    "\n",
    "We'll still fill subreddits w/o posts with subreddit description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4f57397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:08:30 | INFO | \"SUBREDDIT-LEVEL C1 no extra sub description weight - posts + comments + sub descriptions\"\n",
      "22:08:30 | INFO | \"Mean for subs above threshold: 3 (already calculated)\"\n",
      "22:08:30 | INFO | \"Calculating mean for subs BELOW post threshold...\"\n",
      "22:08:40 | INFO | \"65,797\"\n",
      "22:08:49 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '53.52%', 'memory_used': '773,338'}\"\n",
      "22:08:49 | INFO | \"Combining all subreddits...\"\n",
      "22:08:58 | INFO | \"771,760 rows, 515 cols  <- df_subs_agg_c1_uw.shape (posts + comments + sub description)\"\n",
      "22:08:58 | INFO | \"  0:31:18.225451 <- Total Agg fxn time time elapsed\"\n",
      "22:09:07 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '53.62%', 'memory_used': '774,849'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.5362421546325472, 'memory_used': 774849}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(f\"SUBREDDIT-LEVEL C1 no extra sub description weight - posts + comments + sub descriptions\")\n",
    "t_start_agg_subs_c1_uw = datetime.utcnow()\n",
    "\n",
    "# 3+ posts: simple mean()\n",
    "info(f\"Mean for subs above threshold: {n_min_posts_for_regular_mean} (already calculated)\")\n",
    "\n",
    "# calculate mean for all other subs: add UNWEIGHTED subreddit_description into averages\n",
    "info(f\"Calculating mean for subs BELOW post threshold...\")\n",
    "df_subs_agg_c1_Nbelow_uw = (\n",
    "    df_posts_agg_c1[~mask_min_posts_for_reg_mean]\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "# get embeddings for subs w/ zero posts\n",
    "subs_wo_posts = df_posts_for_embedding_count[df_posts_for_embedding_count[c_post_embedding_count] == 0]['subreddit_id']\n",
    "info(f\"{len(subs_wo_posts):,.0f}\")\n",
    "\n",
    "mlf.log_ram_stats(only_memory_used=True)\n",
    "info(f\"Combining all subreddits...\")\n",
    "df_subs_agg_c1_uw = (\n",
    "    df_posts_for_embedding_count\n",
    "    .merge(\n",
    "        pd.concat(\n",
    "            [\n",
    "                df_subs_agg_c1_Nplus, df_subs_agg_c1_Nbelow_uw, \n",
    "                df_v_subs[df_v_subs['subreddit_id'].isin(subs_wo_posts)]\n",
    "            ]\n",
    "        ),\n",
    "        how='outer',\n",
    "        on=l_ix_sub_level\n",
    "    )\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "\n",
    "# Check for dupes\n",
    "assert(len(df_subs_agg_c1_uw) == df_subs_agg_c1_uw['subreddit_id'].nunique()), f\"Found duplicate subreddit_ids\"\n",
    "\n",
    "r_, c_ = df_subs_agg_c1_uw.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_subs_agg_c1_uw-rows\": r_,\n",
    "        f\"df_subs_agg_c1_uw-cols\": c_,\n",
    "    }\n",
    ")\n",
    "info(f\"{r_:,.0f} rows, {c_:,.0f} cols  <- df_subs_agg_c1_uw.shape (posts + comments + sub description)\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_subs_c1_uw = elapsed_time(start_time=t_start_agg_subs_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_subs_agg_c1_uw',\n",
    "                  t_agg_subs_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86091e3",
   "metadata": {},
   "source": [
    "### Check equality of unweighted v. weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd6ebe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be True\n",
    "assert True == np.allclose(\n",
    "    (\n",
    "        df_subs_agg_c1_uw\n",
    "        [df_subs_agg_c1_uw['subreddit_id'].isin(subreddits_above_n_.head(20))]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    ),\n",
    "    (\n",
    "        df_subs_agg_c1\n",
    "        [df_subs_agg_c1['subreddit_id'].isin(subreddits_above_n_.head(20))]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd5b8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be False\n",
    "l_sample_subs_below_n_ = list(subreddits_below_n_)[:20]\n",
    "assert False == np.allclose(\n",
    "    (\n",
    "        df_subs_agg_c1_uw\n",
    "        [df_subs_agg_c1_uw['subreddit_id'].isin(l_sample_subs_below_n_)]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    ),\n",
    "    (\n",
    "        df_subs_agg_c1\n",
    "        [df_subs_agg_c1['subreddit_id'].isin(l_sample_subs_below_n_)]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fc7c9",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "Use dask b/c as we model over 200k subreddits a single file gets too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfe1f4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:20:07 | INFO | \"Converting pandas to dask...\"\n",
      "22:20:07 | INFO | \"  1,618.8 MB <- Memory usage\"\n",
      "22:20:07 | INFO | \"       4\t<- target Dask partitions\t  450.0 <- target MB partition size\"\n",
      "22:20:22 | INFO | \"  Logging df to mlflow...\"\n",
      "22:21:10 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '53.62%', 'memory_used': '774,858'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.7 s, sys: 18 s, total: 1min 3s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.5362483831743555, 'memory_used': 774858}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "name_sub_agg_unweighted = 'df_subs_agg_c1_unweighted'\n",
    "d_dfs_to_save[name_sub_agg_unweighted]['local'] = (\n",
    "    path_this_model / f\"{name_sub_agg_unweighted}-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_subs_agg_c1_uw,\n",
    "    d_dfs_to_save[name_sub_agg_unweighted]['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save[name_sub_agg_unweighted]['local'], artifact_path=name_sub_agg_unweighted)\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a864b",
   "metadata": {},
   "source": [
    "# End run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "101b80b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:22:33 | INFO | \"  13:40:40.488306 <- Total Agg fxn time time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "# finish logging total time + end mlflow run\n",
    "total_fxn_time = elapsed_time(start_time=t_start_agg_embed, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-full_aggregation_fxn_minutes',\n",
    "                  total_fxn_time / timedelta(minutes=1)\n",
    "                  )\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b620a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ad602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
