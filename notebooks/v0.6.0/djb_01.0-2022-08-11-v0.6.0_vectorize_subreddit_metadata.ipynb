{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e6ba52",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to vectorize the text for subreddit metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0c1b0",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1b512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1480293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "subclu\t\tv: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([np, pd, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02907e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:15:42 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ce3ca",
   "metadata": {},
   "source": [
    "# Auth note\n",
    "This notebook assumes you have authenticated using the gcloud CLI. Example</br>\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354536f",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize \n",
    "\n",
    "When we call the vectorizing function, it calls the data loader under the hood.\n",
    "See the configs in:\n",
    "- `subclu2/config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b92cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddit_meta_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_subreddit_meta_v0.6.0'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88773d",
   "metadata": {},
   "source": [
    "## Run in bucket owned by i18n\n",
    "This bucket retains data longer than the gazette temp bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9d1038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text_and_metadata', 'config_description', 'local_cache_path', 'local_model_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "Data Loader kwags:\n",
      "  columns: ['subreddit_id', 'subreddit_name', 'subreddit_meta_for_embeddings']\n",
      "  df_format: pandas\n",
      "  unique_check: False\n",
      "  verbose: True\n",
      "  bucket_name: i18n-subreddit-clustering\n",
      "  gcs_path: i18n_topic_model_batch/runs/20220811/subreddits/text\n",
      "  local_cache_path: /home/jupyter/subreddit_clustering_i18n/data/local_cache/\n",
      "  n_sample_files: None\n",
      "  n_files_slice_start: None\n",
      "  n_files_slice_end: None\n",
      "`2022-08-11 08:28:59,485` | `INFO` | `Using hydra's path`\n",
      "`2022-08-11 08:28:59,485` | `INFO` | `  Log file created at: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-08-11/08-28-59/logs/2022-08-11_08-28-59_vectorize_text.log`\n",
      "`2022-08-11 08:28:59,485` | `INFO` | `Start vectorize function`\n",
      "`2022-08-11 08:28:59,485` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-08-11 08:29:01,704` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-08-11 08:29:01,705` | `INFO` | `Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.`\n",
      "`2022-08-11 08:29:04,714` | `INFO` | `Downloaded https://tfhub.dev/google/universal-sentence-encoder-multilingual/3, Total size: 266.88MB`\n",
      "`2022-08-11 08:29:04,715` | `INFO` | `Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.`\n",
      "`2022-08-11 08:29:12,832` | `INFO` | `Model loaded`\n",
      "`2022-08-11 08:29:12,832` | `INFO` | `  Loading & Processing each file independently`\n",
      "`2022-08-11 08:29:14,462` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/subreddits/text`\n",
      "`2022-08-11 08:29:14,560` | `INFO` | `  3 <- Files matching prefix`\n",
      "`2022-08-11 08:29:14,560` | `INFO` | `  3 <- Files to check`\n",
      "`2022-08-11 08:29:18,012` | `INFO` | `  Files already cached: 0`\n",
      "`2022-08-11 08:29:18,012` | `INFO` | `0:00:05.179272  <- Downloading files elapsed time`\n",
      "`2022-08-11 08:29:18,015` | `INFO` | `  Files already downloaded.`\n",
      "`2022-08-11 08:29:18,722` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-08-11 08:29:18,722` | `INFO` | `Vectorizing column: subreddit_meta_for_embeddings`\n",
      "`2022-08-11 08:29:18,953` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "2022-08-11 08:29:31.357066: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.80GiB (rounded to 3007938560)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-08-11 08:29:31.357677: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************_______*****_______*****___**********************************_______________\n",
      "2022-08-11 08:29:31.357717: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[587488,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-08-11 08:29:31,360` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[587488,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-08-11 08:29:32,704` | `INFO` | `  Vectorizing:   1%|3                            | 1/88 [00:13<19:56, 13.75s/it]`\n",
      "`2022-08-11 08:29:44,704` | `INFO` | `  Vectorizing:  12%|###5                        | 11/88 [00:25<02:35,  2.02s/it]`\n",
      "`2022-08-11 08:29:56,938` | `INFO` | `  Vectorizing:  30%|########2                   | 26/88 [00:37<01:15,  1.22s/it]`\n",
      "`2022-08-11 08:30:08,017` | `INFO` | `  Vectorizing:  30%|########2                   | 26/88 [00:49<01:15,  1.22s/it]`\n",
      "`2022-08-11 08:30:09,356` | `INFO` | `  Vectorizing:  51%|##############3             | 45/88 [00:50<00:39,  1.10it/s]`\n",
      "`2022-08-11 08:30:21,423` | `INFO` | `  Vectorizing:  75%|#####################       | 66/88 [01:02<00:16,  1.33it/s]`\n",
      "`2022-08-11 08:30:33,646` | `INFO` | `  Vectorizing: 100%|############################| 88/88 [01:14<00:00,  1.49it/s]`\n",
      "`2022-08-11 08:30:33,646` | `INFO` | `  Vectorizing: 100%|############################| 88/88 [01:14<00:00,  1.18it/s]`\n",
      "\n",
      "`2022-08-11 08:30:34,361` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859/000000000000-131971_by_514.parquet`\n",
      "`2022-08-11 08:30:47,987` | `INFO` | `Files in batch:  33%|######3            | 1/3 [01:29<02:59, 89.97s/it]`\n",
      "`2022-08-11 08:30:48,478` | `INFO` | `  Processing: 000000000001.parquet`\n",
      "`2022-08-11 08:30:48,479` | `INFO` | `Vectorizing column: subreddit_meta_for_embeddings`\n",
      "`2022-08-11 08:30:48,726` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-08-11 08:31:00,775` | `INFO` | `  Vectorizing:  17%|####4                      | 22/133 [00:12<01:00,  1.83it/s]`\n",
      "`2022-08-11 08:31:13,090` | `INFO` | `  Vectorizing:  34%|#########1                 | 45/133 [00:24<00:47,  1.85it/s]`\n",
      "`2022-08-11 08:31:25,498` | `INFO` | `  Vectorizing:  52%|##############             | 69/133 [00:36<00:33,  1.89it/s]`\n",
      "`2022-08-11 08:31:37,539` | `INFO` | `  Vectorizing:  70%|##################8        | 93/133 [00:48<00:20,  1.93it/s]`\n",
      "`2022-08-11 08:31:48,227` | `INFO` | `  Vectorizing:  70%|##################8        | 93/133 [00:59<00:20,  1.93it/s]`\n",
      "`2022-08-11 08:31:49,938` | `INFO` | `  Vectorizing:  90%|#######################4  | 120/133 [01:01<00:06,  2.02it/s]`\n",
      "`2022-08-11 08:31:55,434` | `INFO` | `  Vectorizing: 100%|##########################| 133/133 [01:06<00:00,  1.99it/s]`\n",
      "\n",
      "`2022-08-11 08:31:56,571` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859/000000000001-198630_by_514.parquet`\n",
      "`2022-08-11 08:32:12,465` | `INFO` | `Files in batch:  67%|############6      | 2/3 [02:54<01:26, 86.74s/it]`\n",
      "`2022-08-11 08:32:13,023` | `INFO` | `  Processing: 000000000002.parquet`\n",
      "`2022-08-11 08:32:13,024` | `INFO` | `Vectorizing column: subreddit_meta_for_embeddings`\n",
      "`2022-08-11 08:32:13,246` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-08-11 08:32:25,310` | `INFO` | `  Vectorizing:  10%|##7                        | 30/295 [00:12<01:46,  2.49it/s]`\n",
      "`2022-08-11 08:32:37,404` | `INFO` | `  Vectorizing:  21%|#####6                     | 62/295 [00:24<01:30,  2.58it/s]`\n",
      "`2022-08-11 08:32:48,444` | `INFO` | `  Vectorizing:  21%|#####6                     | 62/295 [00:35<01:30,  2.58it/s]`\n",
      "`2022-08-11 08:32:49,491` | `INFO` | `  Vectorizing:  32%|########6                  | 95/295 [00:36<01:15,  2.65it/s]`\n",
      "`2022-08-11 08:33:01,664` | `INFO` | `  Vectorizing:  43%|###########2              | 128/295 [00:48<01:02,  2.67it/s]`\n",
      "`2022-08-11 08:33:13,905` | `INFO` | `  Vectorizing:  55%|##############3           | 163/295 [01:00<00:48,  2.74it/s]`\n",
      "`2022-08-11 08:33:26,153` | `INFO` | `  Vectorizing:  67%|#################5        | 199/295 [01:12<00:34,  2.81it/s]`\n",
      "`2022-08-11 08:33:38,435` | `INFO` | `  Vectorizing:  80%|####################7     | 235/295 [01:25<00:21,  2.85it/s]`\n",
      "`2022-08-11 08:33:48,566` | `INFO` | `  Vectorizing:  80%|####################7     | 235/295 [01:35<00:21,  2.85it/s]`\n",
      "`2022-08-11 08:33:50,498` | `INFO` | `  Vectorizing:  92%|#######################7  | 270/295 [01:37<00:08,  2.87it/s]`\n",
      "`2022-08-11 08:33:58,904` | `INFO` | `  Vectorizing: 100%|##########################| 295/295 [01:45<00:00,  2.79it/s]`\n",
      "\n",
      "`2022-08-11 08:34:01,805` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859/000000000002-441159_by_514.parquet`\n",
      "`2022-08-11 08:34:28,169` | `INFO` | `Files in batch: 100%|##################| 3/3 [05:10<00:00, 109.10s/it]`\n",
      "`2022-08-11 08:34:28,169` | `INFO` | `Files in batch: 100%|##################| 3/3 [05:10<00:00, 103.38s/it]`\n",
      "\n",
      "`2022-08-11 08:34:28,170` | `INFO` | `  0:05:15.337113 <- df_subs vectorizing time elapsed`\n",
      "`2022-08-11 08:34:28,170` | `INFO` | `Saving hydra config...`\n",
      "/home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-08-11/08-28-59/.hydra\n",
      "`2022-08-11 08:34:31,573` | `INFO` | `Saving log file...`\n",
      "`2022-08-11 08:34:31,784` | `INFO` | `  0:05:32.298937 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on full data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc476a74",
   "metadata": {},
   "source": [
    "## Rough time projections\n",
    "Based on the file(s) processed above. Here are some rough projections for how long it might take to process all posts needed for the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7624b09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_ec5cd_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >n_rows</th>        <th class=\"col_heading level0 col1\" >n_jobs</th>        <th class=\"col_heading level0 col2\" >projected_hours</th>        <th class=\"col_heading level0 col3\" >projected_days</th>        <th class=\"col_heading level0 col4\" >projected_mins</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_ec5cd_row0_col0\" class=\"data row0 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "                        <td id=\"T_ec5cd_row0_col2\" class=\"data row0 col2\" >6.93</td>\n",
       "                        <td id=\"T_ec5cd_row0_col3\" class=\"data row0 col3\" >0.29</td>\n",
       "                        <td id=\"T_ec5cd_row0_col4\" class=\"data row0 col4\" >416.03</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_ec5cd_row1_col0\" class=\"data row1 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "                        <td id=\"T_ec5cd_row1_col2\" class=\"data row1 col2\" >3.47</td>\n",
       "                        <td id=\"T_ec5cd_row1_col3\" class=\"data row1 col3\" >0.14</td>\n",
       "                        <td id=\"T_ec5cd_row1_col4\" class=\"data row1 col4\" >208.02</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_ec5cd_row2_col0\" class=\"data row2 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row2_col1\" class=\"data row2 col1\" >3</td>\n",
       "                        <td id=\"T_ec5cd_row2_col2\" class=\"data row2 col2\" >2.31</td>\n",
       "                        <td id=\"T_ec5cd_row2_col3\" class=\"data row2 col3\" >0.10</td>\n",
       "                        <td id=\"T_ec5cd_row2_col4\" class=\"data row2 col4\" >138.68</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_ec5cd_row3_col0\" class=\"data row3 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row3_col1\" class=\"data row3 col1\" >4</td>\n",
       "                        <td id=\"T_ec5cd_row3_col2\" class=\"data row3 col2\" >1.73</td>\n",
       "                        <td id=\"T_ec5cd_row3_col3\" class=\"data row3 col3\" >0.07</td>\n",
       "                        <td id=\"T_ec5cd_row3_col4\" class=\"data row3 col4\" >104.01</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_ec5cd_row4_col0\" class=\"data row4 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row4_col1\" class=\"data row4 col1\" >5</td>\n",
       "                        <td id=\"T_ec5cd_row4_col2\" class=\"data row4 col2\" >1.39</td>\n",
       "                        <td id=\"T_ec5cd_row4_col3\" class=\"data row4 col3\" >0.06</td>\n",
       "                        <td id=\"T_ec5cd_row4_col4\" class=\"data row4 col4\" >83.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_ec5cd_row5_col0\" class=\"data row5 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row5_col1\" class=\"data row5 col1\" >6</td>\n",
       "                        <td id=\"T_ec5cd_row5_col2\" class=\"data row5 col2\" >1.16</td>\n",
       "                        <td id=\"T_ec5cd_row5_col3\" class=\"data row5 col3\" >0.05</td>\n",
       "                        <td id=\"T_ec5cd_row5_col4\" class=\"data row5 col4\" >69.34</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_ec5cd_row6_col0\" class=\"data row6 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row6_col1\" class=\"data row6 col1\" >7</td>\n",
       "                        <td id=\"T_ec5cd_row6_col2\" class=\"data row6 col2\" >0.99</td>\n",
       "                        <td id=\"T_ec5cd_row6_col3\" class=\"data row6 col3\" >0.04</td>\n",
       "                        <td id=\"T_ec5cd_row6_col4\" class=\"data row6 col4\" >59.43</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec5cd_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_ec5cd_row7_col0\" class=\"data row7 col0\" >51,906,348</td>\n",
       "                        <td id=\"T_ec5cd_row7_col1\" class=\"data row7 col1\" >8</td>\n",
       "                        <td id=\"T_ec5cd_row7_col2\" class=\"data row7 col2\" >0.87</td>\n",
       "                        <td id=\"T_ec5cd_row7_col3\" class=\"data row7 col3\" >0.04</td>\n",
       "                        <td id=\"T_ec5cd_row7_col4\" class=\"data row7 col4\" >52.00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0456940550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projections\n",
    "l_estimates_ = list()\n",
    "\n",
    "time_mins = 5.5 + (1/6)\n",
    "rows_embedded_ = 707000\n",
    "\n",
    "rows_to_embed_ = int(51906348)\n",
    "projected_time_mins = time_mins * (rows_to_embed_ / rows_embedded_)\n",
    "\n",
    "l_estimates_.append(\n",
    "    {\n",
    "        'n_rows': rows_to_embed_,\n",
    "        'n_jobs': 1,\n",
    "        'projected_hours': projected_time_mins / 60,\n",
    "        'projected_days': projected_time_mins / (60 * 24),\n",
    "        'projected_mins': projected_time_mins,\n",
    "    }\n",
    ")\n",
    "\n",
    "for n_parallel_jobs_ in range(2, 9):\n",
    "    proj_mins_parallel = projected_time_mins / n_parallel_jobs_\n",
    "    l_estimates_.append(\n",
    "        {\n",
    "            'n_rows': rows_to_embed_,\n",
    "            'n_jobs': n_parallel_jobs_,\n",
    "            'projected_hours': proj_mins_parallel / 60, \n",
    "            'projected_days': proj_mins_parallel / (60 * 24), \n",
    "            'projected_mins': proj_mins_parallel,\n",
    "        }\n",
    "    )\n",
    "\n",
    "style_df_numeric(\n",
    "    pd.DataFrame(l_estimates_)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc45143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
