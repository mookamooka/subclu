{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8099fc1b",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "**2022-08-15: v0.6.0**\n",
    "<br>Test `dask.delayed` to run aggregation of multiple subreddits in parallel.\n",
    "With the new project we expect to aggregate posts for over 300k subreddits. For most of the process, each subreddit can be processed independently of other subreddits, so it makes sense to try to split up the work so we can speed things up.\n",
    "\n",
    "**2022-06-29: v0.5.0**\n",
    "<br>Because we embedded post & text as a single embedding and we didn't use MLflow to create those embeddings, it's easier to  run the embeddings in this notebook rather than to re-use or re-write the old `AggregateEmbeddings` class.\n",
    "\n",
    "Provenance:\n",
    "* `v0.4.1 / djb_03.01-2021-12-aggregate_v041_posts_and_comments_pandas.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb460ce",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f68997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d68bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from logging import info\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "from subclu.models import aggregate_embeddings_pd\n",
    "\n",
    "from subclu.utils import set_working_directory, get_project_subfolder\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric,\n",
    "    elapsed_time,\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import LoadSubredditsGCS\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe923a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ab4ac",
   "metadata": {},
   "source": [
    "# Set Local model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c44b252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/manual_v060_2022-08-16_084129')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_model_timestamp = datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')\n",
    "path_this_model = get_project_subfolder(\n",
    "    f\"data/models/aggregate_embeddings/manual_v060_{manual_model_timestamp}\"\n",
    ")\n",
    "Path.mkdir(path_this_model, parents=True, exist_ok=True)\n",
    "path_this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8612af",
   "metadata": {},
   "source": [
    "# Load config for embeddings aggregation\n",
    "\n",
    "For v0.6.0 embeddings I didn't use mlflow to track the embeddings inference. We'll need to get them from these folders in GCS:\n",
    "\n",
    "- [Subreddit metadata](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555`\n",
    "- [Post + Comment Text (already combined)](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede9cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data_text_and_metadata', 'data_embeddings_to_aggregate', 'aggregate_params', 'bucket_output', 'mlflow_tracking_uri', 'mlflow_experiment', 'n_sample_subreddits', 'n_sample_posts_files', 'n_sample_comments_files', 'agg_style'])\n"
     ]
    }
   ],
   "source": [
    "cfg_agg_embeddings = LoadHydraConfig(\n",
    "    config_name='aggregate_embeddings_v0.6.0.yaml',\n",
    "    config_path=\"../config\",\n",
    "    overrides=[\n",
    "        f\"agg_style=serial\",\n",
    "    ],\n",
    ")\n",
    "print(cfg_agg_embeddings.config_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89277716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75138195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_text_and_metadata:\n",
      "    dataset_name: v0.6.0 inputs. ~110k seed subreddits, ~340k with 3+ posts, ~700k total subreddits\n",
      "    bucket_name: i18n-subreddit-clustering\n",
      "    folder_subreddits_text_and_meta: i18n_topic_model_batch/runs/20220811/subreddits/text\n",
      "    folder_posts_text_and_meta: i18n_topic_model_batch/runs/20220811/posts\n",
      "    folder_comments_text_and_meta: i18n_topic_model_batch/runs/20220811/comments\n",
      "    folder_post_and_comment_text_and_meta: i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all\n",
      "data_embeddings_to_aggregate:\n",
      "    bucket_embeddings: i18n-subreddit-clustering\n",
      "    post_and_comments_folder_embeddings: i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218\n",
      "    subreddit_desc_folder_embeddings: i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859\n",
      "    col_subreddit_id: subreddit_id\n",
      "aggregate_params:\n",
      "    min_post_and_comment_text_len: 3\n",
      "    agg_post_post_and_comment_weight: 85\n",
      "    agg_post_subreddit_desc_weight: 15\n",
      "bucket_output: i18n-subreddit-clustering\n",
      "mlflow_tracking_uri: sqlite\n",
      "mlflow_experiment: v0.6.0_mUSE_aggregates\n",
      "n_sample_subreddits: None\n",
      "n_sample_posts_files: None\n",
      "n_sample_comments_files: None\n",
      "agg_style: serial\n"
     ]
    }
   ],
   "source": [
    "for k_, v_ in cfg_agg_embeddings.config_dict.items():\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"{k_}:\")\n",
    "        for k2_, v2_ in v_.items():\n",
    "            print(f\"    {k2_}: {v2_}\")\n",
    "    else:\n",
    "        print(f\"{k_}: {v_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ae3e2",
   "metadata": {},
   "source": [
    "# Start MLflow & Log base params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a617a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MlflowLogger(tracking_uri=cfg_agg_embeddings.config_dict['mlflow_tracking_uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a6a045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:41:31 | INFO | \"== Start run_aggregation() method ==\"\n",
      "08:41:31 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "08:41:31 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "08:41:31 | INFO | \"cpu_count: 96\"\n",
      "08:41:31 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '7.51%', 'memory_total': '1,444,961', 'memory_used': '108,493', 'memory_free': '1,175,788'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_total': 1444961,\n",
       " 'memory_used_percent': 0.0750836873797978,\n",
       " 'memory_used': 108493,\n",
       " 'memory_free': 1175788}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_experiment = cfg_agg_embeddings.config_dict['mlflow_experiment']\n",
    "# 'v0.6.0_mUSE_aggregates', 'v0.6.0_mUSE_aggregates_test'\n",
    "\n",
    "\n",
    "t_start_agg_embed = datetime.utcnow()\n",
    "info(f\"== Start run_aggregation() method ==\")\n",
    "\n",
    "\n",
    "info(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "mlf.set_experiment(mlflow_experiment)\n",
    "mlflow.start_run()\n",
    "mlf.add_git_hash_to_active_run()\n",
    "mlf.set_tag_hostname(key='host_name')\n",
    "mlf.log_param_hostname(key='host_name')\n",
    "mlf.log_cpu_count()\n",
    "mlf.log_ram_stats(param=True, only_memory_used=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b498b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859\n",
      "i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218\n"
     ]
    }
   ],
   "source": [
    "# set weights\n",
    "# Normalize them by dividing by 100\n",
    "WEIGHT_POST_COMMENT = (\n",
    "    cfg_agg_embeddings.config_dict['aggregate_params']['agg_post_post_and_comment_weight'] / 100\n",
    ")\n",
    "WEIGHT_SUB_META = (\n",
    "    cfg_agg_embeddings.config_dict['aggregate_params']['agg_post_subreddit_desc_weight'] / 100\n",
    ")\n",
    "print(WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "assert(1.0 == WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "\n",
    "\n",
    "gcs_sub_embeddings = cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['subreddit_desc_folder_embeddings']\n",
    "print(gcs_sub_embeddings)\n",
    "gcs_post_comment_embeddings = cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['post_and_comments_folder_embeddings']\n",
    "print(gcs_post_comment_embeddings)\n",
    "\n",
    "mlflow.log_params(\n",
    "    {\n",
    "        'embeddings_bucket': cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "        'embeddings_subreddit_path': gcs_sub_embeddings,\n",
    "        'embeddings_post_and_comments_path': gcs_post_comment_embeddings,\n",
    "        'weight_post_and_comments': WEIGHT_POST_COMMENT,\n",
    "        'weight_subreddit_meta': WEIGHT_SUB_META,\n",
    "    }\n",
    ")\n",
    "for k_, v_ in cfg_agg_embeddings.config_dict.items():\n",
    "    if isinstance(v_, str):\n",
    "        try:\n",
    "            mlflow.log_param(k_, v_)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204ee52",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c3e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:41:32 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/subreddits/text/embedding/2022-08-11_082859\"\n",
      "08:41:32 | INFO | \"  7 <- Files matching prefix\"\n",
      "08:41:32 | INFO | \"  7 <- Files to check\"\n",
      "08:41:32 | INFO | \"    000000000000-131971_by_514.parquet <- File already exists, not downloading\"\n",
      "08:41:32 | INFO | \"    000000000001-198630_by_514.parquet <- File already exists, not downloading\"\n",
      "08:41:32 | INFO | \"    000000000002-441159_by_514.parquet <- File already exists, not downloading\"\n",
      "08:41:32 | INFO | \"    2022-08-11_08-28-59_vectorize_text.log <- File already exists, not downloading\"\n",
      "08:41:32 | INFO | \"  Files already cached: 4\"\n",
      "08:41:32 | INFO | \"  Files already downloaded.\"\n",
      "08:41:32 | INFO | \"  df format: pandas\"\n",
      "08:41:37 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771,760 rows, 514 cols\n",
      "CPU times: user 4.47 s, sys: 4.3 s, total: 8.78 s\n",
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_start_data_load_ = datetime.utcnow()\n",
    "\n",
    "subs_v = LoadSubredditsGCS(\n",
    "    bucket_name=cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "    gcs_path=gcs_sub_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    ")\n",
    "subs_v.local_cache()\n",
    "\n",
    "df_v_subs = subs_v.read_as_one_df()\n",
    "r_subs, c_subs = df_v_subs.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_subs-rows\": r_subs,\n",
    "        f\"df_v_subs-cols\": c_subs,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_subs:,.0f} rows, {c_subs:,.0f} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2cb2c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil -m cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218 /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding\n"
     ]
    }
   ],
   "source": [
    "# gsutil is usually faster than the python library.\n",
    "remote_bucket_and_key = 'i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218'\n",
    "remote_gs_path = f'gs://{remote_bucket_and_key}'\n",
    "\n",
    "# Need to remove the last part of the local path otherwise we'll get duplicate subfolders:\n",
    "#. top/2021-12-14/2021-12-14 instead of top/2021-12-14\n",
    "local_f = f\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/{'/'.join(remote_bucket_and_key.split('/')[:-1])}\"\n",
    "Path(local_f).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(f\"Remote path:\\n  {remote_gs_path}\")\n",
    "# print(f\"Local path:\\n  {local_f}\")\n",
    "\n",
    "print(\n",
    "    f\"gsutil -m cp -r -n {remote_gs_path} {local_f}\"\n",
    ")\n",
    "# gsutil -m cp -r -n $remote_gs_path $local_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b8432cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:41:39 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220811/post_and_comment_text_combined/text_all/embedding/2022-08-11_084218\"\n",
      "08:41:39 | INFO | \"  197 <- Files matching prefix\"\n",
      "08:41:39 | INFO | \"  197 <- Files to check\"\n",
      "08:41:39 | INFO | \"    000000000000-264431_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000001-249532_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000002-308094_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000003-331082_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000004-356401_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000005-331679_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000006-253861_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000007-365498_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000008-271629_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000009-355342_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000010-356760_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000011-370490_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000012-357532_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000013-233247_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000014-134434_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000015-250143_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000016-136611_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000017-181862_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000018-140950_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000019-169025_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000020-153909_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000021-121734_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000022-149810_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000023-193707_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000024-157257_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000025-153696_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000026-178417_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000027-158698_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000028-166516_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000029-134767_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000030-167537_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000031-166943_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000032-148950_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000033-214139_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000034-145226_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000035-174913_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000036-129006_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000037-183050_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000038-228930_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000039-169420_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000040-190340_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000041-209568_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000042-166077_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000043-157511_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000044-176937_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000045-192161_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000046-246921_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000047-173822_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000048-185120_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000049-181449_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000050-189495_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000051-141399_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000052-214970_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000053-209653_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000054-227487_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000055-188764_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000056-166519_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000057-204426_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000058-135687_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000059-173364_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000060-171807_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000061-202668_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000062-196125_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000063-165010_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000064-190401_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000065-185782_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000066-158051_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000067-178195_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000068-232743_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000069-164717_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000070-187535_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000071-197794_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000072-188607_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000073-210410_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000074-230000_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000075-222524_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000076-166300_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000077-266372_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000078-209463_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000079-172924_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000080-186829_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000081-225345_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000082-229456_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000083-197983_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000084-219726_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000085-316505_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000086-172618_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000087-250852_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000088-201434_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000089-261433_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000090-223554_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000091-217039_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000092-302573_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000093-223954_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000094-227185_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000095-199144_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000096-176973_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000097-165848_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000098-267512_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000099-226861_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000100-206414_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000101-213130_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000102-247275_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000103-188984_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000104-283839_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000105-217584_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000106-267602_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000107-278886_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000108-265243_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000109-146745_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000110-330246_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000111-228777_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000112-281636_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000113-241766_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000114-245857_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000115-281145_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000116-230592_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000117-258417_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000118-337749_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000119-240860_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000120-271515_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000121-166452_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000122-294465_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000123-238877_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000124-299063_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000125-282082_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000126-205459_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000127-257803_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000128-229708_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000129-248835_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000130-279925_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000131-226903_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000132-315432_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000133-237756_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000134-310613_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000135-329852_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000136-236955_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000137-282993_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000138-258136_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000139-227795_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000140-420758_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000141-420500_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000142-356366_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000143-303307_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000144-227848_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000145-307717_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000146-307629_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000147-243582_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000148-301334_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000149-254682_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000150-301055_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000151-315099_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000152-360964_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000153-354388_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000154-380301_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000155-436827_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000156-334947_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000157-396313_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000158-444881_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000159-451416_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000160-462155_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000161-473842_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000162-407864_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000163-557506_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000164-410085_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000165-388272_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000166-372157_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000167-548366_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000168-423502_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000169-452732_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000170-522805_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000171-497492_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000172-424836_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000173-492358_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000174-693704_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000175-695091_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000176-455451_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000177-491107_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000178-584946_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000179-546528_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000180-411298_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000181-371464_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000182-275564_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000183-195271_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000184-285922_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000185-225947_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000186-360629_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000187-426063_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000188-357355_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000189-235049_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000190-318299_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000191-263350_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    000000000192-328908_by_515.parquet <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"    2022-08-11_08-42-18_vectorize_text.log <- File already exists, not downloading\"\n",
      "08:41:39 | INFO | \"  Files already cached: 194\"\n",
      "08:41:39 | INFO | \"  Files already downloaded.\"\n",
      "08:41:39 | INFO | \"  df format: pandas\"\n",
      "08:45:14 | INFO | \"  0:03:43.152099 <- Data Loading Time time elapsed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,906,348 rows, 515 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:45:18 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '30.61%', 'memory_used': '442,282'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 48s, sys: 16min 57s, total: 23min 46s\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.3060857697889424, 'memory_used': 442282}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pc_v = LoadSubredditsGCS(\n",
    "    bucket_name=cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "    gcs_path=gcs_post_comment_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='post_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=cfg_agg_embeddings.config_dict['n_sample_posts_files'],  # None,\n",
    "    n_files_slice_start=None,  # None,\n",
    "    n_files_slice_end=None,  # None, \n",
    ")\n",
    "pc_v.local_cache()\n",
    "\n",
    "df_v_pc = pc_v.read_as_one_df()\n",
    "r_pc, c_pc = df_v_pc.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_post_comments-rows\": r_pc,\n",
    "        f\"df_v_post_comments-cols\": c_pc,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_pc:,.0f} rows, {c_pc:,.0f} cols\")\n",
    "\n",
    "t_data_load = elapsed_time(start_time=t_start_data_load_, log_label='Data Loading Time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-data_loading_time',\n",
    "                  t_data_load / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368e515",
   "metadata": {},
   "source": [
    "# Set weights & create copy dfs for new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe55ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "l_ix_sub_level = ['subreddit_id', 'subreddit_name']\n",
    "l_ix_post_level = l_ix_sub_level + ['post_id']\n",
    "\n",
    "l_embedding_cols = [c for c in df_v_pc if c.startswith('embeddings_')]\n",
    "print(len(l_embedding_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "348cdd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:46:06 | INFO | \"Initializing weighted SUBS meta\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:46:08 | INFO | \"Initializing weighted POSTS embeddings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "CPU times: user 1min 40s, sys: 1min 45s, total: 3min 25s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_v_pc_weighted = df_v_pc.copy()\n",
    "\n",
    "df_v_subs_weighted = df_v_subs.copy()\n",
    "\n",
    "# should be True b/c they're copies\n",
    "print(np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515]))\n",
    "print(np.allclose(df_v_subs_weighted.iloc[:1000,2:515], df_v_subs.iloc[:1000,2:515]))\n",
    "\n",
    "# apply weight to all posts & subreddit meta at once (vectorized)\n",
    "info(f\"Initializing weighted SUBS meta\")\n",
    "df_v_subs_weighted[l_embedding_cols] = df_v_subs_weighted[l_embedding_cols] * WEIGHT_SUB_META\n",
    "\n",
    "info(f\"Initializing weighted POSTS embeddings\")\n",
    "df_v_pc_weighted[l_embedding_cols] = df_v_pc_weighted[l_embedding_cols] * WEIGHT_POST_COMMENT\n",
    "\n",
    "# NOW they shouldn't be equal (Should be False)\n",
    "print(np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515]))\n",
    "print(np.allclose(df_v_subs_weighted.iloc[:1000,2:515], df_v_subs.iloc[:1000,2:515]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fce625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_describe(df_v_pc[l_ix_post_level])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9138a",
   "metadata": {},
   "source": [
    "# Aggregate to Post-Level: Post&Comments + Subreddit Meta\n",
    "\n",
    "It's better to let pandas handle the interations with `.groupby('subreddit_id')`. Otherwise we have to create masks for each subreddit that can take much longer (17+ hours).\n",
    "\n",
    "- ETA with masks: +17.6 hours\n",
    "- ETA with groupby ~2.5 hours\n",
    "\n",
    "```\n",
    "# mask:\n",
    "0%  329/81973 [04:18<17:42:36, 1.28it/s]\n",
    "\n",
    "# .groupby()\n",
    "6% 4751/81973 [09:56<2:35:06, 8.30it/s]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Updates using `dask.delayed`:\n",
    "By combining .groupby() + `dask.delayed` we can process things ~3x faster:\n",
    "\n",
    "\n",
    "```\n",
    "# .groupby(), no dask delayed\n",
    "100% 3467/3467 [08:20<00:00, 6.97it/s]\n",
    "  0:08:21.661816 <- Total Agg fxn time time elapsed\n",
    "\n",
    "\n",
    "# .groupby() + dask.delayed(....to_numpy())\n",
    "100% 3467/3467 [02:31<00:00, 23.08it/s]\n",
    "  Wall time: 2min 38s\n",
    "\n",
    "\n",
    "# masks with dask.delayed():\n",
    "#  This is 2x faster than serial processing, but .groupby() + dask.delayed() is much faster\n",
    "100% 3467/3467 [00:11<00:00, 299.85it/s]\n",
    "05:44:20 | INFO | \"Define new C1 df DAG in dask\"\n",
    "05:44:20 | INFO | \"COMPUTE new C1 df START\"\n",
    "05:48:20 | INFO | \"COMPUTE new C1 df DONE\"\n",
    "05:48:20 | INFO | \"  0:04:11.393036 <- Total Agg fxn time time elapsed\"\n",
    "CPU times: user 4min 33s, sys: 24.3 s, total: 4min 57s\n",
    "Wall time: 4min 12s\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca94c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:50:55 | INFO | \"Start C1 - posts + comments + sub descriptions with format: `serial`\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2705947456874682ae1eb88b5e30e44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/705963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   4976\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4978\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set style so that we can try output & time in either format\n",
    "AGG_STYLE = cfg_agg_embeddings.config_dict['agg_style']  # serial v. dask.delayed\n",
    "\n",
    "info(f\"Start C1 - posts + comments + sub descriptions with format: `{AGG_STYLE}`\")\n",
    "t_start_agg_post_c1 = datetime.utcnow()\n",
    "\n",
    "l_df_c1_weights = list()\n",
    "\n",
    "if AGG_STYLE == 'serial':\n",
    "    for s_id, df_ in tqdm(\n",
    "        df_v_pc_weighted.groupby('subreddit_id'),\n",
    "        ascii=True, mininterval=5,\n",
    "    ):\n",
    "        df_.loc[:, l_embedding_cols] = np.add(\n",
    "            df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "            df_[l_embedding_cols]\n",
    "        )\n",
    "        l_df_c1_weights.append(df_)\n",
    "        del df_\n",
    "\n",
    "    info(f\"Create new C1 df\")\n",
    "    df_posts_agg_c1 = pd.concat(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "elif AGG_STYLE == 'dask_delayed':\n",
    "    for s_id, df_ in tqdm(\n",
    "        df_v_pc_weighted.groupby('subreddit_id'),\n",
    "        ascii=True, mininterval=5,\n",
    "    ):\n",
    "        df_pc_embeddings_ = dask.delayed(np.add)(\n",
    "            # df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "            dask.delayed(df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy()),\n",
    "            dask.delayed(df_[l_embedding_cols])\n",
    "        )\n",
    "        l_df_c1_weights.append(\n",
    "            dask.delayed(pd.concat)([dask.delayed(df_[l_ix_post_level]), df_pc_embeddings_], ignore_index=False, axis=1)\n",
    "        )\n",
    "\n",
    "    info(f\"Define new posts C1 df DAG in dask\")\n",
    "    df_posts_agg_c1_delayed = dask.delayed(pd.concat)(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "    info(f\"COMPUTE new C1 df START\")\n",
    "    df_posts_agg_c1 = df_posts_agg_c1_delayed.compute()\n",
    "    info(f\"COMPUTE new C1 df DONE\")\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(f'Other agg style not implemented: {AGG_STYLE}')\n",
    "\n",
    "    \n",
    "r_, c_ = df_posts_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_posts_agg_c1-rows\": r_,\n",
    "        f\"df_posts_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_:,.0f} rows, {c_:,.0f} cols\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_pc_c1 = elapsed_time(start_time=t_start_agg_post_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_posts_agg_c1_no_delay',\n",
    "                  t_agg_pc_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "info(f\"C1 - post level complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9388a1b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_posts_agg_c1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-858965b01c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_posts_agg_c1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_posts_agg_c1' is not defined"
     ]
    }
   ],
   "source": [
    "df_posts_agg_c1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_agg_c1.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3662d9a",
   "metadata": {},
   "source": [
    "### Save post-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b737333",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dfs_to_save = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c495e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_posts_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_posts_agg_c1_{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_posts_agg_c1,\n",
    "    d_dfs_to_save['df_posts_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_posts_agg_c1']['local'], artifact_path='df_posts_agg_c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b49dae",
   "metadata": {},
   "source": [
    "# Aggregate to Subreddit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3675225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_for_embeddings[df_posts_for_embeddings[c_post_embedding_count] >= 1][c_post_embedding_count].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_for_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bacfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_for_embeddings.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9781ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_subs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7186f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# first, figure out how many posts each subreddit has\n",
    "info(f\"Count posts per subreddit...\")\n",
    "c_post_embedding_count = 'posts_for_embeddings_count'\n",
    "\n",
    "\n",
    "df_posts_for_embedding_count = (\n",
    "    df_posts_agg_c1\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .agg(**{c_post_embedding_count: ('post_id', 'nunique')})\n",
    ")\n",
    "# fill subs that have no posts\n",
    "df_posts_for_embedding_count = pd.concat(\n",
    "    [\n",
    "        df_posts_for_embedding_count, \n",
    "        df_v_subs[\n",
    "            ~df_v_subs['subreddit_id'].isin(df_posts_agg_c1['subreddit_id'])\n",
    "        ][l_ix_sub_level].assign(**{c_post_embedding_count: 0})\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# min_posts >= -> regular mean. If it's less than this, then mix in subreddit_description into average\n",
    "n_min_posts_for_regular_mean = 3\n",
    "subreddits_above_n_ = (\n",
    "    df_posts_for_embedding_count\n",
    "    [df_posts_for_embedding_count[c_post_embedding_count] >= n_min_posts_for_regular_mean]\n",
    "    ['subreddit_id']\n",
    ")\n",
    "subreddits_below_n_ = set(df_v_subs['subreddit_id']) - set(subreddits_above_n_)\n",
    "mask_min_posts_for_reg_mean = df_posts_agg_c1['subreddit_id'].isin(subreddits_above_n_)\n",
    "\n",
    "\n",
    "info(f\"SUBREDDIT-LEVEL C1 - posts + comments + sub descriptions\")\n",
    "t_start_agg_subs_c1 = datetime.utcnow()\n",
    "\n",
    "# 3+ posts: simple mean()\n",
    "info(f\"Mean for subs above threshold: {n_min_posts_for_regular_mean}\")\n",
    "df_subs_agg_c1_Nplus = (\n",
    "    df_posts_agg_c1[mask_min_posts_for_reg_mean]\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# calculate mean for all other subs: add UNWEIGHTED subreddit_description into averages\n",
    "info(f\"Calculating mean for subs BELOW post threshold...\")\n",
    "df_subs_agg_c1_Nbelow = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df_posts_agg_c1[~mask_min_posts_for_reg_mean],\n",
    "            df_v_subs[df_v_subs['subreddit_id'].isin(subreddits_below_n_)]\n",
    "        ]\n",
    "    )\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "info(f\"Combining all subreddits...\")\n",
    "df_subs_agg_c1 = (\n",
    "    df_posts_for_embedding_count\n",
    "    .merge(\n",
    "        pd.concat([df_subs_agg_c1_Nplus, df_subs_agg_c1_Nbelow]),\n",
    "        how='outer',\n",
    "        on=l_ix_sub_level\n",
    "    )\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "\n",
    "# Check for dupes\n",
    "assert(len(df_subs_agg_c1) == df_subs_agg_c1['subreddit_id'].nunique()), f\"Found duplicate subreddit_ids\"\n",
    "\n",
    "r_, c_ = df_subs_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_subs_agg_c1-rows\": r_,\n",
    "        f\"df_subs_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_:,.0f} rows, {c_:,.0f} cols\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_subs_c1 = elapsed_time(start_time=t_start_agg_subs_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_subs_agg_c1',\n",
    "                  t_agg_subs_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "info(f\"  <- df_subs_agg_c1.shape (posts + comments + sub description)\")\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_agg_c1.iloc[-8:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_agg_c1.iloc[10:18, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1548d",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "This one we can save as a pandas df, no need to split it into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_subs_agg_c1.to_parquet(\n",
    "#     path_this_model / f\"df_subs_agg_c1-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}.parquet\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37519bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_subs_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_subs_agg_c1-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_subs_agg_c1,\n",
    "    d_dfs_to_save['df_subs_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_subs_agg_c1']['local'], artifact_path='df_subs_agg_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f245c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish logging total time + end mlflow run\n",
    "total_fxn_time = elapsed_time(start_time=t_start_agg_embed, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-full_aggregation_fxn_minutes',\n",
    "                  total_fxn_time / timedelta(minutes=1)\n",
    "                  )\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac06bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a374c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
