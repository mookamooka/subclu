{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e839c01",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "**2021-10-04**: We have a new batch of COMMENTS for v0.4.0. Need to process them.\n",
    "\n",
    "Diff from before: This time instead of processing ALL the comment files in one VM, I'm going to try to process only a few at a time (e.g., 14) so that I can run some batches in parallel (in multiple machines or multiple GPUs for machines with multiple GPUs).\n",
    "\n",
    "---\n",
    "Provenance / previous<br>\n",
    "~2021-09-28: Run inference on posts for v0.4.0 POSTS~\n",
    "\n",
    "Diff from before: instead of only using `text` (post title + post body), we'll be using multiple columns to concat and get the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa35a34",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255e7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb88e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25bba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c2a9a",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3906cc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10eca1b",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3a368c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86ef04",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b517a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7128d6",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5032fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.0 inputs - Top Subreddits (no Geo) + Geo-relevant subs, comments: TBD',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-09-24',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-09-27',\n",
       " 'folder_comments_text_and_meta': 'comments/top/2021-10-04'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v040 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.0_19k_top_subs_and_geo_relevant_2021_09_27',\n",
    "#     config_name='top_subreddits_2021_07_16',\n",
    ")\n",
    "config_data_v040.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40829ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_inference'\n",
    "\n",
    "# Add or over-ride configs here\n",
    "bucket_name = config_data_v040.config_dict['bucket_name']\n",
    "subreddits_path = None  # config_data_v040.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = None  # config_data_v040.config_dict['folder_posts_text_and_meta']\n",
    "comments_path = config_data_v040.config_dict['folder_comments_text_and_meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357ea2f",
   "metadata": {},
   "source": [
    "# Test with batching function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b113891",
   "metadata": {},
   "source": [
    "### New function (with batching)\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Added new parameter to sample only first N files (we'll process each file individually)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab09a6",
   "metadata": {},
   "source": [
    "### Notes on batch & characters:\n",
    "\n",
    "Comments tend to be shorter, so we can usually run larger batches. A batch of `6,000` can still result in `OOM` errors, so go lower than that.\n",
    "```python\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=5000,\n",
    "    tf_limit_first_n_chars=900,\n",
    "```\n",
    "\n",
    "Posts tend to be longer, so we're better off running smaller batches:\n",
    "```python\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=2400,\n",
    "    tf_limit_first_n_chars=900,\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0a00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:21:46 | INFO | \"Start vectorize function\"\n",
      "22:21:46 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-10-04_222146\"\n",
      "22:21:46 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "22:21:46 | INFO | \"  Saving config to local path...\"\n",
      "22:21:46 | INFO | \"  Logging config to mlflow...\"\n",
      "22:21:47 | INFO | \"Loading model use_multilingual...\"\n",
      "22:21:56 | INFO | \"  0:00:08.692843 <- Load TF HUB model time elapsed\"\n",
      "22:21:56 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "22:21:56 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "22:21:56 | INFO | \"-- Loading & vectorizing COMMENTS in files: 2 --\n",
      "Expected batch size: 6000\"\n",
      "22:21:56 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]22:21:56 | INFO | \"Processing: comments/top/2021-10-04/000000000000.parquet\"\n",
      "22:22:00 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id', 'comment_id']\"\n",
      "22:22:00 | INFO | \"col_text: comment_body_text\"\n",
      "22:22:00 | INFO | \"lowercase_text: False\"\n",
      "22:22:00 | INFO | \"limit_first_n_chars: 900\"\n",
      "22:22:00 | INFO | \"limit_first_n_chars_retry: 600\"\n",
      "22:22:01 | INFO | \"Getting embeddings in batches of size: 6000\"\n",
      "100%|#########################################| 116/116 [01:58<00:00,  1.03s/it]\n",
      "22:24:03 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 692,592 Rows by 516 Cols\"\n",
      " 50%|#####     | 1/2 [02:19<02:19, 139.33s/it]22:24:15 | INFO | \"Processing: comments/top/2021-10-04/000000000001.parquet\"\n",
      "22:24:19 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id', 'comment_id']\"\n",
      "22:24:19 | INFO | \"col_text: comment_body_text\"\n",
      "22:24:19 | INFO | \"lowercase_text: False\"\n",
      "22:24:19 | INFO | \"limit_first_n_chars: 900\"\n",
      "22:24:19 | INFO | \"limit_first_n_chars_retry: 600\"\n",
      "22:24:19 | INFO | \"Getting embeddings in batches of size: 6000\"\n",
      "100%|#########################################| 100/100 [01:50<00:00,  1.10s/it]\n",
      "22:26:12 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 594,069 Rows by 516 Cols\"\n",
      "100%|##########| 2/2 [04:28<00:00, 134.12s/it]\n",
      "22:26:24 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "22:26:47 | INFO | \"  0:05:01.711163 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_full_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=6000,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    # Sampling/batching files or rows\n",
    "    n_sample_comment_files=2,\n",
    "    # n_sample_comments=49100,\n",
    "    # n_sample_posts=9500,\n",
    "    get_embeddings_verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fc83d",
   "metadata": {},
   "source": [
    "### Test on a slice of files\n",
    "\n",
    "The previous `n_sample_comment_files` would always sample the first N files, but we didn't check whether file list was sorted.\n",
    "\n",
    "```\n",
    "# # TODO(djb): blobs can't be sorted, so maybe I should save files to local cache first...\n",
    "```\n",
    "With new refactoring: \n",
    "- I sort list to ensure consistency on each run \n",
    "- add slice start & end parameters to pick arbitrary files in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b3ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run(status='KILLED')\n",
    "\n",
    "# vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "#     model_name='use_multilingual',\n",
    "#     run_name=f\"posts_as_comments_full_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "#     mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "#     tokenize_lowercase=False,\n",
    "    \n",
    "#     bucket_name=bucket_name,\n",
    "#     subreddits_path=None,\n",
    "#     posts_path=None,\n",
    "#     comments_path=comments_path,\n",
    "\n",
    "#     # TF batches\n",
    "#     tf_batch_inference_rows=7000,\n",
    "#     tf_limit_first_n_chars=900,\n",
    "    \n",
    "#     # Sampling FILES\n",
    "#     # n_sample_comment_files=2,\n",
    "#     n_comment_files_slice_start=2,\n",
    "#     n_comment_files_slice_end=4,\n",
    "    \n",
    "#     # Sampling ROWS\n",
    "#     # n_sample_comments=49100,\n",
    "#     # n_sample_posts=9500,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41737cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47e6f1",
   "metadata": {},
   "source": [
    "## Re-do with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e20a31",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89ba1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:44:31 | INFO | \"Start vectorize function\"\n",
      "01:44:31 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-10-05_014431\"\n",
      "01:44:32 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "01:44:32 | INFO | \"  Saving config to local path...\"\n",
      "01:44:32 | INFO | \"  Logging config to mlflow...\"\n",
      "01:44:33 | INFO | \"Loading model use_multilingual...\"\n",
      "01:44:36 | INFO | \"  0:00:03.205714 <- Load TF HUB model time elapsed\"\n",
      "01:44:36 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "01:44:36 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "01:44:36 | INFO | \"-- Loading & vectorizing COMMENTS in files: 15 --\n",
      "Expected batch size: 3200\"\n",
      "01:44:36 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]01:44:36 | INFO | \"Processing: comments/top/2021-10-04/000000000000.parquet\"\n",
      "01:44:40 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 217/217 [02:28<00:00,  1.46it/s]\n",
      "01:47:12 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 692,592 Rows by 516 Cols\"\n",
      "  7%|6         | 1/15 [02:47<39:10, 167.87s/it]01:47:24 | INFO | \"Processing: comments/top/2021-10-04/000000000001.parquet\"\n",
      "01:47:27 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 186/186 [02:15<00:00,  1.37it/s]\n",
      "01:49:45 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 594,069 Rows by 516 Cols\"\n",
      " 13%|#3        | 2/15 [05:21<34:31, 159.32s/it]01:49:57 | INFO | \"Processing: comments/top/2021-10-04/000000000002.parquet\"\n",
      "01:50:01 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 161/161 [02:09<00:00,  1.24it/s]\n",
      "01:58:08 | INFO | \"  Saving to local: df_vect_comments/000000000004 | 513,754 Rows by 516 Cols\"\n",
      " 33%|###3      | 5/15 [13:42<27:02, 162.28s/it]01:58:19 | INFO | \"Processing: comments/top/2021-10-04/000000000005.parquet\"\n",
      "01:58:22 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 189/189 [02:22<00:00,  1.33it/s]\n",
      "02:00:48 | INFO | \"  Saving to local: df_vect_comments/000000000005 | 603,690 Rows by 516 Cols\"\n",
      " 40%|####      | 6/15 [16:23<24:15, 161.76s/it]02:01:00 | INFO | \"Processing: comments/top/2021-10-04/000000000006.parquet\"\n",
      "02:01:03 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 234/234 [02:40<00:00,  1.46it/s]\n",
      "02:03:48 | INFO | \"  Saving to local: df_vect_comments/000000000006 | 746,500 Rows by 516 Cols\"\n",
      " 47%|####6     | 7/15 [19:24<22:23, 167.93s/it]02:04:00 | INFO | \"Processing: comments/top/2021-10-04/000000000007.parquet\"\n",
      "02:04:04 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 245/245 [02:51<00:00,  1.43it/s]\n",
      "02:06:59 | INFO | \"  Saving to local: df_vect_comments/000000000007 | 783,264 Rows by 516 Cols\"\n",
      " 53%|#####3    | 8/15 [22:35<20:27, 175.42s/it]02:07:12 | INFO | \"Processing: comments/top/2021-10-04/000000000008.parquet\"\n",
      "02:07:15 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 216/216 [02:35<00:00,  1.39it/s]\n",
      "02:09:54 | INFO | \"  Saving to local: df_vect_comments/000000000008 | 689,359 Rows by 516 Cols\"\n",
      " 60%|######    | 9/15 [25:29<17:30, 175.11s/it]02:10:06 | INFO | \"Processing: comments/top/2021-10-04/000000000009.parquet\"\n",
      "02:10:09 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 216/216 [02:36<00:00,  1.38it/s]\n",
      "02:12:49 | INFO | \"  Saving to local: df_vect_comments/000000000009 | 690,616 Rows by 516 Cols\"\n",
      " 67%|######6   | 10/15 [28:25<14:35, 175.16s/it]02:13:01 | INFO | \"Processing: comments/top/2021-10-04/000000000010.parquet\"\n",
      "02:13:04 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 165/165 [02:00<00:00,  1.36it/s]\n",
      "02:15:08 | INFO | \"  Saving to local: df_vect_comments/000000000010 | 526,691 Rows by 516 Cols\"\n",
      " 73%|#######3  | 11/15 [30:42<10:54, 163.57s/it]02:15:19 | INFO | \"Processing: comments/top/2021-10-04/000000000011.parquet\"\n",
      "02:15:22 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 189/189 [02:14<00:00,  1.41it/s]\n",
      "02:17:39 | INFO | \"  Saving to local: df_vect_comments/000000000011 | 604,523 Rows by 516 Cols\"\n",
      " 80%|########  | 12/15 [33:14<07:59, 159.97s/it]02:17:50 | INFO | \"Processing: comments/top/2021-10-04/000000000012.parquet\"\n",
      "02:17:54 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 227/227 [02:46<00:00,  1.36it/s]\n",
      "02:20:45 | INFO | \"  Saving to local: df_vect_comments/000000000012 | 725,922 Rows by 516 Cols\"\n",
      " 87%|########6 | 13/15 [36:20<05:35, 167.99s/it]02:20:57 | INFO | \"Processing: comments/top/2021-10-04/000000000013.parquet\"\n",
      "02:21:01 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 277/277 [03:13<00:00,  1.43it/s]\n",
      "02:24:20 | INFO | \"  Saving to local: df_vect_comments/000000000013 | 884,792 Rows by 516 Cols\"\n",
      " 93%|#########3| 14/15 [39:56<03:02, 182.36s/it]02:24:32 | INFO | \"Processing: comments/top/2021-10-04/000000000014.parquet\"\n",
      "02:24:36 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 214/214 [02:36<00:00,  1.37it/s]\n",
      "02:27:15 | INFO | \"  Saving to local: df_vect_comments/000000000014 | 682,265 Rows by 516 Cols\"\n",
      "100%|##########| 15/15 [42:51<00:00, 171.43s/it]\n",
      "02:27:28 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "02:30:28 | INFO | \"  0:45:56.500533 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"comments_batch_01-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=3200,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    # Sampling FILES\n",
    "    n_sample_comment_files=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd474063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55156"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9b5b5",
   "metadata": {},
   "source": [
    "### Batch two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:29:14 | INFO | \"Start vectorize function\"\n",
      "04:29:14 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-10-05_042914\"\n",
      "04:29:14 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "04:29:15 | INFO | \"  Saving config to local path...\"\n",
      "04:29:15 | INFO | \"  Logging config to mlflow...\"\n",
      "04:29:15 | INFO | \"Loading model use_multilingual...\"\n",
      "04:29:17 | INFO | \"  0:00:02.476168 <- Load TF HUB model time elapsed\"\n",
      "04:29:17 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "04:29:17 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "04:29:17 | INFO | \"-- Loading & vectorizing COMMENTS in files: 59 --\n",
      "Expected batch size: 3200\"\n",
      "04:29:17 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000000.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000001.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000002.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000003.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000004.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000005.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000006.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000007.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000008.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000009.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000010.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000011.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000012.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000013.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000014.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000015.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000016.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000017.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000018.parquet --\"\n",
      "04:29:17 | INFO | \"    -- Skipping file: comments/top/2021-10-04/000000000019.parquet --\"\n",
      "04:29:18 | INFO | \"Processing: comments/top/2021-10-04/000000000020.parquet\"\n",
      "04:29:22 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 225/225 [02:42<00:00,  1.38it/s]\n",
      "04:32:09 | INFO | \"  Saving to local: df_vect_comments/000000000020 | 718,873 Rows by 516 Cols\"\n",
      " 36%|###5      | 21/59 [03:04<05:32,  8.76s/it]04:32:22 | INFO | \"Processing: comments/top/2021-10-04/000000000021.parquet\"\n",
      "04:32:26 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 205/205 [02:36<00:00,  1.31it/s]\n",
      "04:35:06 | INFO | \"  Saving to local: df_vect_comments/000000000021 | 653,758 Rows by 516 Cols\"\n",
      " 37%|###7      | 22/59 [06:00<11:59, 19.45s/it]04:35:18 | INFO | \"Processing: comments/top/2021-10-04/000000000022.parquet\"\n",
      "04:35:23 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 235/235 [02:58<00:00,  1.31it/s]\n",
      "04:38:26 | INFO | \"  Saving to local: df_vect_comments/000000000022 | 749,485 Rows by 516 Cols\"\n",
      " 39%|###8      | 23/59 [09:21<20:44, 34.56s/it]04:38:39 | INFO | \"Processing: comments/top/2021-10-04/000000000023.parquet\"\n",
      "04:38:43 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 195/195 [02:24<00:00,  1.34it/s]\n",
      "04:41:11 | INFO | \"  Saving to local: df_vect_comments/000000000023 | 621,247 Rows by 516 Cols\"\n",
      " 41%|####      | 24/59 [12:06<28:15, 48.45s/it]04:41:24 | INFO | \"Processing: comments/top/2021-10-04/000000000024.parquet\"\n",
      "04:41:28 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 175/175 [02:16<00:00,  1.29it/s]\n",
      "04:43:47 | INFO | \"  Saving to local: df_vect_comments/000000000024 | 557,871 Rows by 516 Cols\"\n",
      " 42%|####2     | 25/59 [14:41<35:25, 62.51s/it]04:43:59 | INFO | \"Processing: comments/top/2021-10-04/000000000025.parquet\"\n",
      "04:44:03 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 216/216 [02:48<00:00,  1.28it/s]\n",
      "04:46:56 | INFO | \"  Saving to local: df_vect_comments/000000000025 | 690,956 Rows by 516 Cols\"\n",
      " 44%|####4     | 26/59 [17:50<45:28, 82.70s/it]04:47:09 | INFO | \"Processing: comments/top/2021-10-04/000000000026.parquet\"\n",
      "04:47:13 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 220/220 [02:43<00:00,  1.34it/s]\n",
      "04:50:01 | INFO | \"  Saving to local: df_vect_comments/000000000026 | 700,838 Rows by 516 Cols\"\n",
      " 46%|####5     | 27/59 [20:56<54:12, 101.64s/it]04:50:14 | INFO | \"Processing: comments/top/2021-10-04/000000000027.parquet\"\n",
      "04:50:18 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 206/206 [02:41<00:00,  1.28it/s]\n",
      "04:53:03 | INFO | \"  Saving to local: df_vect_comments/000000000027 | 658,829 Rows by 516 Cols\"\n",
      " 47%|####7     | 28/59 [23:57<1:01:08, 118.33s/it]04:53:15 | INFO | \"Processing: comments/top/2021-10-04/000000000028.parquet\"\n",
      "04:53:19 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 179/179 [02:16<00:00,  1.31it/s]\n",
      "04:55:39 | INFO | \"  Saving to local: df_vect_comments/000000000028 | 570,328 Rows by 516 Cols\"\n",
      " 49%|####9     | 29/59 [26:32<1:03:24, 126.81s/it]04:55:51 | INFO | \"Processing: comments/top/2021-10-04/000000000029.parquet\"\n",
      "04:55:55 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 195/195 [02:35<00:00,  1.25it/s]\n",
      "04:58:34 | INFO | \"  Saving to local: df_vect_comments/000000000029 | 621,208 Rows by 516 Cols\"\n",
      " 51%|#####     | 30/59 [29:28<1:07:04, 138.79s/it]04:58:46 | INFO | \"Processing: comments/top/2021-10-04/000000000030.parquet\"\n",
      "04:58:50 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 188/188 [02:31<00:00,  1.24it/s]\n",
      "05:01:25 | INFO | \"  Saving to local: df_vect_comments/000000000030 | 598,431 Rows by 516 Cols\"\n",
      " 53%|#####2    | 31/59 [32:19<1:08:41, 147.20s/it]05:01:37 | INFO | \"Processing: comments/top/2021-10-04/000000000031.parquet\"\n",
      "05:01:42 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 221/221 [02:50<00:00,  1.30it/s]\n",
      "05:04:36 | INFO | \"  Saving to local: df_vect_comments/000000000031 | 704,766 Rows by 516 Cols\"\n",
      " 54%|#####4    | 32/59 [35:31<1:11:41, 159.33s/it]05:04:49 | INFO | \"Processing: comments/top/2021-10-04/000000000032.parquet\"\n",
      "05:04:54 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 210/210 [02:41<00:00,  1.30it/s]\n",
      "05:07:39 | INFO | \"  Saving to local: df_vect_comments/000000000032 | 670,589 Rows by 516 Cols\"\n",
      " 56%|#####5    | 33/59 [38:34<1:11:55, 165.97s/it]05:07:52 | INFO | \"Processing: comments/top/2021-10-04/000000000033.parquet\"\n",
      "05:07:56 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      " 80%|################################8        | 164/205 [02:04<00:33,  1.22it/s]05:10:11 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[618563,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_37096]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 205/205 [02:46<00:00,  1.23it/s]\n",
      "05:10:46 | INFO | \"  Saving to local: df_vect_comments/000000000033 | 653,889 Rows by 516 Cols\"\n",
      " 58%|#####7    | 34/59 [41:41<1:11:40, 172.02s/it]05:10:59 | INFO | \"Processing: comments/top/2021-10-04/000000000034.parquet\"\n",
      "05:11:03 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 217/217 [02:40<00:00,  1.35it/s]\n",
      "05:13:48 | INFO | \"  Saving to local: df_vect_comments/000000000034 | 691,548 Rows by 516 Cols\"\n",
      " 59%|#####9    | 35/59 [44:42<1:09:55, 174.81s/it]05:14:01 | INFO | \"Processing: comments/top/2021-10-04/000000000035.parquet\"\n",
      "05:14:06 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 218/218 [02:49<00:00,  1.28it/s]\n",
      "05:16:59 | INFO | \"  Saving to local: df_vect_comments/000000000035 | 695,488 Rows by 516 Cols\"\n",
      " 61%|######1   | 36/59 [47:54<1:08:52, 179.69s/it]05:17:12 | INFO | \"Processing: comments/top/2021-10-04/000000000036.parquet\"\n",
      "05:17:16 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      " 98%|########################################2| 217/221 [02:44<00:03,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"comments_batch_01-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=3200,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    # Sampling FILES\n",
    "    # n_sample_comment_files=15,\n",
    "    n_comment_files_slice_start=20,\n",
    "    n_comment_files_slice_end=62,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4141a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do stuff 20\n",
      "do stuff 21\n",
      "do stuff 22\n",
      "do stuff 23\n"
     ]
    }
   ],
   "source": [
    "slice_start = 20\n",
    "slice_end = 24\n",
    "\n",
    "for i, f_ in enumerate(np.arange(50)):\n",
    "    if not(slice_start <= i < slice_end):\n",
    "        # print(f\"skip: {i}\")\n",
    "        continue\n",
    "    print(f\"do stuff {i}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b46ad",
   "metadata": {},
   "source": [
    "Summary for posts (logs say comments because we used the hack to batch comments):\n",
    "\n",
    "**Params**:\n",
    "```python\n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2450,\n",
    "    tf_limit_first_n_chars=900,\n",
    "```\n",
    "\n",
    "\n",
    "**Timing**\n",
    "```bash\n",
    "23:19:04 | INFO | \"  Saving to local: df_vect_posts/000000000026 | 361,954 Rows by 515 Cols\"\n",
    "100%|##########| 27/27 [1:19:38<00:00, 177.00s/it]\n",
    "23:19:14 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
    "23:22:43 | INFO | \"  1:23:50.042083 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bf4b2",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6eff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"comments_lower_case-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=3200,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    # Sampling FILES\n",
    "    # n_sample_comment_files=15,\n",
    "    # n_comment_files_slice_start=20,\n",
    "    # n_comment_files_slice_end=62,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d39c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44e975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb61fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bcd11d9",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c309ff83",
   "metadata": {},
   "source": [
    "### Notes on previous function (all in memory):\n",
    "- 60GB of RAM wasn't good enough for 19Million comments _lol_ (also might've run into memory leaks in the GPU)\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8d995",
   "metadata": {},
   "source": [
    "### New batching fxn\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n",
    "\n",
    "\n",
    "Notes on new fxn to batch posts as if they're comments. (Because batching logic is only implemented for comments)\n",
    "\n",
    "```python\n",
    "    # Hack: Rename cols so that I can process `posts` as a batch of comments\n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdef3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438dca0",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False (legacy fse/fasttext)\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ba87b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
