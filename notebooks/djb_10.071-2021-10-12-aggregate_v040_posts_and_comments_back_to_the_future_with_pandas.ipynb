{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b753486",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "\n",
    "2021-10-06:\n",
    "We're going back to pandas now that I have the VM machine with a ton of RAM.\n",
    "\n",
    "There might be some tweaks needed to batch a few subreddits at a time, but at least we can get more consistent state/progress than with `dask`.\n",
    "\n",
    "---\n",
    "2021-10-06:\n",
    "The job with dask failed silently - even with 3+ TB of RAM.  `Dask` was reporting that saving was complete - but it only saved one `parquet` file instead of hundreds of files.\n",
    "\n",
    "New direction: now that I have access to a large VM, I might as well try to go back and do the calculations in memory (in pandas).\n",
    "\n",
    "\n",
    "-- \n",
    "2021-10-05:\n",
    "I ran into memory errors with 600GB or RAM, so here's a try with 1.4TB... if this doesn't work. Then I don't know what will...\n",
    "\n",
    "---\n",
    "\n",
    "2021-08-10: Finally completed testing with sampling <= 10 files. Now ready to run process on full data!\n",
    "\n",
    "Ended up doing it all in dask + pandas + numpy because of problems installing `cuDF`.\n",
    "\n",
    "---\n",
    "2021-08-02: Now that I'm processing millions of comments and posts, I need to re-write the functions to try to do some work in parallel and reduce the amount of data loaded in RAM.\n",
    "\n",
    "- `Dask` seems like a great option to load data and only compute some of it as needed.\n",
    "- `cuDF` could be a way to speed up some computation using GPUs\n",
    "- `Dask-delayed` could be a way to create a task DAG lazily before computing all the aggregates.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In notebook 09 I combined embeddings from posts & subreddits (`djb_09.00-combine_post_and_comments_and_visualize_for_presentation.ipynb`).\n",
    "\n",
    "In this notebook I'll be testing functions that include mlflow so that it's easier to try a lot of different weights to find better respresentations.\n",
    "\n",
    "Take embeddings created by other models & combine them:\n",
    "```\n",
    "new post embeddings = post + comments + subreddit description\n",
    "\n",
    "new subreddit embeddings = new posts (weighted by post length or upvotes?)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65caad",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726dbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6914f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "from subclu.models import aggregate_embeddings_pd\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0c8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666102e",
   "metadata": {},
   "source": [
    "# Set sqlite database as MLflow URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6876a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62741b",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b4e9545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d6eb5",
   "metadata": {},
   "source": [
    "## Get runs that we can use for embeddings aggregation jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf8f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 ms, sys: 6.64 ms, total: 283 ms\n",
      "Wall time: 282 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(78, 132)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_mlf_runs =  mlf.search_all_runs(experiment_ids=[13, 14, 15, 16])\n",
    "df_mlf_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accf101e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 132)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_finished = df_mlf_runs['status'] == 'FINISHED'\n",
    "mask_output_over_1M_rows = (\n",
    "    (df_mlf_runs['metrics.df_vect_posts_rows'] >= 1e5) |\n",
    "    (df_mlf_runs['metrics.df_vect_comments'] >= 1e5)\n",
    ")\n",
    "# df_mlf_runs[mask_finished].shape\n",
    "\n",
    "df_mlf_use_for_agg = df_mlf_runs[mask_output_over_1M_rows]\n",
    "df_mlf_use_for_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2629cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_6ce07_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >run id</th>        <th class=\"col_heading level0 col1\" >experiment id</th>        <th class=\"col_heading level0 col2\" >start time</th>        <th class=\"col_heading level0 col3\" >metrics.df vect comments</th>        <th class=\"col_heading level0 col4\" >metrics.vectorizing time minutes full function</th>        <th class=\"col_heading level0 col5\" >metrics.total comment files processed</th>        <th class=\"col_heading level0 col6\" >metrics.vectorizing time minutes comments</th>        <th class=\"col_heading level0 col7\" >params.tf batch inference rows</th>        <th class=\"col_heading level0 col8\" >params.n sample comment files</th>        <th class=\"col_heading level0 col9\" >params.n comment files slice start</th>        <th class=\"col_heading level0 col10\" >params.n comment files slice end</th>        <th class=\"col_heading level0 col11\" >tags.mlflow.runName</th>        <th class=\"col_heading level0 col12\" >tags.model version</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_6ce07_level0_row0\" class=\"row_heading level0 row0\" >52</th>\n",
       "                        <td id=\"T_6ce07_row0_col0\" class=\"data row0 col0\" >deb3454ece2a4a8d8e4149c2d8494c0d</td>\n",
       "                        <td id=\"T_6ce07_row0_col1\" class=\"data row0 col1\" >14</td>\n",
       "                        <td id=\"T_6ce07_row0_col2\" class=\"data row0 col2\" >2021-10-05 01:44:32.386000+00:00</td>\n",
       "                        <td id=\"T_6ce07_row0_col3\" class=\"data row0 col3\" >10,121,046.00</td>\n",
       "                        <td id=\"T_6ce07_row0_col4\" class=\"data row0 col4\" >45.94</td>\n",
       "                        <td id=\"T_6ce07_row0_col5\" class=\"data row0 col5\" >15.00</td>\n",
       "                        <td id=\"T_6ce07_row0_col6\" class=\"data row0 col6\" >39.11</td>\n",
       "                        <td id=\"T_6ce07_row0_col7\" class=\"data row0 col7\" >3200</td>\n",
       "                        <td id=\"T_6ce07_row0_col8\" class=\"data row0 col8\" >15</td>\n",
       "                        <td id=\"T_6ce07_row0_col9\" class=\"data row0 col9\" >None</td>\n",
       "                        <td id=\"T_6ce07_row0_col10\" class=\"data row0 col10\" >None</td>\n",
       "                        <td id=\"T_6ce07_row0_col11\" class=\"data row0 col11\" >comments_batch_01-2021-10-05_014431</td>\n",
       "                        <td id=\"T_6ce07_row0_col12\" class=\"data row0 col12\" >None</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6ce07_level0_row1\" class=\"row_heading level0 row1\" >53</th>\n",
       "                        <td id=\"T_6ce07_row1_col0\" class=\"data row1 col0\" >5f10cd75334142168a6ebb787e477c1f</td>\n",
       "                        <td id=\"T_6ce07_row1_col1\" class=\"data row1 col1\" >14</td>\n",
       "                        <td id=\"T_6ce07_row1_col2\" class=\"data row1 col2\" >2021-10-05 00:22:20.334000+00:00</td>\n",
       "                        <td id=\"T_6ce07_row1_col3\" class=\"data row1 col3\" >13,558,304.00</td>\n",
       "                        <td id=\"T_6ce07_row1_col4\" class=\"data row1 col4\" >57.33</td>\n",
       "                        <td id=\"T_6ce07_row1_col5\" class=\"data row1 col5\" >20.00</td>\n",
       "                        <td id=\"T_6ce07_row1_col6\" class=\"data row1 col6\" >47.64</td>\n",
       "                        <td id=\"T_6ce07_row1_col7\" class=\"data row1 col7\" >4200</td>\n",
       "                        <td id=\"T_6ce07_row1_col8\" class=\"data row1 col8\" >20</td>\n",
       "                        <td id=\"T_6ce07_row1_col9\" class=\"data row1 col9\" >None</td>\n",
       "                        <td id=\"T_6ce07_row1_col10\" class=\"data row1 col10\" >None</td>\n",
       "                        <td id=\"T_6ce07_row1_col11\" class=\"data row1 col11\" >comments_batch_01-2021-10-05_002219</td>\n",
       "                        <td id=\"T_6ce07_row1_col12\" class=\"data row1 col12\" >0.4.0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6ce07_level0_row2\" class=\"row_heading level0 row2\" >57</th>\n",
       "                        <td id=\"T_6ce07_row2_col0\" class=\"data row2 col0\" >9a27f9a72cf348c98d50f486abf3b009</td>\n",
       "                        <td id=\"T_6ce07_row2_col1\" class=\"data row2 col1\" >13</td>\n",
       "                        <td id=\"T_6ce07_row2_col2\" class=\"data row2 col2\" >2021-10-04 22:21:46.401000+00:00</td>\n",
       "                        <td id=\"T_6ce07_row2_col3\" class=\"data row2 col3\" >1,286,661.00</td>\n",
       "                        <td id=\"T_6ce07_row2_col4\" class=\"data row2 col4\" >5.03</td>\n",
       "                        <td id=\"T_6ce07_row2_col5\" class=\"data row2 col5\" >2.00</td>\n",
       "                        <td id=\"T_6ce07_row2_col6\" class=\"data row2 col6\" >3.93</td>\n",
       "                        <td id=\"T_6ce07_row2_col7\" class=\"data row2 col7\" >6000</td>\n",
       "                        <td id=\"T_6ce07_row2_col8\" class=\"data row2 col8\" >2</td>\n",
       "                        <td id=\"T_6ce07_row2_col9\" class=\"data row2 col9\" >None</td>\n",
       "                        <td id=\"T_6ce07_row2_col10\" class=\"data row2 col10\" >None</td>\n",
       "                        <td id=\"T_6ce07_row2_col11\" class=\"data row2 col11\" >posts_as_comments_full_text-2021-10-04_222146</td>\n",
       "                        <td id=\"T_6ce07_row2_col12\" class=\"data row2 col12\" >None</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa85c89af90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_multiple_vals = df_mlf_use_for_agg.columns[df_mlf_use_for_agg.nunique(dropna=False) > 1]\n",
    "# len(cols_with_multiple_vals)\n",
    "\n",
    "style_df_numeric(\n",
    "    df_mlf_use_for_agg\n",
    "    [cols_with_multiple_vals]\n",
    "    .drop(['artifact_uri', 'end_time',\n",
    "           # 'start_time',\n",
    "           ], \n",
    "          axis=1)\n",
    "    .dropna(axis='columns', how='all')\n",
    "    .iloc[:, :30]\n",
    "    ,\n",
    "    rename_cols_for_display=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ba7db",
   "metadata": {},
   "source": [
    "# Load configs for aggregation jobs\n",
    "\n",
    "`n_sample_comments_files` and `n_sample_posts_files` allow us to only load a few files at a time (e.g., 2 instead of 50) to test the process end-to-end.\n",
    "\n",
    "---\n",
    "Note that by default `hydra` is a cli tool. If we want to call use it in jupyter, we need to manually initialize configs & compose the configuration. See my custom function `load_config_agg_jupyter`. Also see:\n",
    "- [Notebook with `Hydra` examples in a notebook](https://github.com/facebookresearch/hydra/blob/master/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb).\n",
    "- [Hydra docs, Hydra in Jupyter](https://hydra.cc/docs/next/advanced/jupyter_notebooks/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4e6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=4',     # \n",
    "               'n_sample_comments_files=4',  # 6 is limit for logging unique counts at comment level\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "config_full_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "               'n_sample_posts_files=null', \n",
    "               'n_sample_comments_files=null',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "# config_full_lc_true = AggregateEmbeddingsConfig(\n",
    "#     config_path=\"../config\",\n",
    "#     config_name='aggregate_embeddings',\n",
    "#     overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "#                'n_sample_posts_files=null', \n",
    "#                'n_sample_comments_files=null',\n",
    "#                'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_true',\n",
    "#               ]\n",
    "# )\n",
    "# pprint(config_test_sample_lc_false.config_dict, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caaa51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_test_sample_lc_false.config_flat,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d10543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_configs = pd.DataFrame(\n",
    "    [\n",
    "        config_test_sample_lc_false.config_flat,\n",
    "        # config_test_full_lc_false.config_flat,\n",
    "        config_full_lc_false.config_flat,\n",
    "        # config_full_lc_true.config_flat,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34f56af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>comments_uuid</th>\n",
       "      <th>mlflow_experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       comments_vectorized_mlflow_uuids       posts_vectorized_mlflow_uuids posts_vectorized_mlflow_uuids_lowercase subreddit_meta_vectorized_mlflow_uuids subreddit_meta_vectorized_mlflow_uuids_lowercase  \\\n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "\n",
       "                                                          comments_uuid                 mlflow_experiment  \n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  v0.4.0_use_multi_aggregates_test  \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]       v0.4.0_use_multi_aggregates  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can't use (df_configs.nunique(dropna=False) > 1)\n",
    "#  because when a col's content is a list or something unhashable, we get an error\n",
    "#  so instead we'll check each column individually\n",
    "\n",
    "# cols_with_diffs_config = df_configs.columns[df_configs.nunique(dropna=False) > 1]\n",
    "cols_with_diffs_config = list()\n",
    "for c_ in df_configs.columns:\n",
    "    try:\n",
    "        if df_configs[c_].nunique() > 1:\n",
    "            cols_with_diffs_config.append(c_)\n",
    "    except TypeError:\n",
    "        cols_with_diffs_config.append(c_)\n",
    "        \n",
    "\n",
    "df_configs[cols_with_diffs_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf63405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(config_test_sample_lc_false.config_flat, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab531085",
   "metadata": {},
   "source": [
    "# Run Full data with `lower_case=False`\n",
    "\n",
    "The logic for sampling files and download/`caching` files locally lives in the `mlf` custom function.\n",
    "\n",
    "Caching can save 9+ minutes if we try to download the files from GCS every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a6ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_experiment: \tv0.4.0_use_multi_aggregates\n",
      "n_sample_posts_files: \tNone\n",
      "n_sample_comments_files: \tNone\n",
      "\n",
      "aggregate_params:\n",
      "  min_comment_text_len: \t2\n",
      "  agg_comments_to_post_weight_col: \tNone\n",
      "  agg_post_to_subreddit_weight_col: \tNone\n",
      "  agg_post_post_weight: \t70\n",
      "  agg_post_comment_weight: \t20\n",
      "  agg_post_subreddit_desc_weight: \t10\n",
      "calculate_similarites: \tTrue\n"
     ]
    }
   ],
   "source": [
    "keys_to_check_in_config = ['mlflow_experiment', 'n_sample_posts_files', 'n_sample_comments_files', 'aggregate_params', 'calculate_similarites']\n",
    "\n",
    "for k_ in keys_to_check_in_config:\n",
    "    v_ = config_full_lc_false.config_dict.get(k_)\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"\\n{k_}:\")\n",
    "        [print(f\"  {k2_}: \\t{v2_}\") for k2_, v2_ in v_.items()]\n",
    "    else:\n",
    "        print(f\"{k_}: \\t{v_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26b273cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d2945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:46:05 | INFO | \"== Start run_aggregation() method ==\"\n",
      "10:46:05 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "10:46:05 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "10:46:05 | INFO | \"cpu_count: 160\"\n",
      "10:46:05 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '7.19%', 'memory_total': '3,874,634', 'memory_used': '278,514', 'memory_free': '3,465,918'}\"\n",
      "10:46:05 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-10-12_104605-agg_full_lc_false_pd-2021-10-12_104604\"\n",
      "10:46:05 | INFO | \"  Saving config to local path...\"\n",
      "10:46:05 | INFO | \"  Logging config to mlflow...\"\n",
      "10:46:06 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "10:46:06 | INFO | \"Loading subreddit description embeddings...\"\n",
      "10:46:07 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_subreddits_description\"\n",
      "100%|##########################################| 5/5 [00:00<00:00, 16008.79it/s]\n",
      "10:46:07 | INFO | \"  Parquet files found: 2\"\n",
      "10:46:07 | INFO | \"      19,262 | 514 <- Raw vectorized subreddit description shape\"\n",
      "10:46:07 | INFO | \"  Unique check for subreddit description...\"\n",
      "10:46:07 | INFO | \"Loading POSTS embeddings...\"\n",
      "10:46:08 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_posts\"\n",
      "100%|########################################| 28/28 [00:00<00:00, 33081.83it/s]\n",
      "10:46:08 | INFO | \"  Parquet files found: 27\"\n",
      "10:46:49 | INFO | \"  Getting df_v_posts.shape ...\"\n",
      "10:46:49 | INFO | \"   8,439,672 |  515 <- Raw POSTS shape\"\n",
      "10:46:49 | INFO | \"  Checking that posts are unique...\"\n",
      "10:46:56 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "10:46:56 | INFO | \"  Found 2 run UUIDs with COMMENT embeddings...\"\n",
      "10:46:57 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/5f10cd75334142168a6ebb787e477c1f/artifacts/df_vect_comments\"\n",
      "100%|########################################| 21/21 [00:00<00:00, 24105.20it/s]\n",
      "10:46:57 | INFO | \"  Parquet files found: 20\"\n",
      "10:47:56 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
      "100%|########################################| 40/40 [00:00<00:00, 36464.28it/s]\n",
      "10:47:56 | INFO | \"  Parquet files found: 39\"\n",
      "10:50:40 | INFO | \"  39,901,968 |  516 <- Raw COMMENTS shape\"\n",
      "10:50:40 | INFO | \"  Keep only comments for posts with embeddings\"\n",
      "10:52:02 | INFO | \"  39,901,968 |  516 <- COMMENTS shape, after keeping only comments to loaded posts\"\n",
      "10:53:20 | INFO | \"  0:07:14.227620 <- Total raw embeddings load time elapsed\"\n",
      "10:53:23 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '13.25%', 'memory_used': '513,249'}\"\n",
      "10:53:23 | INFO | \"-- Start _load_metadata() method --\"\n",
      "10:53:23 | INFO | \"Loading POSTS metadata...\"\n",
      "10:53:23 | INFO | \"Reading raw data...\"\n",
      "10:53:23 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-09-27\"\n",
      "100%|##############################| 27/27 [00:00<00:00, 26045.59it/s]\n",
      "10:53:35 | INFO | \"  Applying transformations...\"\n",
      "10:54:13 | INFO | \"  (8439672, 15) <- Raw META POSTS shape\"\n",
      "10:54:13 | INFO | \"Loading subs metadata...\"\n",
      "10:54:13 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "10:54:13 | INFO | \"Reading raw data...\"\n",
      "10:54:13 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-09-24\"\n",
      "100%|#################################| 1/1 [00:00<00:00, 4911.36it/s]\n",
      "10:54:14 | INFO | \"  Applying transformations...\"\n",
      "10:54:24 | INFO | \"  (19262, 91) <- Raw META subreddit description shape\"\n",
      "10:54:24 | INFO | \"Loading COMMENTS metadata...\"\n",
      "10:54:24 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
      "100%|##############################| 59/59 [00:00<00:00, 54268.41it/s]\n",
      "10:54:55 | INFO | \"  (39901968, 8) <- Raw META COMMENTS shape\"\n",
      "10:54:55 | INFO | \"  0:01:31.806193 <- Total metadata loading time elapsed\"\n",
      "10:54:58 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '13.56%', 'memory_used': '525,436'}\"\n",
      "10:54:58 | INFO | \"2 <- Removing comments shorter than 2 characters.\"\n",
      "10:56:15 | INFO | \"  (39760856, 516) <- df_v_comments.shape AFTER removing short comments\"\n",
      "10:56:16 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "10:56:16 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "11:02:22 | INFO | \"  (7029301, 515) <- df_v_com_agg shape after aggregation\"\n",
      "11:02:22 | WARNING | \"  TODO(djb): DELETE df_v_com (raw) to free up memory...\"\n",
      "11:02:22 | INFO | \"  0:06:06.223981 <- Total comments to post agg loading time elapsed\"\n",
      "11:02:25 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '13.95%', 'memory_used': '540,615'}\"\n",
      "11:02:25 | INFO | \"SKIPPING: B = Calculating aggregation for POSTS + COMMENTS\"\n",
      "11:02:25 | INFO | \"-- Start (df_posts_agg_c) _agg_posts_comments_and_sub_descriptions_to_post_level() method --\"\n",
      "11:02:25 | INFO | \"Getting count of comments per post...\"\n",
      "11:03:51 | INFO | \"  7,029,301 <- Posts that need weighted average\"\n",
      "11:03:51 | INFO | \"Create df with weights for weighted-average calculation\"\n",
      "11:04:52 | INFO | \"Get weighted average for POST + COMMENT + SUBREDDIT-META\"\n",
      "100%|##############################| 8439672/8439672 [2:53:22<00:00, 811.32it/s]\n",
      "14:06:46 | INFO | \"  (8439672, 512) <- df_agg_posts_w_sub.shape (only posts with comments)\"\n",
      "14:06:46 | INFO | \"Re-append multi-index so it's the same in original and new output\"\n",
      "14:11:02 | INFO | \"Check that post-ID is unique...\"\n",
      "14:11:12 | INFO | \"  (8439672, 515) <- df_posts_agg_c shape after aggregation\"\n",
      "14:11:12 | INFO | \"  3:08:46.927287 <- Total posts+comments+subs agg time elapsed\"\n",
      "14:11:36 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '16.23%', 'memory_used': '628,739'}\"\n",
      "14:11:38 | INFO | \"-- Start _agg_post_aggregates_to_subreddit_level() method --\"\n",
      "14:11:38 | INFO | \"No column to weight comments, simple mean to roll up posts to subreddit-level...\"\n",
      "14:11:38 | INFO | \"A - posts only\"\n",
      "14:12:35 | INFO | \"  (19192, 514) <- df_subs_agg_a.shape (only posts)\"\n",
      "14:12:35 | INFO | \"C - posts + comments + sub descriptions\"\n",
      "14:13:53 | INFO | \"  (19192, 514) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
      "14:13:53 | INFO | \"  0:02:14.275068 <- Total for all subreddit-level agg time elapsed\"\n",
      "14:13:57 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '14.81%', 'memory_used': '573,701'}\"\n",
      "14:13:58 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "14:13:58 | INFO | \"Dictionary of dfs to log & save (only dfs that have been created):\n",
      "{'df_sub_level_agg_c_post_comments_and_sub_desc': 'df_sub_level_agg_c_post_comments_and_sub_desc', 'df_post_level_agg_c_post_comments_sub_desc': 'df_post_level_agg_c_post_comments_sub_desc', 'df_sub_level_agg_a_post_only': 'df_sub_level_agg_a_post_only'}\"\n",
      "14:13:58 | INFO | \"  Saving config to local path...\"\n",
      "14:13:58 | INFO | \"  Logging config to mlflow...\"\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]14:13:58 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "14:13:58 | INFO | \"  Saving locally...\"\n",
      "14:13:58 | INFO | \"Converting pandas to dask...\"\n",
      "14:13:58 | INFO | \"    77.6 MB <- Memory usage\"\n",
      "14:13:58 | INFO | \"       3\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "14:13:59 | INFO | \"  Logging df shape...\"\n",
      "14:14:01 | INFO | \"  Logging artifact to mlflow...\"\n",
      "14:14:04 | INFO | \"  0:00:06.026222 <- Total for saving & logging ** df_sub_level_agg_c_post_comments_and_sub_desc ** time elapsed\"\n",
      " 33%|###############                              | 1/3 [00:06<00:12,  6.03s/it]14:14:04 | INFO | \"** df_post_level_agg_c_post_comments_sub_desc **\"\n",
      "14:14:04 | INFO | \"  Saving locally...\"\n",
      "14:14:23 | INFO | \"Converting pandas to dask...\"\n",
      "14:14:25 | INFO | \"  34,631.0 MB <- Memory usage\"\n",
      "14:14:25 | INFO | \"     462\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "14:17:53 | INFO | \"  Logging df shape...\"\n",
      "14:17:57 | INFO | \"  Logging artifact to mlflow...\"\n",
      "14:30:17 | INFO | \"  0:16:12.518817 <- Total for saving & logging ** df_post_level_agg_c_post_comments_sub_desc ** time elapsed\"\n",
      " 67%|#############################3              | 2/3 [16:18<09:34, 574.55s/it]14:30:17 | INFO | \"** df_sub_level_agg_a_post_only **\"\n",
      "14:30:17 | INFO | \"  Saving locally...\"\n",
      "14:30:17 | INFO | \"Converting pandas to dask...\"\n",
      "14:30:17 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "14:30:17 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "14:30:18 | INFO | \"  Logging df shape...\"\n",
      "14:30:18 | INFO | \"  Logging artifact to mlflow...\"\n",
      "14:30:19 | INFO | \"  0:00:02.331687 <- Total for saving & logging ** df_sub_level_agg_a_post_only ** time elapsed\"\n",
      "100%|############################################| 3/3 [16:20<00:00, 326.96s/it]\n",
      "14:30:19 | INFO | \"  0:16:21.366025 <- Total for _save_and_log_aggregate_and_similarity_dfs() time elapsed\"\n",
      "14:30:23 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '14.81%', 'memory_used': '573,969'}\"\n",
      "14:30:23 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
      "14:30:23 | INFO | \"A...\"\n",
      "14:30:27 | INFO | \"  (19192, 19192) <- df_subs_agg_a_similarity.shape\"\n",
      "14:40:37 | INFO | \"Merge distance + metadata...\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    job_agg1._send_log_file_to_mlflow()\n",
    "    mlflow.end_run(\"FAILED\")\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    setup_logging()\n",
    "    \n",
    "    del job_agg1\n",
    "    del d_dfs1\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(\"FAILED\")\n",
    "\n",
    "\n",
    "job_agg1 = aggregate_embeddings_pd.AggregateEmbeddings(\n",
    "    run_name=f\"agg_full_lc_false_pd-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    **config_full_lc_false.config_flat\n",
    ")\n",
    "job_agg1.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deb41efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:47:20 | INFO | \"Could NOT log to MLFLow, there's no active run.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_agg1._send_log_file_to_mlflow()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aba2e29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c6724",
   "metadata": {},
   "source": [
    "# Run full data, `lower_case=True`\n",
    "\n",
    "Looks like the problem I ran into with the file being corrupted might've been a problem with downloading the file(s). Fix: delete the local cache and download the files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6daaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# mlflow.end_run(\"FAILED\")\n",
    "# gc.collect()\n",
    "# try:\n",
    "#     # run setup_logging() to remove logging to the file of a failed job\n",
    "#     setup_logging()\n",
    "    \n",
    "#     del job_agg2\n",
    "#     del d_dfs2\n",
    "# except NameError:\n",
    "#     pass\n",
    "# gc.collect()\n",
    "\n",
    "# job_agg2 = AggregateEmbeddings(\n",
    "#     run_name=f\"full_lc_true-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "#     **config_full_lc_true.config_flat\n",
    "# )\n",
    "# job_agg2.run_aggregation()\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed695212",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb05ce",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b517902",
   "metadata": {},
   "source": [
    "### Check computed dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "150 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_, v_ in {k_: v_ for k_, v_ in vars(job_agg1).items() if 'df_' in k_}.items():\n",
    "    print(f\"\\n{k_}\")\n",
    "    try:\n",
    "        print(f\"  {v_.shape}\")\n",
    "        display(v_.iloc[:8, :10])\n",
    "        if not ('meta' in k_):\n",
    "            print(v_.info())\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cb80be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_agg_test._save_and_log_aggregate_and_similarity_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee6c7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2794"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.end_run(\"FAILED\")\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
