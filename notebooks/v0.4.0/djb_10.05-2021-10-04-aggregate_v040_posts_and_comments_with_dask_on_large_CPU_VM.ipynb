{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d7ce7c",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-08-10: Finally completed testing with sampling <= 10 files. Now ready to run process on full data!\n",
    "\n",
    "Ended up doing it all in dask + pandas + numpy because of problems installing `cuDF`.\n",
    "\n",
    "---\n",
    "2021-08-02: Now that I'm processing millions of comments and posts, I need to re-write the functions to try to do some work in parallel and reduce the amount of data loaded in RAM.\n",
    "\n",
    "- `Dask` seems like a great option to load data and only compute some of it as needed.\n",
    "- `cuDF` could be a way to speed up some computation using GPUs\n",
    "- `Dask-delayed` could be a way to create a task DAG lazily before computing all the aggregates.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In notebook 09 I combined embeddings from posts & subreddits (`djb_09.00-combine_post_and_comments_and_visualize_for_presentation.ipynb`).\n",
    "\n",
    "In this notebook I'll be testing functions that include mlflow so that it's easier to try a lot of different weights to find better respresentations.\n",
    "\n",
    "Take embeddings created by other models & combine them:\n",
    "```\n",
    "new post embeddings = post + comments + subreddit description\n",
    "\n",
    "new subreddit embeddings = new posts (weighted by post length or upvotes?)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f86c7",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b2f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e70110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2dad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf3045",
   "metadata": {},
   "source": [
    "# Set sqlite database as MLflow URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e22db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c2015",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0364935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd92c2d",
   "metadata": {},
   "source": [
    "## Get runs that we can use for embeddings aggregation jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5af210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 0 ns, total: 280 ms\n",
      "Wall time: 280 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(118, 104)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_mlf_runs =  mlf.search_all_runs(experiment_ids=[9, '10', 11, 12])\n",
    "df_mlf_runs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66931c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 104)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_finished = df_mlf_runs['status'] == 'FINISHED'\n",
    "mask_output_over_1M_rows = (\n",
    "    (df_mlf_runs['metrics.df_vect_posts_rows'] >= 1e6) |\n",
    "    (df_mlf_runs['metrics.df_vect_comments'] >= 1e6)\n",
    ")\n",
    "# df_mlf_runs[mask_finished].shape\n",
    "\n",
    "df_mlf_use_for_agg = df_mlf_runs[mask_output_over_1M_rows]\n",
    "df_mlf_use_for_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4715d731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_e1f17_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >run id</th>        <th class=\"col_heading level0 col1\" >experiment id</th>        <th class=\"col_heading level0 col2\" >status</th>        <th class=\"col_heading level0 col3\" >start time</th>        <th class=\"col_heading level0 col4\" >metrics.vectorizing time minutes subreddit meta</th>        <th class=\"col_heading level0 col5\" >metrics.vectorizing time minutes full function</th>        <th class=\"col_heading level0 col6\" >metrics.df vect comments</th>        <th class=\"col_heading level0 col7\" >metrics.vectorizing time minutes posts</th>        <th class=\"col_heading level0 col8\" >metrics.df vect subreddits description cols</th>        <th class=\"col_heading level0 col9\" >metrics.df vect posts rows</th>        <th class=\"col_heading level0 col10\" >metrics.df vect subreddits description rows</th>        <th class=\"col_heading level0 col11\" >metrics.vectorizing time minutes comments</th>        <th class=\"col_heading level0 col12\" >metrics.df vect posts cols</th>        <th class=\"col_heading level0 col13\" >metrics.total comment files processed</th>        <th class=\"col_heading level0 col14\" >params.subreddits path</th>        <th class=\"col_heading level0 col15\" >params.tf batch inference rows</th>        <th class=\"col_heading level0 col16\" >params.tf limit first n chars</th>        <th class=\"col_heading level0 col17\" >params.tokenize lowercase</th>        <th class=\"col_heading level0 col18\" >params.batch comment files</th>        <th class=\"col_heading level0 col19\" >params.posts path</th>        <th class=\"col_heading level0 col20\" >tags.mlflow.source.git.commit</th>        <th class=\"col_heading level0 col21\" >tags.mlflow.runName</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e1f17_level0_row0\" class=\"row_heading level0 row0\" >95</th>\n",
       "                        <td id=\"T_e1f17_row0_col0\" class=\"data row0 col0\" >a948e9fd651545f997430cddc6b529eb</td>\n",
       "                        <td id=\"T_e1f17_row0_col1\" class=\"data row0 col1\" >10</td>\n",
       "                        <td id=\"T_e1f17_row0_col2\" class=\"data row0 col2\" >FINISHED</td>\n",
       "                        <td id=\"T_e1f17_row0_col3\" class=\"data row0 col3\" >2021-07-29 23:02:33.997000+00:00</td>\n",
       "                        <td id=\"T_e1f17_row0_col4\" class=\"data row0 col4\" >0.08</td>\n",
       "                        <td id=\"T_e1f17_row0_col5\" class=\"data row0 col5\" >176.77</td>\n",
       "                        <td id=\"T_e1f17_row0_col6\" class=\"data row0 col6\" >19,168,154.00</td>\n",
       "                        <td id=\"T_e1f17_row0_col7\" class=\"data row0 col7\" >14.74</td>\n",
       "                        <td id=\"T_e1f17_row0_col8\" class=\"data row0 col8\" >514.00</td>\n",
       "                        <td id=\"T_e1f17_row0_col9\" class=\"data row0 col9\" >1,649,929.00</td>\n",
       "                        <td id=\"T_e1f17_row0_col10\" class=\"data row0 col10\" >3,767.00</td>\n",
       "                        <td id=\"T_e1f17_row0_col11\" class=\"data row0 col11\" >145.73</td>\n",
       "                        <td id=\"T_e1f17_row0_col12\" class=\"data row0 col12\" >515.00</td>\n",
       "                        <td id=\"T_e1f17_row0_col13\" class=\"data row0 col13\" >37.00</td>\n",
       "                        <td id=\"T_e1f17_row0_col14\" class=\"data row0 col14\" >subreddits/top/2021-07-16</td>\n",
       "                        <td id=\"T_e1f17_row0_col15\" class=\"data row0 col15\" >2000</td>\n",
       "                        <td id=\"T_e1f17_row0_col16\" class=\"data row0 col16\" >1000</td>\n",
       "                        <td id=\"T_e1f17_row0_col17\" class=\"data row0 col17\" >True</td>\n",
       "                        <td id=\"T_e1f17_row0_col18\" class=\"data row0 col18\" >True</td>\n",
       "                        <td id=\"T_e1f17_row0_col19\" class=\"data row0 col19\" >posts/top/2021-07-16</td>\n",
       "                        <td id=\"T_e1f17_row0_col20\" class=\"data row0 col20\" >63f5f420fb6b48d8243749cba183071757dac531</td>\n",
       "                        <td id=\"T_e1f17_row0_col21\" class=\"data row0 col21\" >new_batch_fxn-2021-07-29_230233</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e1f17_level0_row1\" class=\"row_heading level0 row1\" >97</th>\n",
       "                        <td id=\"T_e1f17_row1_col0\" class=\"data row1 col0\" >e66c5db26bd64f6da09c012eea700d0a</td>\n",
       "                        <td id=\"T_e1f17_row1_col1\" class=\"data row1 col1\" >10</td>\n",
       "                        <td id=\"T_e1f17_row1_col2\" class=\"data row1 col2\" >FINISHED</td>\n",
       "                        <td id=\"T_e1f17_row1_col3\" class=\"data row1 col3\" >2021-07-29 18:59:48.715000+00:00</td>\n",
       "                        <td id=\"T_e1f17_row1_col4\" class=\"data row1 col4\" >-</td>\n",
       "                        <td id=\"T_e1f17_row1_col5\" class=\"data row1 col5\" >133.16</td>\n",
       "                        <td id=\"T_e1f17_row1_col6\" class=\"data row1 col6\" >19,200,854.00</td>\n",
       "                        <td id=\"T_e1f17_row1_col7\" class=\"data row1 col7\" >-</td>\n",
       "                        <td id=\"T_e1f17_row1_col8\" class=\"data row1 col8\" >-</td>\n",
       "                        <td id=\"T_e1f17_row1_col9\" class=\"data row1 col9\" >-</td>\n",
       "                        <td id=\"T_e1f17_row1_col10\" class=\"data row1 col10\" >-</td>\n",
       "                        <td id=\"T_e1f17_row1_col11\" class=\"data row1 col11\" >117.47</td>\n",
       "                        <td id=\"T_e1f17_row1_col12\" class=\"data row1 col12\" >-</td>\n",
       "                        <td id=\"T_e1f17_row1_col13\" class=\"data row1 col13\" >37.00</td>\n",
       "                        <td id=\"T_e1f17_row1_col14\" class=\"data row1 col14\" >None</td>\n",
       "                        <td id=\"T_e1f17_row1_col15\" class=\"data row1 col15\" >6100</td>\n",
       "                        <td id=\"T_e1f17_row1_col16\" class=\"data row1 col16\" >850</td>\n",
       "                        <td id=\"T_e1f17_row1_col17\" class=\"data row1 col17\" >False</td>\n",
       "                        <td id=\"T_e1f17_row1_col18\" class=\"data row1 col18\" >True</td>\n",
       "                        <td id=\"T_e1f17_row1_col19\" class=\"data row1 col19\" >None</td>\n",
       "                        <td id=\"T_e1f17_row1_col20\" class=\"data row1 col20\" >64f49e85a8ef56a6795edf9da9a6f5964cb6830b</td>\n",
       "                        <td id=\"T_e1f17_row1_col21\" class=\"data row1 col21\" >new_batch_fxn_2021-07-29_185948</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e1f17_level0_row2\" class=\"row_heading level0 row2\" >106</th>\n",
       "                        <td id=\"T_e1f17_row2_col0\" class=\"data row2 col0\" >614a38e6690c4f3ba08725b1585b2ee9</td>\n",
       "                        <td id=\"T_e1f17_row2_col1\" class=\"data row2 col1\" >9</td>\n",
       "                        <td id=\"T_e1f17_row2_col2\" class=\"data row2 col2\" >KILLED</td>\n",
       "                        <td id=\"T_e1f17_row2_col3\" class=\"data row2 col3\" >2021-07-29 11:49:53.924000+00:00</td>\n",
       "                        <td id=\"T_e1f17_row2_col4\" class=\"data row2 col4\" >0.07</td>\n",
       "                        <td id=\"T_e1f17_row2_col5\" class=\"data row2 col5\" >-</td>\n",
       "                        <td id=\"T_e1f17_row2_col6\" class=\"data row2 col6\" >-</td>\n",
       "                        <td id=\"T_e1f17_row2_col7\" class=\"data row2 col7\" >10.01</td>\n",
       "                        <td id=\"T_e1f17_row2_col8\" class=\"data row2 col8\" >514.00</td>\n",
       "                        <td id=\"T_e1f17_row2_col9\" class=\"data row2 col9\" >1,649,929.00</td>\n",
       "                        <td id=\"T_e1f17_row2_col10\" class=\"data row2 col10\" >3,767.00</td>\n",
       "                        <td id=\"T_e1f17_row2_col11\" class=\"data row2 col11\" >-</td>\n",
       "                        <td id=\"T_e1f17_row2_col12\" class=\"data row2 col12\" >515.00</td>\n",
       "                        <td id=\"T_e1f17_row2_col13\" class=\"data row2 col13\" >-</td>\n",
       "                        <td id=\"T_e1f17_row2_col14\" class=\"data row2 col14\" >subreddits/top/2021-07-16</td>\n",
       "                        <td id=\"T_e1f17_row2_col15\" class=\"data row2 col15\" >2100</td>\n",
       "                        <td id=\"T_e1f17_row2_col16\" class=\"data row2 col16\" >1000</td>\n",
       "                        <td id=\"T_e1f17_row2_col17\" class=\"data row2 col17\" >False</td>\n",
       "                        <td id=\"T_e1f17_row2_col18\" class=\"data row2 col18\" >None</td>\n",
       "                        <td id=\"T_e1f17_row2_col19\" class=\"data row2 col19\" >posts/top/2021-07-16</td>\n",
       "                        <td id=\"T_e1f17_row2_col20\" class=\"data row2 col20\" >64f49e85a8ef56a6795edf9da9a6f5964cb6830b</td>\n",
       "                        <td id=\"T_e1f17_row2_col21\" class=\"data row2 col21\" >test_new_fxn2021-07-29_114953</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fd3b8ee2590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_multiple_vals = df_mlf_use_for_agg.columns[df_mlf_use_for_agg.nunique(dropna=False) > 1]\n",
    "# len(cols_with_multiple_vals)\n",
    "\n",
    "style_df_numeric(\n",
    "    df_mlf_use_for_agg\n",
    "    [cols_with_multiple_vals]\n",
    "    .drop(['artifact_uri', 'end_time',\n",
    "           # 'start_time',\n",
    "           ], \n",
    "          axis=1)\n",
    "    .dropna(axis='columns', how='all')\n",
    "    .iloc[:, :30]\n",
    "    ,\n",
    "    rename_cols_for_display=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bd132",
   "metadata": {},
   "source": [
    "# Load configs for aggregation jobs\n",
    "\n",
    "`n_sample_comments_files` and `n_sample_posts_files` allow us to only load a few files at a time (e.g., 2 instead of 50) to test the process end-to-end.\n",
    "\n",
    "---\n",
    "Note that by default `hydra` is a cli tool. If we want to call use it in jupyter, we need to manually initialize configs & compose the configuration. See my custom function `load_config_agg_jupyter`. Also see:\n",
    "- [Notebook with `Hydra` examples in a notebook](https://github.com/facebookresearch/hydra/blob/master/examples/jupyter_notebooks/compose_configs_in_notebook.ipynb).\n",
    "- [Hydra docs, Hydra in Jupyter](https://hydra.cc/docs/next/advanced/jupyter_notebooks/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea573ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=4',     # \n",
    "               'n_sample_comments_files=4',  # 6 is limit for logging unique counts at comment level\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "config_full_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "               'n_sample_posts_files=null', \n",
    "               'n_sample_comments_files=null',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")\n",
    "\n",
    "# config_full_lc_true = AggregateEmbeddingsConfig(\n",
    "#     config_path=\"../config\",\n",
    "#     config_name='aggregate_embeddings',\n",
    "#     overrides=[f\"mlflow_experiment={mlflow_experiment_full}\",\n",
    "#                'n_sample_posts_files=null', \n",
    "#                'n_sample_comments_files=null',\n",
    "#                'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_true',\n",
    "#               ]\n",
    "# )\n",
    "# pprint(config_test_sample_lc_false.config_dict, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc61082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_test_sample_lc_false.config_flat,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c54147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_configs = pd.DataFrame(\n",
    "    [\n",
    "        config_test_sample_lc_false.config_flat,\n",
    "        # config_test_full_lc_false.config_flat,\n",
    "        config_full_lc_false.config_flat,\n",
    "        # config_full_lc_true.config_flat,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6fdbb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids</th>\n",
       "      <th>posts_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids</th>\n",
       "      <th>subreddit_meta_vectorized_mlflow_uuids_lowercase</th>\n",
       "      <th>comments_uuid</th>\n",
       "      <th>mlflow_experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[8eef951842a34a6e81d176b15ae74afd]</td>\n",
       "      <td>[537514ab3c724b10903000501802de0e]</td>\n",
       "      <td>[5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       comments_vectorized_mlflow_uuids       posts_vectorized_mlflow_uuids posts_vectorized_mlflow_uuids_lowercase subreddit_meta_vectorized_mlflow_uuids subreddit_meta_vectorized_mlflow_uuids_lowercase  \\\n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  [8eef951842a34a6e81d176b15ae74afd]      [537514ab3c724b10903000501802de0e]     [8eef951842a34a6e81d176b15ae74afd]               [537514ab3c724b10903000501802de0e]   \n",
       "\n",
       "                                                          comments_uuid                 mlflow_experiment  \n",
       "0  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]  v0.4.0_use_multi_aggregates_test  \n",
       "1  [5f10cd75334142168a6ebb787e477c1f, 2fcfefc3d5af43328168d3478b4fdeb6]       v0.4.0_use_multi_aggregates  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can't use (df_configs.nunique(dropna=False) > 1)\n",
    "#  because when a col's content is a list or something unhashable, we get an error\n",
    "#  so instead we'll check each column individually\n",
    "\n",
    "# cols_with_diffs_config = df_configs.columns[df_configs.nunique(dropna=False) > 1]\n",
    "cols_with_diffs_config = list()\n",
    "for c_ in df_configs.columns:\n",
    "    try:\n",
    "        if df_configs[c_].nunique() > 1:\n",
    "            cols_with_diffs_config.append(c_)\n",
    "    except TypeError:\n",
    "        cols_with_diffs_config.append(c_)\n",
    "        \n",
    "\n",
    "df_configs[cols_with_diffs_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0267eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(config_test_sample_lc_false.config_flat, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be9db7",
   "metadata": {},
   "source": [
    "# Initialize a local dask client\n",
    "so that we can see the progress/process for dask jobs.\n",
    "\n",
    "**dask default**: 8 workers with 64 CPUs present<br>\n",
    "I tested: 16 & 12 workers, but they runs out of RAM and job crashes.\n",
    "\n",
    "I rolled back to 8 workers to complete job /prevent OOM errors even if it feels like it's wasting workers, at least it finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d954c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 374 ms, sys: 158 ms, total: 533 ms\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# dask default: 8 workers with 64 CPUs present,\n",
    "cluster = LocalCluster(n_workers=8)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b31e1baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://127.0.0.1:8787/status'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf0e0c",
   "metadata": {},
   "source": [
    "# Run Full data with `lower_case=False`\n",
    "\n",
    "The logic for sampling files and download/`caching` files locally lives in the `mlf` custom function.\n",
    "\n",
    "Caching can save 9+ minutes if we try to download the files from GCS every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009699de",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c8c8445",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:17:48 | INFO | \"      775,092 <- Comments that DON'T need to be averaged\"\n",
      "23:17:48 | INFO | \"   39,126,876 <- Comments that need to be averaged\"\n",
      "23:17:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "(\"('dataframe-groupby-sum-agg-849e94fd54a49f8ed34330862f20cb9d', 0)\", <WorkerState 'tcp://127.0.0.1:41351', name: 6, memory: 0, processing: 1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py\u001b[0m in \u001b[0;36mrun_aggregation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# - up-votes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_comments_to_post_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py\u001b[0m in \u001b[0;36m_agg_comments_to_post_level\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;31m#        - in this loop check compute the values of the unique() and count() fxns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m#  Alternative: don't compute in that loop and create a 2nd loop where the tuples actually get compared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mr_com_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_v_com_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_post_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0mc_com_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_v_com_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactive_run\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1986\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1989\u001b[0m             )\n\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             return sync(\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m             )\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKilledWorker\u001b[0m: (\"('dataframe-groupby-sum-agg-849e94fd54a49f8ed34330862f20cb9d', 0)\", <WorkerState 'tcp://127.0.0.1:41351', name: 6, memory: 0, processing: 1>)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlflow.end_run(\"FAILED\")\n",
    "gc.collect()\n",
    "try:\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    setup_logging()\n",
    "    \n",
    "    del job_agg1\n",
    "    del d_dfs1\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "job_agg1 = AggregateEmbeddings(\n",
    "    run_name=f\"full_lc_false-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    **config_full_lc_false.config_flat\n",
    ")\n",
    "job_agg1.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283968c0",
   "metadata": {},
   "source": [
    "# Run full data, `lower_case=True`\n",
    "\n",
    "Looks like the problem I ran into with the file being corrupted might've been a problem with downloading the file(s). Fix: delete the local cache and download the files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869667a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:51 | INFO | \"== Start run_aggregation() method ==\"\n",
      "15:47:51 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "15:47:52 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-08-10_154752-full_lc_true-2021-08-10_154751\"\n",
      "15:47:52 | INFO | \"  Saving config to local path...\"\n",
      "15:47:52 | INFO | \"  Logging config to mlflow...\"\n",
      "15:47:52 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "15:47:52 | INFO | \"Loading subreddit description embeddings...\"\n",
      "15:47:53 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_subreddits_description\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7e6e4097cc4648a0a6dcf405072654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:54 | INFO | \"  Reading 1 files\"\n",
      "15:47:55 | INFO | \"       3,767 |  513 <- Raw vectorized subreddit description shape\"\n",
      "15:47:56 | INFO | \"Loading POSTS embeddings...\"\n",
      "15:47:57 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_posts\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b70848d70c4886834fc853336b4b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:48:44 | INFO | \"  Reading 48 files\"\n",
      "15:48:47 | INFO | \"   1,649,929 |  514 <- Raw POSTS shape\"\n",
      "15:48:51 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "15:48:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/10/a948e9fd651545f997430cddc6b529eb/artifacts/df_vect_comments\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ab07ab988b41b9b0d92315679c18a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:48 | INFO | \"  Reading 37 files\"\n",
      "15:54:49 | INFO | \"  0:06:56.293258 <- Total raw embeddings load time elapsed\"\n",
      "15:54:49 | INFO | \"-- Start _load_metadata() method --\"\n",
      "15:54:49 | INFO | \"Loading POSTS metadata...\"\n",
      "15:54:49 | INFO | \"Reading raw data...\"\n",
      "15:54:49 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-07-16\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59eb9efacfd0489f806b804fd167453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:51 | INFO | \"  Applying transformations...\"\n",
      "15:54:52 | INFO | \"  (1649929, 14) <- Raw META POSTS shape\"\n",
      "15:54:52 | INFO | \"Loading subs metadata...\"\n",
      "15:54:52 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "15:54:52 | INFO | \"Reading raw data...\"\n",
      "15:54:52 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-07-16\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98ebb0befa4f09aa0cc006cb99f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:53 | INFO | \"  Applying transformations...\"\n",
      "15:54:54 | INFO | \"  (3767, 38) <- Raw META subreddit description shape\"\n",
      "15:54:54 | INFO | \"Loading COMMENTS metadata...\"\n",
      "15:54:54 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-07-09\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e52ad01a47422bbde292fb39492af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:54:55 | INFO | \"  (Delayed('int-11aa2518-d088-4702-bee1-c90e9c40927d'), 7) <- Raw META COMMENTS shape\"\n",
      "15:54:55 | INFO | \"  0:00:05.773888 <- Total metadata loading time elapsed\"\n",
      "15:54:55 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "15:54:55 | INFO | \"Getting count of comments per post...\"\n",
      "15:55:17 | WARNING | \"Error creating summary of comments per post.\n",
      "'<=' not supported between instances of 'NoneType' and 'int'\"\n",
      "15:55:18 | INFO | \"Filtering which comments need to be averaged...\"\n",
      "15:56:48 | INFO | \"      126,642 <- Comments that DON'T need to be averaged\"\n",
      "15:56:48 | INFO | \"   19,041,512 <- Comments that need to be averaged\"\n",
      "15:56:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "15:59:15 | INFO | \"      979,701 |  514 <- df_v_com_agg SHAPE\"\n",
      "15:59:15 | INFO | \"  0:04:20.021986 <- Total comments to post agg loading time elapsed\"\n",
      "15:59:15 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
      "15:59:17 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
      "15:59:17 | INFO | \"  (Delayed('int-2f2dd3eb-5832-498e-ac7f-0f8fc90cbef9'), 513) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n",
      "15:59:17 | INFO | \"Concat aggregated comments+posts with posts-without comments\"\n",
      "16:09:32 | INFO | \"    1,649,929 |  514 <- df_posts_agg_b shape after aggregation\"\n",
      "16:09:32 | INFO | \"  0:10:16.855088 <- Total (df_posts_agg_b) posts & comments agg time elapsed\"\n",
      "16:09:32 | INFO | \"-- Start (df_posts_agg_c) _agg_posts_comments_and_sub_descriptions_to_post_level() method --\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-2afffaf2-f55e-4a71-b7a9-43db8c7e9419'), 513) <- df_agg_posts_w_sub.shape (only posts with comments)\"\n",
      "16:09:35 | INFO | \"  0:00:02.606942 <- Total (df_posts_agg_c) posts+comments+subs agg time elapsed\"\n",
      "16:09:35 | INFO | \"-- Start _agg_post_aggregates_to_subreddit_level() method --\"\n",
      "16:09:35 | INFO | \"No column to weight comments, simple mean to roll up posts to subreddit-level...\"\n",
      "16:09:35 | INFO | \"A - posts only\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-c0cdc12a-7599-4f26-8877-628f56e69ca7'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
      "16:09:35 | INFO | \"B - posts + comments\"\n",
      "16:09:35 | INFO | \"  (Delayed('int-4a88fcb6-f299-4f12-96d0-1c52f2200024'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
      "16:09:35 | INFO | \"C - posts + comments + sub descriptions\"\n",
      "16:09:36 | INFO | \"  (Delayed('int-2c3b142e-c4c6-44e7-8f3b-5b7bafa6f4a0'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
      "16:09:36 | INFO | \"  0:00:01.266842 <- Total for ALL subreddit-level agg time elapsed\"\n",
      "16:09:36 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
      "16:09:36 | INFO | \"A...\"\n",
      "16:09:48 | INFO | \"  (3767, 3767) <- df_subs_agg_a_similarity.shape\"\n",
      "16:10:07 | INFO | \"Merge distance + metadata...\"\n",
      "16:10:37 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "16:10:44 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "16:10:44 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "16:10:50 | INFO | \"B...\"\n",
      "16:29:57 | INFO | \"  (3767, 3767) <- df_subs_agg_b_similarity.shape\"\n",
      "16:30:16 | INFO | \"Merge distance + metadata...\"\n",
      "16:30:46 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "16:30:54 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "16:30:54 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "16:31:00 | INFO | \"C...\"\n",
      "17:17:45 | INFO | \"  (3767, 3767) <- df_subs_agg_c_similarity.shape\"\n",
      "17:18:06 | INFO | \"Merge distance + metadata...\"\n",
      "17:18:36 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
      "17:18:44 | INFO | \"  (14186522, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
      "17:18:44 | INFO | \"  (75340, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
      "17:18:49 | INFO | \"  1:09:13.049794 <- Total for _calculate_subreddit_similarities() time elapsed\"\n",
      "17:18:49 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "17:18:49 | INFO | \"  Saving config to local path...\"\n",
      "17:18:49 | INFO | \"  Logging config to mlflow...\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f472b6e6ac4d91b6d55cd455fbfc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:18:50 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "17:18:50 | INFO | \"Saving locally...\"\n",
      "17:42:53 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "18:06:23 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:06:25 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"\n",
      "18:06:25 | INFO | \"Saving locally...\"\n",
      "18:06:25 | INFO | \"Keeping index intact...\"\n",
      "18:06:25 | INFO | \"Converting pandas to dask...\"\n",
      "18:06:25 | INFO | \"   108.6 MB <- Memory usage\"\n",
      "18:06:25 | INFO | \"       3\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
      "18:06:29 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:06:31 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity_pair **\"\n",
      "18:06:31 | INFO | \"Saving locally...\"\n",
      "18:06:33 | INFO | \"Converting pandas to dask...\"\n",
      "18:06:40 | INFO | \"  6,002.0 MB <- Memory usage\"\n",
      "18:06:40 | INFO | \"      81\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "18:06:53 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:07:16 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
      "18:07:16 | INFO | \"Saving locally...\"\n",
      "18:17:47 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "18:28:42 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:28:44 | INFO | \"** df_sub_level_agg_b_post_and_comments_similarity **\"\n",
      "18:28:44 | INFO | \"Saving locally...\"\n",
      "18:28:44 | INFO | \"Keeping index intact...\"\n",
      "18:28:44 | INFO | \"Converting pandas to dask...\"\n",
      "18:28:44 | INFO | \"   108.6 MB <- Memory usage\"\n",
      "18:28:44 | INFO | \"       3\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
      "18:28:48 | INFO | \"Logging artifact to mlflow...\"\n",
      "18:28:50 | INFO | \"** df_sub_level_agg_b_post_and_comments_similarity_pair **\"\n",
      "18:28:50 | INFO | \"Saving locally...\"\n",
      "18:28:52 | INFO | \"Converting pandas to dask...\"\n",
      "18:28:59 | INFO | \"  6,002.0 MB <- Memory usage\"\n",
      "18:28:59 | INFO | \"      81\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# mlflow.end_run(\"FAILED\")\n",
    "# gc.collect()\n",
    "# try:\n",
    "#     # run setup_logging() to remove logging to the file of a failed job\n",
    "#     setup_logging()\n",
    "    \n",
    "#     del job_agg2\n",
    "#     del d_dfs2\n",
    "# except NameError:\n",
    "#     pass\n",
    "# gc.collect()\n",
    "\n",
    "# job_agg2 = AggregateEmbeddings(\n",
    "#     run_name=f\"full_lc_true-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "#     **config_full_lc_true.config_flat\n",
    "# )\n",
    "# job_agg2.run_aggregation()\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43966ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e5bd2",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4633316",
   "metadata": {},
   "source": [
    "## VM size notes\n",
    "\n",
    "`614 GB` of RAM is not enough for 40 million posts...\n",
    "\n",
    "VM & cluster set up:\n",
    "```bash\n",
    "96 CPUS\n",
    "640 GB RAM\n",
    "\n",
    "8 workers\n",
    "- 12 threads per worker\n",
    "- 76 GB per worker\n",
    "```\n",
    "\n",
    "Traceback:\n",
    "\n",
    "```bash\n",
    "23:17:48 | INFO | \"      775,092 <- Comments that DON'T need to be averaged\"\n",
    "23:17:48 | INFO | \"   39,126,876 <- Comments that need to be averaged\"\n",
    "23:17:48 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
    "distributed.nanny - WARNING - Restarting worker\n",
    "---------------------------------------------------------------------------\n",
    "KilledWorker                              Traceback (most recent call last)\n",
    "<timed exec> in <module>\n",
    "\n",
    "/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/aggregate_embeddings.py in run_aggregation(self)\n",
    "    249         # - up-votes\n",
    "    250         # ---\n",
    "--> 251         self._agg_comments_to_post_level()\n",
    "...\n",
    "KilledWorker: (\"('dataframe-groupby-sum-agg-849e94fd54a49f8ed34330862f20cb9d', 0)\", <WorkerState 'tcp://127.0.0.1:41351', name: 6, memory: 0, processing: 1>)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e8cf7",
   "metadata": {},
   "source": [
    "# Run test with `lower_case=False\n",
    "\n",
    "Sample only a few files in comments/ posts to make sure that job completes even when we're testing new code/logic.\n",
    "\n",
    "Limit to only 2 files of each kind to get minimum test to run end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2021ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run setup_logging() to remove logging to the file of a failed job\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e33ef683",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.debug(\"debug test\")\n",
    "logging.info(\"info test\")\n",
    "logging.warning(\"warning message\")\n",
    "logging.error(\"error message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2cc4d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_aggregates_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_aggregates'\n",
    "\n",
    "root_agg_config_name = 'aggregate_embeddings_v0.4.0'\n",
    "\n",
    "config_test_sample_lc_false = AggregateEmbeddingsConfig(\n",
    "    config_path=\"../config\",\n",
    "    config_name=root_agg_config_name,\n",
    "    overrides=[f\"mlflow_experiment={mlflow_experiment_test}\",\n",
    "               'n_sample_posts_files=2',     # \n",
    "               'n_sample_comments_files=2',  # 6 is limit for logging unique counts at comment level\n",
    "               'calculate_similarites=false',\n",
    "               # 'data_embeddings_to_aggregate=top_subs-2021_07_16-use_multi_lower_case_false',\n",
    "              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "475c7a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_experiment: \tv0.4.0_use_multi_aggregates_test\n",
      "n_sample_posts_files: \t2\n",
      "n_sample_comments_files: \t2\n",
      "\n",
      "aggregate_params:\n",
      "  min_comment_text_len: \t10\n",
      "  agg_comments_to_post_weight_col: \tNone\n",
      "  agg_post_to_subreddit_weight_col: \tNone\n",
      "  agg_post_post_weight: \t70\n",
      "  agg_post_comment_weight: \t20\n",
      "  agg_post_subreddit_desc_weight: \t10\n",
      "calculate_similarites: \tFalse\n"
     ]
    }
   ],
   "source": [
    "keys_to_check_in_config = ['mlflow_experiment', 'n_sample_posts_files', 'n_sample_comments_files', 'aggregate_params', 'calculate_similarites']\n",
    "\n",
    "for k_ in keys_to_check_in_config:\n",
    "    v_ = config_test_sample_lc_false.config_dict.get(k_)\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"\\n{k_}:\")\n",
    "        [print(f\"  {k2_}: \\t{v2_}\") for k2_, v2_ in v_.items()]\n",
    "    else:\n",
    "        print(f\"{k_}: \\t{v_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c11d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6662271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:59:27 | INFO | \"== Start run_aggregation() method ==\"\n",
      "05:59:27 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "05:59:27 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/2021-10-06_055927-sample_test_lc_false-2021-10-06_055927\"\n",
      "05:59:27 | INFO | \"  Saving config to local path...\"\n",
      "05:59:27 | INFO | \"  Logging config to mlflow...\"\n",
      "05:59:28 | INFO | \"-- Start _load_raw_embeddings() method --\"\n",
      "05:59:28 | INFO | \"Loading subreddit description embeddings...\"\n",
      "05:59:29 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_subreddits_description\"\n",
      "100%|##########################################| 5/5 [00:00<00:00, 15341.27it/s]\n",
      "05:59:29 | INFO | \"  Parquet files found: 2\"\n",
      "05:59:29 | INFO | \"      19,262 |  513 <- Raw vectorized subreddit description shape\"\n",
      "05:59:29 | INFO | \"  Unique check for subreddit description...\"\n",
      "05:59:29 | INFO | \"Loading POSTS embeddings...\"\n",
      "05:59:29 | INFO | \"  Sampling POSTS FILES down to: 2\"\n",
      "05:59:30 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/8eef951842a34a6e81d176b15ae74afd/artifacts/df_vect_posts\"\n",
      "100%|########################################| 28/28 [00:00<00:00, 42955.56it/s]\n",
      "05:59:30 | INFO | \"  Parquet files found: 2\"\n",
      "05:59:35 | INFO | \"     702,743 |  514 <- Raw POSTS shape\"\n",
      "05:59:35 | INFO | \"  Checking that posts are unique...\"\n",
      "05:59:40 | INFO | \"Loading COMMENTS embeddings...\"\n",
      "05:59:40 | INFO | \"  Sampling COMMENTS FILES down to: 2\"\n",
      "05:59:40 | INFO | \"  Found 2 run UUIDs with COMMENT embeddings...\"\n",
      "05:59:40 | INFO | \"    Sampling 1 FILES per run UUID\"\n",
      "05:59:41 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/5f10cd75334142168a6ebb787e477c1f/artifacts/df_vect_comments\"\n",
      "100%|########################################| 21/21 [00:00<00:00, 33300.71it/s]\n",
      "05:59:41 | INFO | \"  Parquet files found: 1\"\n",
      "05:59:43 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
      "100%|########################################| 40/40 [00:00<00:00, 37650.84it/s]\n",
      "05:59:43 | INFO | \"  Parquet files found: 1\"\n",
      "05:59:44 | INFO | \"  Keep only comments for posts with embeddings\"\n",
      "05:59:58 | INFO | \"      121,345 |  515 <- COMMENTS shape, after keeping only existing posts\"\n",
      "05:59:58 | INFO | \"  0:00:29.717653 <- Total raw embeddings load time elapsed\"\n",
      "05:59:59 | INFO | \"-- Start _load_metadata() method --\"\n",
      "05:59:59 | INFO | \"Loading POSTS metadata...\"\n",
      "05:59:59 | INFO | \"Reading raw data...\"\n",
      "05:59:59 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/posts/top/2021-09-27\"\n",
      "100%|##############################| 27/27 [00:00<00:00, 35556.11it/s]\n",
      "06:00:09 | INFO | \"  Applying transformations...\"\n",
      "06:00:46 | INFO | \"  (8439672, 15) <- Raw META POSTS shape\"\n",
      "06:00:46 | INFO | \"Loading subs metadata...\"\n",
      "06:00:46 | INFO | \"  reading sub-level data & merging with aggregates...\"\n",
      "06:00:46 | INFO | \"Reading raw data...\"\n",
      "06:00:46 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/subreddits/top/2021-09-24\"\n",
      "100%|#################################| 1/1 [00:00<00:00, 3748.26it/s]\n",
      "06:00:47 | INFO | \"  Applying transformations...\"\n",
      "06:00:56 | INFO | \"  (19262, 91) <- Raw META subreddit description shape\"\n",
      "06:00:56 | INFO | \"Loading COMMENTS metadata...\"\n",
      "06:00:56 | INFO | \"Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
      "100%|##############################| 59/59 [00:00<00:00, 47343.40it/s]\n",
      "06:00:56 | INFO | \"  (Delayed('int-1d2685f8-2b70-4e74-bccc-2a48d376a8cc'), 7) <- Raw META COMMENTS shape\"\n",
      "06:00:56 | INFO | \"  0:00:57.326112 <- Total metadata loading time elapsed\"\n",
      "06:00:58 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
      "06:00:59 | INFO | \"Getting count of comments per post...\"\n",
      "06:01:17 | INFO | \"Comments per post summary:\n",
      "    comment_count  posts_count  percent_of_posts\n",
      "0               0       680254          0.967998\n",
      "1               1         3082          0.004386\n",
      "2               2         2978          0.004238\n",
      "3               3         2492          0.003546\n",
      "4               4         2011          0.002862\n",
      "5               5         1619          0.002304\n",
      "6               6         1295          0.001843\n",
      "7               7         1051          0.001496\n",
      "8               8          838          0.001192\n",
      "9               9         6439          0.009163\n",
      "10             10          242          0.000344\n",
      "11             11          157          0.000223\n",
      "12             12          100          0.000142\n",
      "13             13           68          0.000097\n",
      "14             14           45          0.000064\"\n",
      "06:01:17 | INFO | \"TESTING that all post IDs are counted in df_comment_count_per_post...\"\n",
      "06:01:31 | INFO | \"  True <- Post IDs in df_v_posts == df_comment_count_per_post\"\n",
      "06:01:33 | INFO | \"Filtering which comments need to be averaged...\"\n",
      "06:01:50 | INFO | \"        3,082 <- Comments that DON'T need to be averaged\"\n",
      "06:01:50 | INFO | \"      118,263 <- Comments that need to be averaged\"\n",
      "06:01:50 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
      "06:02:02 | INFO | \"       22,489 |  514 <- df_v_com_agg SHAPE\"\n",
      "06:02:02 | INFO | \"  0:01:03.062409 <- Total comments to post agg loading time elapsed\"\n",
      "06:02:02 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
      "06:02:08 | INFO | \"      680,254 <- Posts that DON'T need weighted average\"\n",
      "06:02:08 | INFO | \"       22,489 <- Posts that need weighted average\"\n",
      "06:02:18 | INFO | \"       44,978 |  515 <- Shape of df(posts+comments) to weight\"\n",
      "06:02:18 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
      "06:02:19 | INFO | \"  (Delayed('int-3777df01-e276-43f2-8a63-3faf3771a5e6'), 513) <- df_agg_posts_w_comments.shape (only posts with comments)\"\n",
      "06:02:19 | INFO | \"Concat aggregated comments+posts with posts-without comments\"\n",
      "06:02:50 | INFO | \"      702,743 |  514 <- df_posts_agg_b shape after aggregation\"\n",
      "06:02:50 | INFO | \"  0:00:47.864259 <- Total (df_posts_agg_b) posts & comments agg time elapsed\"\n",
      "06:02:50 | INFO | \"-- Start (df_posts_agg_c) _agg_posts_comments_and_sub_descriptions_to_post_level() method --\"\n",
      "06:02:51 | INFO | \"  (Delayed('int-e55772e7-30fc-43cb-a252-41d414faff85'), 513) <- df_agg_posts_w_sub.shape (only posts with comments)\"\n",
      "06:02:51 | INFO | \"  0:00:01.300803 <- Total (df_posts_agg_c) posts+comments+subs agg time elapsed\"\n",
      "06:02:51 | INFO | \"-- Start _agg_post_aggregates_to_subreddit_level() method --\"\n",
      "06:02:51 | INFO | \"No column to weight comments, simple mean to roll up posts to subreddit-level...\"\n",
      "06:02:51 | INFO | \"A - posts only\"\n",
      "06:02:52 | INFO | \"  (Delayed('int-ab571b30-8958-4f07-9ae6-ae837c2bd237'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
      "06:02:52 | INFO | \"B - posts + comments\"\n",
      "06:02:52 | INFO | \"  (Delayed('int-3a0d8fdb-d747-4304-b633-6b298561d301'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
      "06:02:52 | INFO | \"C - posts + comments + sub descriptions\"\n",
      "06:02:53 | INFO | \"  (Delayed('int-54d6dd4a-5a42-4544-9b51-862420679fa3'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
      "06:02:53 | INFO | \"  0:00:01.266962 <- Total for ALL subreddit-level agg time elapsed\"\n",
      "06:02:53 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "06:02:53 | INFO | \"  Saving config to local path...\"\n",
      "06:02:53 | INFO | \"  Logging config to mlflow...\"\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]06:02:53 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "06:02:53 | INFO | \"Saving locally...\"\n",
      "06:02:53 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "06:16:13 | INFO | \"  NOT logging shape of dask df to save time & compute\"\n",
      "06:16:13 | INFO | \"Logging artifact to mlflow...\"\n",
      " 20%|########8                                   | 1/5 [13:21<53:25, 801.27s/it]06:16:14 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
      "06:16:14 | INFO | \"Saving locally...\"\n",
      "06:16:14 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "06:16:44 | INFO | \"  NOT logging shape of dask df to save time & compute\"\n",
      "06:16:44 | INFO | \"Logging artifact to mlflow...\"\n",
      " 40%|#################6                          | 2/5 [13:51<17:23, 347.98s/it]06:16:45 | INFO | \"** df_sub_level_agg_a_post_only **\"\n",
      "06:16:45 | INFO | \"Saving locally...\"\n",
      "06:16:47 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "06:16:55 | INFO | \"  NOT logging shape of dask df to save time & compute\"\n",
      "06:16:55 | INFO | \"Logging artifact to mlflow...\"\n",
      " 60%|##########################4                 | 3/5 [14:02<06:28, 194.02s/it]06:16:56 | INFO | \"** df_post_level_agg_b_post_and_comments **\"\n",
      "06:16:56 | INFO | \"Saving locally...\"\n",
      "06:16:56 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "06:17:45 | INFO | \"  NOT logging shape of dask df to save time & compute\"\n",
      "06:17:45 | INFO | \"Logging artifact to mlflow...\"\n",
      " 80%|###################################2        | 4/5 [15:14<02:25, 145.77s/it]06:18:08 | INFO | \"** df_post_level_agg_c_post_comments_sub_desc **\"\n",
      "06:18:08 | INFO | \"Saving locally...\"\n",
      "06:18:08 | INFO | \"  Saving existing dask df as parquet...\"\n",
      "06:32:53 | INFO | \"  NOT logging shape of dask df to save time & compute\"\n",
      "06:32:53 | INFO | \"Logging artifact to mlflow...\"\n",
      "100%|############################################| 5/5 [30:23<00:00, 364.76s/it]\n",
      "06:33:17 | INFO | \"  0:30:24.220077 <- Total for _save_and_log_aggregate_and_similarity_dfs() time elapsed\"\n",
      "06:33:17 | INFO | \"  0:33:49.782488 <- Total Agg fxn time time elapsed\"\n",
      "06:33:17 | INFO | \"== COMPLETE run_aggregation() method ==\"\n",
      "06:33:17 | INFO | \"    Removing fileHandler...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 26s, sys: 54.2 s, total: 17min 20s\n",
      "Wall time: 33min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7927"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlflow.end_run(\"FAILED\")\n",
    "gc.collect()\n",
    "try:\n",
    "    # run setup_logging() to remove logging to the file of a failed job\n",
    "    setup_logging()\n",
    "    del job_agg1\n",
    "    del d_dfs1\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "job_agg_test = AggregateEmbeddings(\n",
    "    run_name=f\"sample_test_lc_false-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    **config_test_sample_lc_false.config_flat\n",
    ")\n",
    "job_agg_test.run_aggregation()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "48ea8511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:33:20 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
      "06:33:20 | INFO | \"  Saving config to local path...\"\n",
      "06:33:20 | INFO | \"  Logging config to mlflow...\"\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]06:33:21 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
      "06:33:21 | INFO | \"  ** SKIPPING because path already exits **\"\n",
      "06:33:21 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
      "06:33:21 | INFO | \"  ** SKIPPING because path already exits **\"\n",
      "06:33:21 | INFO | \"** df_sub_level_agg_a_post_only **\"\n",
      "06:33:21 | INFO | \"  ** SKIPPING because path already exits **\"\n",
      "06:33:21 | INFO | \"** df_post_level_agg_b_post_and_comments **\"\n",
      "06:33:21 | INFO | \"  ** SKIPPING because path already exits **\"\n",
      "06:33:21 | INFO | \"** df_post_level_agg_c_post_comments_sub_desc **\"\n",
      "06:33:21 | INFO | \"  ** SKIPPING because path already exits **\"\n",
      "100%|############################################| 5/5 [00:00<00:00, 971.62it/s]\n",
      "06:33:21 | INFO | \"  0:00:00.441245 <- Total for _save_and_log_aggregate_and_similarity_dfs() time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "job_agg_test._save_and_log_aggregate_and_similarity_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "35f215d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4449"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d1ef5e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0dc5a63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['              total        used        free      shared  buff/cache   available\\n',\n",
       " 'Mem:         628888      157012      347006          16      124870      467921\\n',\n",
       " 'Swap:             0           0           0\\n',\n",
       " 'Total:       628888      157012      347006\\n']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.popen('free -t -m').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d6349c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_memory, used_memory, free_memory = map(\n",
    "    int, os.popen('free -t -m').readlines()[-1].split()[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "47be4f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(628888, 156936, 347082)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_memory, used_memory, free_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c29a51",
   "metadata": {},
   "source": [
    "# Check output dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca8aea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vars(job_agg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94f4037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb91de8523c48adb7f4eb91404ed2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df_v_posts\n",
      "  (Delayed('int-016a44e7-a6e2-4f66-ab22-7879eafed940'), 514) <- df shape\n",
      "  2 <- dask partitions\n",
      "\n",
      "df_v_comments\n",
      "  (Delayed('int-d94ef501-7b59-467f-be73-7b49a6a61c51'), 515) <- df shape\n",
      "  2 <- dask partitions\n",
      "\n",
      "df_v_sub\n",
      "  (Delayed('int-3ac61eff-3b0a-437d-9442-72bca0c459f2'), 513) <- df shape\n",
      "  2 <- dask partitions\n",
      "\n",
      "df_subs_meta\n",
      "  (19262, 91) <- df shape\n",
      "  (19262, 91) <- df shape\n",
      "\n",
      "df_posts_meta\n",
      "  (8439672, 15) <- df shape\n",
      "  (8439672, 15) <- df shape\n",
      "\n",
      "df_comments_meta\n",
      "  (Delayed('int-fa373a7f-2e27-4e2d-b4fb-07c3494ed543'), 7) <- df shape\n",
      "  59 <- dask partitions\n",
      "\n",
      "df_comment_count_per_post\n",
      "  (Delayed('int-65fed992-ca7b-4d15-b040-ab0babc6c958'), 3) <- df shape\n",
      "  3 <- dask partitions\n",
      "\n",
      "df_posts_agg_b\n",
      "  (Delayed('int-e1b4d90a-ab33-4573-b0d2-8eef4a628a78'), 514) <- df shape\n",
      "  7 <- dask partitions\n",
      "\n",
      "df_posts_agg_c\n",
      "  (Delayed('int-8d9409fb-344c-418f-8163-51e7a9479068'), 514) <- df shape\n",
      "  14 <- dask partitions\n",
      "\n",
      "df_subs_agg_a\n",
      "  (Delayed('int-64aadf89-57c3-4135-b649-c1130bae1390'), 513) <- df shape\n",
      "  1 <- dask partitions\n",
      "\n",
      "df_subs_agg_b\n",
      "  (Delayed('int-814ba1f4-a1fb-4c3e-930e-35952e2c6105'), 513) <- df shape\n",
      "  1 <- dask partitions\n",
      "\n",
      "df_subs_agg_c\n",
      "  (Delayed('int-09dad0a8-e29d-431f-8338-ac66a4bd7918'), 513) <- df shape\n",
      "  1 <- dask partitions\n",
      "\n",
      "df_subs_agg_a_similarity\n",
      "\n",
      "df_subs_agg_b_similarity\n",
      "\n",
      "df_subs_agg_c_similarity\n",
      "\n",
      "df_v_com_agg\n",
      "  (Delayed('int-3c323bb6-99e2-45db-a012-1fbff243628c'), 514) <- df shape\n",
      "  3 <- dask partitions\n",
      "\n",
      "df_posts_for_weights\n",
      "  (Delayed('int-3ac9a518-677b-47a5-9d19-146ebdc3f2d3'), 516) <- df shape\n",
      "  14 <- dask partitions\n",
      "\n",
      "ddf_agg_posts_w_comments\n",
      "  (Delayed('int-a16e3324-6d67-4a59-a993-3f6ab5c3642c'), 513) <- df shape\n",
      "  5 <- dask partitions\n",
      "\n",
      "df_agg_posts_w_sub\n",
      "  (Delayed('int-480c5be9-a7dc-4432-8cc5-ff5b16e349d5'), 513) <- df shape\n",
      "  14 <- dask partitions\n"
     ]
    }
   ],
   "source": [
    "d_dfs2 = {k: v for k, v in vars(job_agg_test).items() if 'df_' in k}\n",
    "\n",
    "\n",
    "for k2, df_2 in tqdm(d_dfs2.items()):\n",
    "    print(f\"\\n{k2}\")\n",
    "    try:\n",
    "        print(f\"  {df_2.shape} <- df shape\")\n",
    "        print(f\"  {df_2.npartitions} <- dask partitions\")\n",
    "        # print(f\"{get_dask_df_shape(df_2)} <- df.shape\")\n",
    "        # print(f\"  {df_2.memory_usage(deep=True).sum() / 1048576:4,.1f} MB <- Memory usage\")\n",
    "        if any(['meta' in k2, '_v_' in k2]):\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "#             display(df_2.iloc[:5, :15])\n",
    "\n",
    "    except (TypeError, AttributeError):\n",
    "        if isinstance(df_2, pd.DataFrame):\n",
    "            print(f\"  {df_2.shape} <- df shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698bc8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c82ad5",
   "metadata": {},
   "source": [
    "### time profiling\n",
    "\n",
    "inputs:\n",
    "``` python\n",
    "mlflow_experiment: \tv0.4.0_use_multi_aggregates_test\n",
    "n_sample_posts_files: \t5\n",
    "n_sample_comments_files: \t10\n",
    "\n",
    "aggregate_params:\n",
    "  min_comment_text_len: \t10\n",
    "  agg_comments_to_post_weight_col: \tNone\n",
    "  agg_post_to_subreddit_weight_col: \tNone\n",
    "  agg_post_post_weight: \t70\n",
    "  agg_post_comment_weight: \t20\n",
    "  agg_post_subreddit_desc_weight: \t10\n",
    "```\n",
    "\n",
    "VM & cluster set up:\n",
    "```\n",
    "96 CPUS\n",
    "640 GB RAM\n",
    "\n",
    "8 workers\n",
    "- 12 threads per worker\n",
    "- 76 GB per worker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d465c2",
   "metadata": {},
   "source": [
    "### Filtered/selected logs\n",
    "\n",
    "Overview:\n",
    "\n",
    "| Time/ETA | Step | Notes |\n",
    "| --- | --- | --- |\n",
    "| `0:11:43` minutes | load raw embeddings (w/o caching) | |\n",
    "| `0:03:15` minutes | Load metadata (w/o caching):  |  | \n",
    "| `0:04:30` minutes | Aggegation steps (all) | Note that this might only be the time to create the dag, not necessarily the time to actually compute the data | \n",
    "| `0:37:24` minutes | Calculate similarities  |  | \n",
    "| `1:30:00` HOURS | Saving & logging files | Saving alone could take more than 1 hour... mand I'd forgotten about this | \n",
    "|  |  |  | \n",
    "\n",
    "\n",
    "Note that there's very different ETAs for saving each DF, the first 2 are really large and take a long time. The last few are smaller, so the time estimates from `tqdm` can vary a ton:\n",
    "```bash\n",
    "3/11 [40:25<1:14:34, 559.33s/it]   27%\n",
    "9/11 [50:22<03:22, 101.02s/it]     82% \n",
    "11/11 [1:30:11<00:00, 750.74s/it] 100%\n",
    "```\n",
    "\n",
    "\n",
    "Getting shape of `dask df` is taking almost half of the saving time!\n",
    "\n",
    "**TODO: REMOVE** logging df shape for now to save a ton of time!\n",
    "\n",
    "```bash\n",
    "20:54:05 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
    "20:54:05 | INFO | \"Saving locally...\"                                   # get_df_shape() starts here...\n",
    "21:13:56 | INFO | \"  Saving existing dask df as parquet...\"             # get df_shape() ends here, ABOUT 40 MINUTES!\n",
    "21:33:11 | INFO | \"Logging artifact to mlflow...\"                       # In contrast, SAVING the dask df only takes about 20 MINUTES!\n",
    "21:33:14 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"    # And logging the dfs up to GCS only takes about 3 seconds?!\n",
    "21:33:14 | INFO | \"Saving locally...\"\n",
    "21:33:14 | INFO | \"Keeping index intact...\"\n",
    "21:33:14 | INFO | \"Converting pandas to dask...\"\n",
    "21:33:15 | INFO | \"   185.4 MB <- Memory usage\"\n",
    "21:33:15 | INFO | \"       5\t<- target Dask partitions\t   40.0 <- target MB partition size\"\n",
    "21:33:19 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:33:22 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity_pair **\"\n",
    "21:33:22 | INFO | \"Saving locally...\"\n",
    "21:33:24 | INFO | \"Converting pandas to dask...\"\n",
    "21:33:35 | INFO | \"  10,391.8 MB <- Memory usage\"\n",
    "21:33:35 | INFO | \"     139\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
    "21:33:55 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:34:30 | INFO | \"** df_sub_level_agg_b_post_and_comments **\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c2435",
   "metadata": {},
   "source": [
    "More details in log file:\n",
    "\n",
    "`logs/AggregateEmbeddings/2021-10-05_195710-sample_test_lc_false-2021-10-05_195710.log`\n",
    "\n",
    "\n",
    "```bash\n",
    "# load raw embeddings (w/o caching): 11:43 minutes\n",
    "# ---\n",
    "20:01:19 | INFO | \"Local folder to download artifact(s):\n",
    "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/mlflow/mlruns/14/2fcfefc3d5af43328168d3478b4fdeb6/artifacts/df_vect_comments\"\n",
    "40/40 [07:29<00:00, 8.17s/it]\n",
    "20:08:49 | INFO | \"  Parquet files found: 5\"\n",
    "20:08:49 | INFO | \"  Keep only comments for posts with embeddings\"\n",
    "20:08:54 | INFO | \"  0:11:43.326935 <- Total raw embeddings load time elapsed\"\n",
    "\n",
    "\n",
    "# Load metadata (w/o caching): 3:15 minutes\n",
    "# ---\n",
    "20:08:54 | INFO | \"-- Start _load_metadata() method --\"\n",
    "20:08:54 | INFO | \"Loading POSTS metadata...\"\n",
    "\n",
    "20:10:39 | INFO | \"Local folder to download artifact(s):\n",
    "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/comments/top/2021-10-04\"\n",
    "100%\n",
    "59/59 [01:30<00:00, 1.43s/it]\n",
    "20:12:10 | INFO | \"  (Delayed('int-e6188e6d-6319-487d-b054-bea8a30d912b'), 7) <- Raw META COMMENTS shape\"\n",
    "20:12:10 | INFO | \"  0:03:15.218880 <- Total metadata loading time elapsed\"\n",
    "\n",
    "\n",
    "# Aggegation steps (all): 4:30 minutes\n",
    "#   Note that this might only be the time to create the dag, not necessarily the time to actually compute the data\n",
    "20:12:10 | INFO | \"-- Start _agg_comments_to_post_level() method --\"\n",
    "20:12:10 | INFO | \"Getting count of comments per post...\"\n",
    "20:12:39 | INFO | \"Filtering which comments need to be averaged...\"\n",
    "20:13:23 | INFO | \"       22,197 <- Comments that DON'T need to be averaged\"\n",
    "20:13:23 | INFO | \"    1,087,458 <- Comments that need to be averaged\"\n",
    "20:13:28 | INFO | \"No column to weight comments, simple mean for comments at post level\"\n",
    "20:13:57 | INFO | \"      191,558 |  514 <- df_v_com_agg SHAPE\"\n",
    "20:13:57 | INFO | \"  0:01:46.878385 <- Total comments to post agg loading time elapsed\"\n",
    "20:13:57 | INFO | \"-- Start (df_posts_agg_b) _agg_posts_and_comments_to_post_level() method --\"\n",
    "20:13:59 | INFO | \"DEFINE agg_posts_w_comments...\"\n",
    "...\n",
    "20:16:38 | INFO | \"A - posts only\"\n",
    "20:16:39 | INFO | \"  (Delayed('int-3ee084d4-434c-4a38-aeb1-185b50648908'), 513) <- df_subs_agg_a.shape (only posts)\"\n",
    "20:16:39 | INFO | \"B - posts + comments\"\n",
    "20:16:39 | INFO | \"  (Delayed('int-6bd21f72-fc1d-495c-987a-1da6e4a18683'), 513) <- df_subs_agg_b.shape (posts + comments)\"\n",
    "20:16:39 | INFO | \"C - posts + comments + sub descriptions\"\n",
    "20:16:40 | INFO | \"  (Delayed('int-59c54047-7ac8-49cb-81b3-478b4ae5b60e'), 513) <- df_subs_agg_c.shape (posts + comments + sub description)\"\n",
    "20:16:40 | INFO | \"  0:00:01.507065 <- Total for ALL subreddit-level agg time elapsed\"\n",
    "\n",
    "\n",
    "# Calculate similarities 37:24 minutes\n",
    "20:16:40 | INFO | \"-- Start _calculate_subreddit_similarities() method --\"\n",
    "20:16:40 | INFO | \"A...\"\n",
    "20:16:56 | INFO | \"  (4924, 4924) <- df_subs_agg_a_similarity.shape\"\n",
    "20:17:21 | INFO | \"Merge distance + metadata...\"\n",
    "20:17:59 | INFO | \"Create new df to keep only top 20 subs by distance...\"\n",
    "20:18:10 | INFO | \"  (24240852, 11) <- df_dist_pair_meta.shape (before setting index)\"\n",
    "20:18:10 | INFO | \"  (98480, 11) <- df_dist_pair_meta_top_only.shape (before setting index)\"\n",
    "...\n",
    "20:54:04 | INFO | \"  0:37:24.347689 <- Total for _calculate_subreddit_similarities() time elapsed\"\n",
    "\n",
    "\n",
    "# *** Saving & logging file: WTF? Saving alone could take more than 2 hours!! WTF?!!  ***\n",
    "20:54:04 | INFO | \"-- Start _save_and_log_aggregate_and_similarity_dfs() method --\"\n",
    "20:54:04 | INFO | \"  Saving config to local path...\"\n",
    "20:54:04 | INFO | \"  Logging config to mlflow...\"\n",
    "*** 3/11 [40:25<1:14:34, 559.33s/it]  27%   ***\n",
    "20:54:05 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc **\"\n",
    "20:54:05 | INFO | \"Saving locally...\"\n",
    "...\n",
    "21:13:56 | INFO | \"  Saving existing dask df as parquet...\"\n",
    "21:33:11 | INFO | \"Logging artifact to mlflow...\"\n",
    "21:33:14 | INFO | \"** df_sub_level_agg_c_post_comments_and_sub_desc_similarity **\"\n",
    "21:33:14 | INFO | \"Saving locally...\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b043e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
