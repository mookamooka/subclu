{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea623b",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-09-28: Run inference on posts for v0.4.0 POSTS.\n",
    "\n",
    "Diff from before: instead of only using `text` (post title + post body), we'll be using multiple columns to concat and get the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a12ed4",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726a9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a611cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ace200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416026bf",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74eb6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee358722",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a90e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cfe82b",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3400d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e417502",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86793971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.0 inputs - Top Subreddits (no Geo) + Geo-relevant subs, comments: TBD',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-09-24',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-09-27',\n",
       " 'folder_comments_text_and_meta': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v040 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.0_19k_top_subs_and_geo_relevant_2021_09_27',\n",
    "#     config_name='top_subreddits_2021_07_16',\n",
    ")\n",
    "config_data_v040.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762900b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_inference'\n",
    "\n",
    "bucket_name = config_data_v040.config_dict['bucket_name']\n",
    "subreddits_path = config_data_v040.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = config_data_v040.config_dict['folder_posts_text_and_meta']\n",
    "comments_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57335fd1",
   "metadata": {},
   "source": [
    "# Call function to vectorize text\n",
    "\n",
    "- Batch of: 3000 \n",
    "- Limit characters to: 1000\n",
    "Finally leaves enough room to use around 50% of RAM (of 60GB)\n",
    "\n",
    "The problem is that each iteration takes around 3 minutes, which means whole job for GERMAN only will tka around 4:42 hours:mins..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31997091",
   "metadata": {},
   "source": [
    "## Test on a `sample` of posts & comments to make sure entire process works first (before running long job)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune.\n",
    "\n",
    "```\n",
    "08:27:18 | INFO | \"Start vectorize function\"\n",
    "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
    "08:27:18 | INFO | \"Loading df_posts...\n",
    "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
    "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
    "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
    "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
    "08:27:27 | INFO | \"Load comments df...\"\n",
    "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
    "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
    "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
    "08:28:11 | INFO | \"Load subreddits df...\"\n",
    "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "...\n",
    "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.67s/it]\n",
    "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
    "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
    "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:00<00:00, 2.42it/s]\n",
    "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
    "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
    "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "3/3 [00:01<00:00, 1.95it/s]\n",
    "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191805f",
   "metadata": {},
   "source": [
    "# Test new batching function\n",
    "\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Add new parameter to sample only first N files (we'll process each file individually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfc448",
   "metadata": {},
   "source": [
    "### Timing is super fast, even with a bigger sample size\n",
    "\n",
    "```\n",
    "11:40:06 | INFO | \"Start vectorize function\"\n",
    "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
    "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
    "11:40:07 | INFO | \"  Saving config to local path...\"\n",
    "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
    "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
    "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
    "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
    "11:40:10 | INFO | \"Load subreddits df...\"\n",
    "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
    "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.65s/it]\n",
    "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
    "...\n",
    "11:40:16 | INFO | \"Loading df_posts...\n",
    "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
    "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
    "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
    "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
    "100%\n",
    "5/5 [00:03<00:00, 1.59it/s]\n",
    "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
    "...\n",
    "11:40:30 | INFO | \"Load comments df...\"\n",
    "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
    "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
    "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
    "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "100%\n",
    "10/10 [00:05<00:00, 1.60it/s]\n",
    "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
    "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
    "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
    "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
    "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210ed82",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=False`\n",
    "Let's see if the current refactor is good enough or if I really need to manually batch files...\n",
    "\n",
    "**answer**: no it wasn't good enough -- 60GB of RAM wasn't good enough for 19Million comments _lol_.\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f72c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:52:09 | INFO | \"Start vectorize function\"\n",
      "17:52:09 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_175209\"\n",
      "17:52:09 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "17:52:10 | INFO | \"  Saving config to local path...\"\n",
      "17:52:10 | INFO | \"  Logging config to mlflow...\"\n",
      "17:52:10 | INFO | \"Loading model use_multilingual...\"\n",
      "17:52:13 | INFO | \"  0:00:02.347350 <- Load TF HUB model time elapsed\"\n",
      "17:52:13 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "17:52:13 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "17:52:13 | INFO | \"-- Loading & vectorizing COMMENTS in files: 2 --\n",
      "Expected batch size: 2200\"\n",
      "17:52:13 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]17:52:13 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      "17:52:15 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.03it/s]\n",
      "17:52:16 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id']\"\n",
      "17:52:16 | INFO | \"col_text: flair_post_ocr_url_text\"\n",
      "17:52:16 | INFO | \"lowercase_text: False\"\n",
      "17:52:16 | INFO | \"limit_first_n_chars: 900\"\n",
      "17:52:16 | INFO | \"Getting embeddings in batches of size: 2200\"\n",
      " 55%|#######################1                  | 74/134 [01:32<01:13,  1.23s/it]17:53:59 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[575381,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_5582594]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 134/134 [02:54<00:00,  1.30s/it]\n",
      "17:55:13 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      " 50%|#####     | 1/2 [03:10<03:10, 190.06s/it]17:55:23 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "17:55:26 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  4.98it/s]\n",
      "17:55:27 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id']\"\n",
      "17:55:27 | INFO | \"col_text: flair_post_ocr_url_text\"\n",
      "17:55:27 | INFO | \"lowercase_text: False\"\n",
      "17:55:27 | INFO | \"limit_first_n_chars: 900\"\n",
      "17:55:28 | INFO | \"Getting embeddings in batches of size: 2200\"\n",
      "100%|#########################################| 187/187 [03:50<00:00,  1.24s/it]\n",
      "17:59:22 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "100%|##########| 2/2 [07:19<00:00, 219.57s/it]\n",
      "17:59:32 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "17:59:47 | INFO | \"  0:07:37.460369 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_full_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "    \n",
    "    tf_batch_inference_rows=2400,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    n_sample_comment_files=2,\n",
    "    get_embeddings_verbose=True,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdfd271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba404d",
   "metadata": {},
   "source": [
    "## Re-do with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29e96b",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5f009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:58:53 | INFO | \"Start vectorize function\"\n",
      "21:58:53 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_215853\"\n",
      "21:58:53 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "21:58:54 | INFO | \"  Saving config to local path...\"\n",
      "21:58:54 | INFO | \"  Logging config to mlflow...\"\n",
      "21:58:54 | INFO | \"Loading model use_multilingual...\"\n",
      "21:59:03 | INFO | \"  0:00:09.057354 <- Load TF HUB model time elapsed\"\n",
      "21:59:03 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "21:59:03 | INFO | \"Load subreddits df...\"\n",
      "21:59:05 | INFO | \"  0:00:01.338779 <- df_subs loading time elapsed\"\n",
      "21:59:05 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "21:59:05 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "21:59:05 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]21:59:17 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[575014,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:27<00:00,  3.38s/it]\n",
      "21:59:32 | INFO | \"  0:00:27.413692 <- df_subs vectorizing time elapsed\"\n",
      "21:59:32 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "21:59:32 | INFO | \"Converting pandas to dask...\"\n",
      "21:59:32 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "21:59:32 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "21:59:33 | INFO | \"  Logging to mlflow...\"\n",
      "21:59:35 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "21:59:35 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2450\"\n",
      "21:59:35 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]21:59:35 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      "21:59:38 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.77it/s]\n",
      "21:59:39 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 120/120 [02:29<00:00,  1.25s/it]\n",
      "22:02:09 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:43<1:11:02, 163.95s/it]22:02:19 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "22:02:23 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  5.05it/s]\n",
      "22:02:24 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 168/168 [03:30<00:00,  1.25s/it]\n",
      "22:05:56 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:31<1:23:53, 201.35s/it]22:06:06 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "22:06:09 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.85it/s]\n",
      "22:06:10 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 135/135 [02:45<00:00,  1.22s/it]\n",
      "22:08:57 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [09:31<1:16:42, 191.77s/it]22:09:07 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "22:09:09 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.06it/s]\n",
      "22:09:10 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 104/104 [02:13<00:00,  1.28s/it]\n",
      "22:11:24 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [11:59<1:06:45, 174.17s/it]22:11:34 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "22:11:36 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.63it/s]\n",
      "22:11:37 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 113/113 [02:19<00:00,  1.24s/it]\n",
      "22:13:58 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [14:33<1:01:13, 166.97s/it]22:14:08 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "22:14:11 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.26it/s]\n",
      "22:14:12 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 146/146 [03:00<00:00,  1.24s/it]\n",
      "22:17:15 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [17:49<1:01:58, 177.06s/it]22:17:25 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "22:17:28 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.90it/s]\n",
      "22:17:29 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 134/134 [02:50<00:00,  1.28s/it]\n",
      "22:20:21 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 326,821 Rows by 515 Cols\"\n",
      " 26%|##5       | 7/27 [20:56<1:00:00, 180.02s/it]22:20:31 | INFO | \"Processing: posts/top/2021-09-27/000000000007.parquet\"\n",
      "22:20:34 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.56it/s]\n",
      "22:20:35 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 114/114 [02:24<00:00,  1.27s/it]\n",
      "22:23:00 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 277,995 Rows by 515 Cols\"\n",
      " 30%|##9       | 8/27 [23:34<54:50, 173.19s/it]  22:23:09 | INFO | \"Processing: posts/top/2021-09-27/000000000008.parquet\"\n",
      "22:23:12 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.35it/s]\n",
      "22:23:14 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 151/151 [03:14<00:00,  1.29s/it]\n",
      "22:26:29 | INFO | \"  Saving to local: df_vect_posts/000000000008 | 368,185 Rows by 515 Cols\"\n",
      " 33%|###3      | 9/27 [27:04<55:26, 184.79s/it]22:26:40 | INFO | \"Processing: posts/top/2021-09-27/000000000009.parquet\"\n",
      "22:26:42 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.64it/s]\n",
      "22:26:44 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      " 10%|####1                                     | 12/122 [00:15<02:24,  1.31s/it]22:27:09 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[641650,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 122/122 [02:47<00:00,  1.37s/it]\n",
      "22:29:32 | INFO | \"  Saving to local: df_vect_posts/000000000009 | 297,827 Rows by 515 Cols\"\n",
      " 37%|###7      | 10/27 [30:06<52:05, 183.85s/it]22:29:41 | INFO | \"Processing: posts/top/2021-09-27/000000000010.parquet\"\n",
      "22:29:44 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.34it/s]\n",
      "22:29:45 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 130/130 [02:45<00:00,  1.27s/it]\n",
      "22:32:32 | INFO | \"  Saving to local: df_vect_posts/000000000010 | 316,831 Rows by 515 Cols\"\n",
      " 41%|####      | 11/27 [33:06<48:42, 182.68s/it]22:32:41 | INFO | \"Processing: posts/top/2021-09-27/000000000011.parquet\"\n",
      "22:32:44 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.10it/s]\n",
      "22:32:45 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 124/124 [02:34<00:00,  1.24s/it]\n",
      "22:40:58 | INFO | \"  Saving to local: df_vect_posts/000000000013 | 302,591 Rows by 515 Cols\"\n",
      " 52%|#####1    | 14/27 [41:32<37:25, 172.76s/it]22:41:07 | INFO | \"Processing: posts/top/2021-09-27/000000000014.parquet\"\n",
      "22:41:10 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.89it/s]\n",
      "22:41:11 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 132/132 [02:47<00:00,  1.27s/it]\n",
      "22:44:00 | INFO | \"  Saving to local: df_vect_posts/000000000014 | 322,877 Rows by 515 Cols\"\n",
      " 56%|#####5    | 15/27 [44:34<35:07, 175.66s/it]22:44:10 | INFO | \"Processing: posts/top/2021-09-27/000000000015.parquet\"\n",
      "22:44:12 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.90it/s]\n",
      "22:44:13 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 132/132 [02:45<00:00,  1.26s/it]\n",
      "22:47:00 | INFO | \"  Saving to local: df_vect_posts/000000000015 | 322,124 Rows by 515 Cols\"\n",
      " 59%|#####9    | 16/27 [47:35<32:27, 177.07s/it]22:47:10 | INFO | \"Processing: posts/top/2021-09-27/000000000016.parquet\"\n",
      "22:47:14 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.28it/s]\n",
      "22:47:15 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 127/127 [02:38<00:00,  1.25s/it]\n",
      "22:49:55 | INFO | \"  Saving to local: df_vect_posts/000000000016 | 309,522 Rows by 515 Cols\"\n",
      " 63%|######2   | 17/27 [50:29<29:22, 176.21s/it]22:50:04 | INFO | \"Processing: posts/top/2021-09-27/000000000017.parquet\"\n",
      "22:50:08 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.12it/s]\n",
      "22:50:09 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 129/129 [02:46<00:00,  1.29s/it]\n",
      "22:52:56 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 315,380 Rows by 515 Cols\"\n",
      " 67%|######6   | 18/27 [53:31<26:40, 177.88s/it]22:53:06 | INFO | \"Processing: posts/top/2021-09-27/000000000018.parquet\"\n",
      "22:53:10 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.36it/s]\n",
      "22:53:11 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 123/123 [02:36<00:00,  1.27s/it]\n",
      "22:55:49 | INFO | \"  Saving to local: df_vect_posts/000000000018 | 300,659 Rows by 515 Cols\"\n",
      " 70%|#######   | 19/27 [56:23<23:29, 176.18s/it]22:55:58 | INFO | \"Processing: posts/top/2021-09-27/000000000019.parquet\"\n",
      "22:56:01 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00, 10.67it/s]\n",
      "22:56:02 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 102/102 [02:09<00:00,  1.27s/it]\n",
      "22:58:12 | INFO | \"  Saving to local: df_vect_posts/000000000019 | 248,516 Rows by 515 Cols\"\n",
      " 74%|#######4  | 20/27 [58:46<19:23, 166.15s/it]22:58:21 | INFO | \"Processing: posts/top/2021-09-27/000000000020.parquet\"\n",
      "22:58:24 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.89it/s]\n",
      "22:58:25 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 122/122 [02:35<00:00,  1.28s/it]\n",
      "23:01:02 | INFO | \"  Saving to local: df_vect_posts/000000000020 | 298,379 Rows by 515 Cols\"\n",
      " 78%|#######7  | 21/27 [1:01:36<16:45, 167.50s/it]23:01:11 | INFO | \"Processing: posts/top/2021-09-27/000000000021.parquet\"\n",
      "23:01:16 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.62it/s]\n",
      "23:01:17 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 137/137 [02:55<00:00,  1.28s/it]\n",
      "23:04:13 | INFO | \"  Saving to local: df_vect_posts/000000000021 | 334,647 Rows by 515 Cols\"\n",
      " 81%|########1 | 22/27 [1:04:47<14:33, 174.65s/it]23:04:23 | INFO | \"Processing: posts/top/2021-09-27/000000000022.parquet\"\n",
      "23:04:27 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.46it/s]\n",
      "23:04:28 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 135/135 [02:51<00:00,  1.27s/it]\n",
      "23:07:21 | INFO | \"  Saving to local: df_vect_posts/000000000022 | 329,376 Rows by 515 Cols\"\n",
      " 85%|########5 | 23/27 [1:07:55<11:53, 178.50s/it]23:07:30 | INFO | \"Processing: posts/top/2021-09-27/000000000023.parquet\"\n",
      "23:07:36 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.46it/s]\n",
      "23:07:37 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      " 24%|##########1                               | 32/132 [00:42<01:58,  1.18s/it]23:13:21 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.91it/s]\n",
      "23:13:22 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 121/121 [02:27<00:00,  1.22s/it]\n",
      "23:15:50 | INFO | \"  Saving to local: df_vect_posts/000000000025 | 295,990 Rows by 515 Cols\"\n",
      " 96%|#########6| 26/27 [1:16:24<02:51, 171.01s/it]23:15:59 | INFO | \"Processing: posts/top/2021-09-27/000000000026.parquet\"\n",
      "23:16:05 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.35it/s]\n",
      "23:16:06 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 148/148 [02:56<00:00,  1.19s/it]\n",
      "23:19:04 | INFO | \"  Saving to local: df_vect_posts/000000000026 | 361,954 Rows by 515 Cols\"\n",
      "100%|##########| 27/27 [1:19:38<00:00, 177.00s/it]\n",
      "23:19:14 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "23:22:43 | INFO | \"  1:23:50.042083 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_concat_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "    \n",
    "    tf_batch_inference_rows=2450,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0268828c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55224"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ec19a",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff1e82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:22:44 | INFO | \"Start vectorize function\"\n",
      "23:22:44 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_232244\"\n",
      "23:22:44 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "23:22:44 | INFO | \"  Saving config to local path...\"\n",
      "23:22:44 | INFO | \"  Logging config to mlflow...\"\n",
      "23:22:45 | INFO | \"Loading model use_multilingual...\"\n",
      "23:22:47 | INFO | \"  0:00:02.176534 <- Load TF HUB model time elapsed\"\n",
      "23:22:47 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "23:22:47 | INFO | \"Load subreddits df...\"\n",
      "23:22:48 | INFO | \"  0:00:00.693968 <- df_subs loading time elapsed\"\n",
      "23:22:48 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "23:22:48 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "23:22:48 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]23:22:59 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[565648,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_33873424]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:25<00:00,  3.20s/it]\n",
      "23:23:14 | INFO | \"  0:00:26.021886 <- df_subs vectorizing time elapsed\"\n",
      "23:23:14 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "23:23:14 | INFO | \"Converting pandas to dask...\"\n",
      "23:23:14 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "23:23:14 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "23:23:14 | INFO | \"  Logging to mlflow...\"\n",
      "23:23:16 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "23:23:16 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2450\"\n",
      "23:23:16 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]23:23:16 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      "23:23:19 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.42it/s]\n",
      "23:23:20 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 120/120 [02:21<00:00,  1.18s/it]\n",
      "23:25:43 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:35<1:07:25, 155.60s/it]23:25:52 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "23:25:55 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  5.51it/s]\n",
      "23:25:56 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 168/168 [03:20<00:00,  1.19s/it]\n",
      "23:29:19 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:12<1:19:49, 191.59s/it]23:29:29 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "23:29:31 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.95it/s]\n",
      "23:29:32 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 135/135 [02:37<00:00,  1.16s/it]\n",
      "23:32:10 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [09:03<1:12:55, 182.31s/it]23:32:20 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "23:32:22 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  9.05it/s]\n",
      "23:32:23 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 104/104 [02:07<00:00,  1.23s/it]\n",
      "23:34:32 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [11:24<1:03:36, 165.93s/it]23:34:41 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "23:34:43 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.69it/s]\n",
      "23:34:44 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 113/113 [02:16<00:00,  1.21s/it]\n",
      "23:37:01 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [13:54<58:41, 160.08s/it]  23:37:11 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "23:37:13 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.64it/s]\n",
      "23:37:14 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 146/146 [02:57<00:00,  1.22s/it]\n",
      "23:40:13 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [17:06<59:52, 171.06s/it]23:40:23 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "23:40:25 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.45it/s]\n",
      "23:40:26 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 134/134 [02:47<00:00,  1.25s/it]\n",
      "23:43:16 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 326,821 Rows by 515 Cols\"\n",
      " 26%|##5       | 7/27 [20:08<58:14, 174.70s/it]23:43:25 | INFO | \"Processing: posts/top/2021-09-27/000000000007.parquet\"\n",
      "23:43:28 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  9.25it/s]\n",
      "23:43:29 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 114/114 [02:22<00:00,  1.25s/it]\n",
      "23:45:52 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 277,995 Rows by 515 Cols\"\n",
      " 30%|##9       | 8/27 [22:45<53:27, 168.84s/it]23:46:01 | INFO | \"Processing: posts/top/2021-09-27/000000000008.parquet\"\n",
      "23:46:04 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.49it/s]\n",
      "23:46:06 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 151/151 [03:11<00:00,  1.27s/it]\n",
      "23:49:19 | INFO | \"  Saving to local: df_vect_posts/000000000008 | 368,185 Rows by 515 Cols\"\n",
      " 33%|###3      | 9/27 [26:12<54:16, 180.91s/it]23:49:29 | INFO | \"Processing: posts/top/2021-09-27/000000000009.parquet\"\n",
      "23:49:31 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.56it/s]\n",
      "23:49:32 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      " 10%|####1                                     | 12/122 [00:15<02:21,  1.29s/it]23:49:58 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[621150,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_33873424]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 122/122 [02:45<00:00,  1.36s/it]\n",
      "23:52:19 | INFO | \"  Saving to local: df_vect_posts/000000000009 | 297,827 Rows by 515 Cols\"\n",
      " 37%|###7      | 10/27 [29:11<51:06, 180.40s/it]23:52:28 | INFO | \"Processing: posts/top/2021-09-27/000000000010.parquet\"\n",
      "23:52:31 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.94it/s]\n",
      "23:52:32 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 130/130 [02:44<00:00,  1.27s/it]\n",
      "23:55:18 | INFO | \"  Saving to local: df_vect_posts/000000000010 | 316,831 Rows by 515 Cols\"\n",
      " 41%|####      | 11/27 [32:10<48:00, 180.02s/it]23:55:27 | INFO | \"Processing: posts/top/2021-09-27/000000000011.parquet\"\n",
      "23:55:30 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.22it/s]\n",
      "23:55:31 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 124/124 [02:31<00:00,  1.22s/it]\n",
      "00:03:41 | INFO | \"  Saving to local: df_vect_posts/000000000013 | 302,591 Rows by 515 Cols\"\n",
      " 52%|#####1    | 14/27 [40:34<37:04, 171.09s/it]00:03:51 | INFO | \"Processing: posts/top/2021-09-27/000000000014.parquet\"\n",
      "00:03:53 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.18it/s]\n",
      "00:03:54 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 132/132 [02:48<00:00,  1.27s/it]\n",
      "00:06:44 | INFO | \"  Saving to local: df_vect_posts/000000000014 | 322,877 Rows by 515 Cols\"\n",
      " 56%|#####5    | 15/27 [43:36<34:54, 174.56s/it]00:06:53 | INFO | \"Processing: posts/top/2021-09-27/000000000015.parquet\"\n",
      "00:06:56 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.13it/s]\n",
      "00:06:57 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 132/132 [02:47<00:00,  1.27s/it]\n",
      "00:09:45 | INFO | \"  Saving to local: df_vect_posts/000000000015 | 322,124 Rows by 515 Cols\"\n",
      " 59%|#####9    | 16/27 [46:38<32:23, 176.65s/it]00:09:55 | INFO | \"Processing: posts/top/2021-09-27/000000000016.parquet\"\n",
      "00:09:57 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.53it/s]\n",
      "00:09:58 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 127/127 [02:38<00:00,  1.24s/it]\n",
      "00:12:38 | INFO | \"  Saving to local: df_vect_posts/000000000016 | 309,522 Rows by 515 Cols\"\n",
      " 63%|######2   | 17/27 [49:30<29:13, 175.37s/it]00:12:47 | INFO | \"Processing: posts/top/2021-09-27/000000000017.parquet\"\n",
      "00:12:50 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.99it/s]\n",
      "00:12:51 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 129/129 [02:47<00:00,  1.30s/it]\n",
      "00:15:40 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 315,380 Rows by 515 Cols\"\n",
      " 67%|######6   | 18/27 [52:33<26:37, 177.51s/it]00:15:50 | INFO | \"Processing: posts/top/2021-09-27/000000000018.parquet\"\n",
      "00:15:52 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.51it/s]\n",
      "00:15:53 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 123/123 [02:38<00:00,  1.29s/it]\n",
      "00:18:32 | INFO | \"  Saving to local: df_vect_posts/000000000018 | 300,659 Rows by 515 Cols\"\n",
      " 70%|#######   | 19/27 [55:25<23:28, 176.05s/it]00:18:42 | INFO | \"Processing: posts/top/2021-09-27/000000000019.parquet\"\n",
      "00:18:45 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00, 10.37it/s]\n",
      "00:18:46 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 102/102 [02:09<00:00,  1.27s/it]\n",
      "00:20:57 | INFO | \"  Saving to local: df_vect_posts/000000000019 | 248,516 Rows by 515 Cols\"\n",
      " 74%|#######4  | 20/27 [57:49<19:24, 166.31s/it]00:21:06 | INFO | \"Processing: posts/top/2021-09-27/000000000020.parquet\"\n",
      "00:21:08 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.96it/s]\n",
      "00:21:09 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 122/122 [02:35<00:00,  1.28s/it]\n",
      "00:23:46 | INFO | \"  Saving to local: df_vect_posts/000000000020 | 298,379 Rows by 515 Cols\"\n",
      " 78%|#######7  | 21/27 [1:00:39<16:43, 167.30s/it]00:23:55 | INFO | \"Processing: posts/top/2021-09-27/000000000021.parquet\"\n",
      "00:23:58 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.70it/s]\n",
      "00:23:59 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 137/137 [02:56<00:00,  1.29s/it]\n",
      "00:26:57 | INFO | \"  Saving to local: df_vect_posts/000000000021 | 334,647 Rows by 515 Cols\"\n",
      " 81%|########1 | 22/27 [1:03:50<14:32, 174.57s/it]00:27:07 | INFO | \"Processing: posts/top/2021-09-27/000000000022.parquet\"\n",
      "00:27:10 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.55it/s]\n",
      "00:27:11 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 135/135 [02:54<00:00,  1.29s/it]\n",
      "00:30:06 | INFO | \"  Saving to local: df_vect_posts/000000000022 | 329,376 Rows by 515 Cols\"\n",
      " 85%|########5 | 23/27 [1:06:59<11:55, 178.87s/it]00:30:16 | INFO | \"Processing: posts/top/2021-09-27/000000000023.parquet\"\n",
      "00:30:19 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.71it/s]\n",
      "00:30:20 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 112/112 [02:25<00:00,  1.30s/it]\n",
      "00:35:52 | INFO | \"  Saving to local: df_vect_posts/000000000024 | 273,047 Rows by 515 Cols\"\n",
      " 93%|#########2| 25/27 [1:12:44<05:48, 174.47s/it]00:36:01 | INFO | \"Processing: posts/top/2021-09-27/000000000025.parquet\"\n",
      "00:36:04 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.88it/s]\n",
      "00:36:05 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 121/121 [02:29<00:00,  1.23s/it]\n",
      "00:38:35 | INFO | \"  Saving to local: df_vect_posts/000000000025 | 295,990 Rows by 515 Cols\"\n",
      " 96%|#########6| 26/27 [1:15:28<02:51, 171.40s/it]00:38:45 | INFO | \"Processing: posts/top/2021-09-27/000000000026.parquet\"\n",
      "00:38:48 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.44it/s]\n",
      "00:38:49 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 148/148 [02:57<00:00,  1.20s/it]\n",
      "00:41:47 | INFO | \"  Saving to local: df_vect_posts/000000000026 | 361,954 Rows by 515 Cols\"\n",
      "100%|##########| 27/27 [1:18:40<00:00, 174.84s/it]\n",
      "00:41:57 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "00:45:14 | INFO | \"  1:22:30.406175 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_concat_text_lowercase-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "    \n",
    "    tf_batch_inference_rows=2450,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c619fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d7710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51693e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3dfe72",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False (legacy fse/fasttext)\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85b52e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
