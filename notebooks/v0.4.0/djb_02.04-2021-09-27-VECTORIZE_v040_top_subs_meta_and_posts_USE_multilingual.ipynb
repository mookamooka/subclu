{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87998430",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-07-28: Run it on the top Subreddits + German subs. Ideally this should help us find counterpart subs in other languages.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2e958",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba4c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51eb3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8294252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3499e",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245d8c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8aca78",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1729c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939adf84",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc6315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06a802",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69850ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.0 inputs - Top Subreddits (no Geo) + Geo-relevant subs, comments: TBD',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-09-24',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-09-27',\n",
       " 'folder_comments_text_and_meta': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v040 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.0_19k_top_subs_and_geo_relevant_2021_09_27',\n",
    "#     config_name='top_subreddits_2021_07_16',\n",
    ")\n",
    "config_data_v040.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75582139",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_inference'\n",
    "\n",
    "bucket_name = config_data_v040.config_dict['bucket_name']\n",
    "subreddits_path = config_data_v040.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = config_data_v040.config_dict['folder_posts_text_and_meta']\n",
    "# comments_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c9352",
   "metadata": {},
   "source": [
    "# Call function to vectorize text\n",
    "\n",
    "- Batch of: 3000 \n",
    "- Limit characters to: 1000\n",
    "Finally leaves enough room to use around 50% of RAM (of 60GB)\n",
    "\n",
    "The problem is that each iteration takes around 3 minutes, which means whole job for GERMAN only will tka around 4:42 hours:mins..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4cd62",
   "metadata": {},
   "source": [
    "```\n",
    "When subreddit_id column was missing:\n",
    "CPU times: user 75.8 ms, sys: 21.1 ms, total: 96.9 ms\n",
    "Wall time: 884 ms\n",
    "(3767, 28)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41608a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns in subreddit meta...\n",
    "# %%time\n",
    "\n",
    "# df_subs = pd.read_parquet(\n",
    "#     path=f\"gs://{bucket_name}/{subreddits_path}\",\n",
    "#     # columns=l_cols_subreddits,\n",
    "# )\n",
    "# df_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4da3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589e608",
   "metadata": {},
   "source": [
    "## Test on a `sample` of posts & comments to make sure entire process works first (before running long job)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune.\n",
    "\n",
    "```\n",
    "08:27:18 | INFO | \"Start vectorize function\"\n",
    "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
    "08:27:18 | INFO | \"Loading df_posts...\n",
    "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
    "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
    "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
    "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
    "08:27:27 | INFO | \"Load comments df...\"\n",
    "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
    "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
    "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
    "08:28:11 | INFO | \"Load subreddits df...\"\n",
    "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "...\n",
    "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.67s/it]\n",
    "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
    "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
    "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:00<00:00, 2.42it/s]\n",
    "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
    "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
    "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "3/3 [00:01<00:00, 1.95it/s]\n",
    "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90379e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:07:25 | INFO | \"Start vectorize function\"\n",
      "07:07:25 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_0707\"\n",
      "07:07:25 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-09-27\"\n",
      "07:07:46 | INFO | \"  0:00:21.124010 <- df_post time elapsed\"\n",
      "07:07:46 | INFO | \"  (8439672, 6) <- df_posts.shape\"\n",
      "07:07:51 | INFO | \"  Sampling posts down to: 2,500\"\n",
      "07:07:52 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
      "07:07:52 | INFO | \"Load subreddits df...\"\n",
      "07:07:53 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "07:07:53 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "07:07:54 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n",
      "07:08:02 | INFO | \"  0:00:08.667182 <- Load TF HUB model time elapsed\"\n",
      "07:08:02 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "07:08:02 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "07:08:02 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385ada2fe38b4893863676e3785bfe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:08:17 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "07:08:18 | INFO | \"  Logging to mlflow...\"\n",
      "07:08:19 | INFO | \"Vectorizing POSTS...\"\n",
      "07:08:19 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedd9e2a61a84e06a449c7abfb107727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:08:20 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "07:08:20 | INFO | \"  Logging to mlflow...\"\n",
      "07:08:21 | INFO | \"  0:00:55.876173 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='test_n_samples',\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=None,\n",
    "    \n",
    "    tf_batch_inference_rows=2000,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=2500,\n",
    "    n_sample_comments=5100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54507960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19262, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>askreddit</th>\n",
       "      <th>t5_2qh1i</th>\n",
       "      <td>-0.015748</td>\n",
       "      <td>-0.057748</td>\n",
       "      <td>-0.012613</td>\n",
       "      <td>0.034228</td>\n",
       "      <td>0.070311</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>0.052430</td>\n",
       "      <td>-0.052749</td>\n",
       "      <td>0.053685</td>\n",
       "      <td>-0.021486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pics</th>\n",
       "      <th>t5_2qh0u</th>\n",
       "      <td>-0.056925</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>-0.009723</td>\n",
       "      <td>-0.009849</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.045963</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>-0.061319</td>\n",
       "      <td>0.053243</td>\n",
       "      <td>-0.052809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <th>t5_2qh33</th>\n",
       "      <td>0.049139</td>\n",
       "      <td>-0.038282</td>\n",
       "      <td>-0.031780</td>\n",
       "      <td>-0.030479</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.056662</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>0.047494</td>\n",
       "      <td>-0.043262</td>\n",
       "      <td>0.012178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memes</th>\n",
       "      <th>t5_2qjpg</th>\n",
       "      <td>-0.022032</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>-0.067775</td>\n",
       "      <td>-0.026031</td>\n",
       "      <td>0.061845</td>\n",
       "      <td>0.063659</td>\n",
       "      <td>-0.066072</td>\n",
       "      <td>0.048686</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.020530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interestingasfuck</th>\n",
       "      <th>t5_2qhsa</th>\n",
       "      <td>-0.020677</td>\n",
       "      <td>0.061429</td>\n",
       "      <td>-0.029565</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>0.066374</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>0.069265</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.044498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name    subreddit_id                                                                                                                                            \n",
       "askreddit         t5_2qh1i         -0.015748     -0.057748     -0.012613      0.034228      0.070311      0.045727      0.052430     -0.052749      0.053685     -0.021486\n",
       "pics              t5_2qh0u         -0.056925      0.027936     -0.009723     -0.009849      0.043200      0.045963      0.049922     -0.061319      0.053243     -0.052809\n",
       "funny             t5_2qh33          0.049139     -0.038282     -0.031780     -0.030479      0.074800      0.056662      0.006651      0.047494     -0.043262      0.012178\n",
       "memes             t5_2qjpg         -0.022032      0.010012     -0.067775     -0.026031      0.061845      0.063659     -0.066072      0.048686      0.035556      0.020530\n",
       "interestingasfuck t5_2qhsa         -0.020677      0.061429     -0.029565      0.029978      0.066374      0.061271      0.069265      0.028228      0.004899      0.044498"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect_subs.shape)\n",
    "df_vect_subs.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "526c35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xboxone</th>\n",
       "      <th>t5_2xbci</th>\n",
       "      <th>t3_ozpl4x</th>\n",
       "      <td>-0.043297</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>-0.067325</td>\n",
       "      <td>0.020786</td>\n",
       "      <td>-0.012332</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.071557</td>\n",
       "      <td>-0.074539</td>\n",
       "      <td>-0.047362</td>\n",
       "      <td>0.030414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>celeb_nylons</th>\n",
       "      <th>t5_wqh04</th>\n",
       "      <th>t3_pj7bnw</th>\n",
       "      <td>0.115459</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>0.011925</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>-0.045785</td>\n",
       "      <td>0.040692</td>\n",
       "      <td>-0.096398</td>\n",
       "      <td>-0.007461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animalsbeingmoms</th>\n",
       "      <th>t5_37h9r</th>\n",
       "      <th>t3_pdgq6w</th>\n",
       "      <td>-0.042089</td>\n",
       "      <td>-0.014872</td>\n",
       "      <td>-0.015677</td>\n",
       "      <td>-0.008563</td>\n",
       "      <td>-0.128294</td>\n",
       "      <td>0.019726</td>\n",
       "      <td>0.016793</td>\n",
       "      <td>0.051896</td>\n",
       "      <td>-0.060583</td>\n",
       "      <td>-0.043760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preppers</th>\n",
       "      <th>t5_2riow</th>\n",
       "      <th>t3_p5gmfl</th>\n",
       "      <td>-0.057952</td>\n",
       "      <td>0.077444</td>\n",
       "      <td>-0.008418</td>\n",
       "      <td>-0.015304</td>\n",
       "      <td>0.025608</td>\n",
       "      <td>0.073165</td>\n",
       "      <td>-0.061645</td>\n",
       "      <td>0.046887</td>\n",
       "      <td>0.042444</td>\n",
       "      <td>-0.026510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ancientcoins</th>\n",
       "      <th>t5_2wmz0</th>\n",
       "      <th>t3_oyojy8</th>\n",
       "      <td>-0.044230</td>\n",
       "      <td>0.026513</td>\n",
       "      <td>-0.041851</td>\n",
       "      <td>-0.017429</td>\n",
       "      <td>-0.079371</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>-0.079442</td>\n",
       "      <td>-0.042409</td>\n",
       "      <td>-0.040717</td>\n",
       "      <td>-0.022141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name   subreddit_id post_id                                                                                                                                              \n",
       "xboxone          t5_2xbci     t3_ozpl4x     -0.043297      0.020574     -0.067325      0.020786     -0.012332      0.005021      0.071557     -0.074539     -0.047362      0.030414\n",
       "celeb_nylons     t5_wqh04     t3_pj7bnw      0.115459      0.076455      0.011925      0.045632      0.015686      0.003254     -0.045785      0.040692     -0.096398     -0.007461\n",
       "animalsbeingmoms t5_37h9r     t3_pdgq6w     -0.042089     -0.014872     -0.015677     -0.008563     -0.128294      0.019726      0.016793      0.051896     -0.060583     -0.043760\n",
       "preppers         t5_2riow     t3_p5gmfl     -0.057952      0.077444     -0.008418     -0.015304      0.025608      0.073165     -0.061645      0.046887      0.042444     -0.026510\n",
       "ancientcoins     t5_2wmz0     t3_oyojy8     -0.044230      0.026513     -0.041851     -0.017429     -0.079371      0.001800     -0.079442     -0.042409     -0.040717     -0.022141"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect.shape)\n",
    "df_vect.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3431329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_vect_comments.shape)\n",
    "# df_vect_comments.iloc[10:15, -10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650ba2a",
   "metadata": {},
   "source": [
    "# Test new batching function\n",
    "\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Add new parameter to sample only first N files (we'll process each file individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4894d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.bucket(bucket_name)\n",
    "# # folder = configs.gcp_storage_folder\n",
    "\n",
    "# # print( str(configs.gcp_bucket) +\"/\"+ str(folder))\n",
    "# for blob in tqdm(list(bucket.list_blobs(prefix=posts_path))[:5]):\n",
    "# #     print(blob)\n",
    "#     print(blob.name)\n",
    "#     print(blob.name.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71bc569e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:38:08 | INFO | \"Start vectorize function\"\n",
      "07:38:08 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_073808\"\n",
      "07:38:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "07:38:08 | INFO | \"  Saving config to local path...\"\n",
      "07:38:08 | INFO | \"  Logging config to mlflow...\"\n",
      "07:38:09 | INFO | \"Loading model use_multilingual...\"\n",
      "07:38:12 | INFO | \"  0:00:03.218737 <- Load TF HUB model time elapsed\"\n",
      "07:38:12 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "07:38:12 | INFO | \"Load subreddits df...\"\n",
      "07:38:13 | INFO | \"  0:00:00.915581 <- df_subs loading time elapsed\"\n",
      "07:38:13 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "07:38:13 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "07:38:13 | INFO | \"Getting embeddings in batches of size: 2200\"\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]07:38:25 | WARNING | \"ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[573337,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\"\n",
      "100%|#############################################| 9/9 [00:27<00:00,  3.10s/it]\n",
      "07:38:41 | INFO | \"  0:00:28.303091 <- df_subs vectorizing time elapsed\"\n",
      "07:38:41 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "07:38:41 | INFO | \"Converting pandas to dask...\"\n",
      "07:38:41 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "07:38:41 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "07:38:42 | INFO | \"  Logging to mlflow...\"\n",
      "07:38:44 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-09-27\"\n",
      "07:39:04 | INFO | \"  0:00:20.336083 <- df_post loading time elapsed\"\n",
      "07:39:04 | INFO | \"  (8439672, 6) <- df_posts.shape\"\n",
      "07:39:09 | INFO | \"  Sampling posts down to: 3,500\"\n",
      "07:39:10 | INFO | \"  (3500, 6) <- df_posts.shape AFTER sampling\"\n",
      "07:39:10 | INFO | \"Vectorizing POSTS...\"\n",
      "07:39:11 | INFO | \"Getting embeddings in batches of size: 2200\"\n",
      "100%|#############################################| 2/2 [00:01<00:00,  1.04it/s]\n",
      "07:39:13 | INFO | \"  0:00:02.280381 <- df_posts vectorizing time elapsed\"\n",
      "07:39:13 | INFO | \"  Saving to local: df_vect_posts/df | 3,500 Rows by 515 Cols\"\n",
      "07:39:13 | INFO | \"Converting pandas to dask...\"\n",
      "07:39:13 | INFO | \"     7.5 MB <- Memory usage\"\n",
      "07:39:13 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "07:39:13 | INFO | \"  Logging to mlflow...\"\n",
      "07:39:14 | INFO | \"  0:01:06.291165 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"batch_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=None,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,  # Getting OOM errors with 1,000 chars\n",
    "    n_sample_posts=4500,\n",
    "    n_sample_comments=4100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73977ca6",
   "metadata": {},
   "source": [
    "### Timing is super fast, even with a bigger sample size\n",
    "\n",
    "```\n",
    "11:40:06 | INFO | \"Start vectorize function\"\n",
    "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
    "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
    "11:40:07 | INFO | \"  Saving config to local path...\"\n",
    "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
    "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
    "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
    "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
    "11:40:10 | INFO | \"Load subreddits df...\"\n",
    "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
    "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.65s/it]\n",
    "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
    "...\n",
    "11:40:16 | INFO | \"Loading df_posts...\n",
    "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
    "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
    "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
    "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
    "100%\n",
    "5/5 [00:03<00:00, 1.59it/s]\n",
    "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
    "...\n",
    "11:40:30 | INFO | \"Load comments df...\"\n",
    "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
    "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
    "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
    "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "100%\n",
    "10/10 [00:05<00:00, 1.60it/s]\n",
    "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
    "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
    "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
    "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
    "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1436e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run(status='KILLED')\n",
    "\n",
    "# vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "#     model_name='use_multilingual',\n",
    "#     run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "#     mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "#     tokenize_lowercase=False,\n",
    "    \n",
    "#     bucket_name=bucket_name,\n",
    "#     subreddits_path=subreddits_path,\n",
    "#     posts_path=posts_path,\n",
    "#     comments_path=comments_path,\n",
    "    \n",
    "#     tf_batch_inference_rows=2100,\n",
    "#     tf_limit_first_n_chars=1000,\n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255177cb",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=False`\n",
    "Let's see if the current refactor is good enough or if I really need to manually batch files...\n",
    "\n",
    "**answer**: no it wasn't good enough -- 60GB of RAM wasn't good enough for 19Million comments _lol_.\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6eb93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91aea36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:13 | INFO | \"Start vectorize function\"\n",
      "08:28:13 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_082813\"\n",
      "08:28:13 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "08:28:13 | INFO | \"  Saving config to local path...\"\n",
      "08:28:13 | INFO | \"  Logging config to mlflow...\"\n",
      "08:28:14 | INFO | \"Loading model use_multilingual...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 817 calls to <function recreate_function.<locals>.restored_function_body at 0x7f2195fe5dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:16 | INFO | \"  0:00:02.249177 <- Load TF HUB model time elapsed\"\n",
      "08:28:16 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "08:28:16 | INFO | \"Load subreddits df...\"\n",
      "08:28:17 | INFO | \"  0:00:00.686549 <- df_subs loading time elapsed\"\n",
      "08:28:17 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "08:28:17 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "08:28:17 | INFO | \"Getting embeddings in batches of size: 2300\"\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 818 calls to <function recreate_function.<locals>.restored_function_body at 0x7f219c0a9dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#############################################| 9/9 [00:17<00:00,  1.95s/it]\n",
      "08:28:35 | INFO | \"  0:00:18.179895 <- df_subs vectorizing time elapsed\"\n",
      "08:28:35 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "08:28:35 | INFO | \"Converting pandas to dask...\"\n",
      "08:28:35 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "08:28:35 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "08:28:36 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:37 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "08:28:37 | INFO | \"-- Loading & vectorizing COMMENTS in files: 2 --\n",
      "Expected batch size: 2300\"\n",
      "08:28:37 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]08:28:37 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      " 55%|#######################2                  | 71/128 [01:23<01:10,  1.24s/it]08:30:14 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[598746,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_8732604]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 128/128 [02:39<00:00,  1.25s/it]\n",
      "08:31:21 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      " 50%|#####     | 1/2 [02:52<02:52, 172.98s/it]08:31:30 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "100%|#########################################| 179/179 [03:30<00:00,  1.18s/it]\n",
      "08:35:06 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "100%|##########| 2/2 [06:38<00:00, 199.24s/it]\n",
      "08:35:16 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "08:35:31 | INFO | \"  0:07:18.544643 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2500,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=2,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d56a25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfc184",
   "metadata": {},
   "source": [
    "## Test - Re-do comments with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8743d4",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fdc7d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:42:39 | INFO | \"Start vectorize function\"\n",
      "08:42:39 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_084239\"\n",
      "08:42:39 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "08:42:40 | INFO | \"  Saving config to local path...\"\n",
      "08:42:40 | INFO | \"  Logging config to mlflow...\"\n",
      "08:42:40 | INFO | \"Loading model use_multilingual...\"\n",
      "08:42:42 | INFO | \"  0:00:02.266991 <- Load TF HUB model time elapsed\"\n",
      "08:42:42 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "08:42:42 | INFO | \"Load subreddits df...\"\n",
      "08:42:43 | INFO | \"  0:00:00.692434 <- df_subs loading time elapsed\"\n",
      "08:42:43 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "08:42:43 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "08:42:44 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]08:42:55 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[577467,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_11636638]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:26<00:00,  3.37s/it]\n",
      "08:43:11 | INFO | \"  0:00:27.608167 <- df_subs vectorizing time elapsed\"\n",
      "08:43:11 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "08:43:11 | INFO | \"Converting pandas to dask...\"\n",
      "08:43:11 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "08:43:11 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "08:43:11 | INFO | \"  Logging to mlflow...\"\n",
      "08:43:13 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "08:43:13 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2600\"\n",
      "08:43:13 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]08:43:13 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      " 56%|#######################4                  | 63/113 [01:19<01:05,  1.30s/it]08:44:46 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[594203,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_11636638]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 113/113 [02:32<00:00,  1.35s/it]\n",
      "08:45:49 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:45<1:11:54, 165.92s/it]08:45:59 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "100%|#########################################| 158/158 [03:18<00:00,  1.26s/it]\n",
      "08:49:22 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:19<1:20:48, 193.95s/it]08:49:33 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "100%|#########################################| 127/127 [02:38<00:00,  1.25s/it]\n",
      "08:52:15 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [09:12<1:13:39, 184.16s/it]08:52:25 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "100%|###########################################| 98/98 [02:06<00:00,  1.29s/it]\n",
      "08:54:36 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [11:31<1:03:52, 166.62s/it]08:54:45 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "100%|#########################################| 106/106 [02:14<00:00,  1.27s/it]\n",
      "08:57:03 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [13:59<58:36, 159.82s/it]  08:57:13 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "100%|#########################################| 138/138 [02:59<00:00,  1.30s/it]\n",
      "09:00:16 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [17:12<59:56, 171.27s/it]09:00:26 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "100%|#########################################| 126/126 [02:48<00:00,  1.34s/it]\n",
      "09:03:19 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 326,821 Rows by 515 Cols\"\n",
      " 26%|##5       | 7/27 [20:15<58:17, 174.86s/it]09:03:28 | INFO | \"Processing: posts/top/2021-09-27/000000000007.parquet\"\n",
      "100%|#########################################| 107/107 [02:22<00:00,  1.33s/it]\n",
      "09:05:55 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 277,995 Rows by 515 Cols\"\n",
      " 30%|##9       | 8/27 [22:51<53:30, 168.95s/it]09:06:05 | INFO | \"Processing: posts/top/2021-09-27/000000000008.parquet\"\n",
      "100%|#########################################| 142/142 [03:14<00:00,  1.37s/it]\n",
      "09:09:23 | INFO | \"  Saving to local: df_vect_posts/000000000008 | 368,185 Rows by 515 Cols\"\n",
      " 33%|###3      | 9/27 [26:19<54:22, 181.23s/it]09:09:33 | INFO | \"Processing: posts/top/2021-09-27/000000000009.parquet\"\n",
      "100%|#########################################| 115/115 [02:37<00:00,  1.37s/it]\n",
      "09:12:14 | INFO | \"  Saving to local: df_vect_posts/000000000009 | 297,827 Rows by 515 Cols\"\n",
      " 37%|###7      | 10/27 [29:10<50:26, 178.06s/it]09:12:24 | INFO | \"Processing: posts/top/2021-09-27/000000000010.parquet\"\n",
      "100%|#########################################| 122/122 [02:45<00:00,  1.36s/it]\n",
      "09:15:13 | INFO | \"  Saving to local: df_vect_posts/000000000010 | 316,831 Rows by 515 Cols\"\n",
      " 41%|####      | 11/27 [32:09<47:33, 178.35s/it]09:15:23 | INFO | \"Processing: posts/top/2021-09-27/000000000011.parquet\"\n",
      "100%|#########################################| 117/117 [02:33<00:00,  1.31s/it]\n",
      "09:23:37 | INFO | \"  Saving to local: df_vect_posts/000000000013 | 302,591 Rows by 515 Cols\"\n",
      " 52%|#####1    | 14/27 [40:33<36:59, 170.76s/it]09:23:47 | INFO | \"Processing: posts/top/2021-09-27/000000000014.parquet\"\n",
      "100%|#########################################| 125/125 [02:51<00:00,  1.37s/it]\n",
      "09:26:43 | INFO | \"  Saving to local: df_vect_posts/000000000014 | 322,877 Rows by 515 Cols\"\n",
      " 56%|#####5    | 15/27 [43:39<35:03, 175.26s/it]09:26:52 | INFO | \"Processing: posts/top/2021-09-27/000000000015.parquet\"\n",
      "100%|#########################################| 124/124 [02:50<00:00,  1.38s/it]\n",
      "09:29:47 | INFO | \"  Saving to local: df_vect_posts/000000000015 | 322,124 Rows by 515 Cols\"\n",
      " 59%|#####9    | 16/27 [46:43<32:37, 177.95s/it]09:29:57 | INFO | \"Processing: posts/top/2021-09-27/000000000016.parquet\"\n",
      "100%|#########################################| 120/120 [02:41<00:00,  1.35s/it]\n",
      "09:32:42 | INFO | \"  Saving to local: df_vect_posts/000000000016 | 309,522 Rows by 515 Cols\"\n",
      " 63%|######2   | 17/27 [49:38<29:29, 176.95s/it]09:32:51 | INFO | \"Processing: posts/top/2021-09-27/000000000017.parquet\"\n",
      "100%|#########################################| 122/122 [02:49<00:00,  1.39s/it]\n",
      "09:35:45 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 315,380 Rows by 515 Cols\"\n",
      " 67%|######6   | 18/27 [52:40<26:48, 178.75s/it]09:35:54 | INFO | \"Processing: posts/top/2021-09-27/000000000018.parquet\"\n",
      "100%|#########################################| 116/116 [02:40<00:00,  1.38s/it]\n",
      "09:38:38 | INFO | \"  Saving to local: df_vect_posts/000000000018 | 300,659 Rows by 515 Cols\"\n",
      " 70%|#######   | 19/27 [55:34<23:36, 177.10s/it]09:38:47 | INFO | \"Processing: posts/top/2021-09-27/000000000019.parquet\"\n",
      "100%|###########################################| 96/96 [02:13<00:00,  1.39s/it]\n",
      "09:41:04 | INFO | \"  Saving to local: df_vect_posts/000000000019 | 248,516 Rows by 515 Cols\"\n",
      " 74%|#######4  | 20/27 [57:59<19:33, 167.58s/it]09:41:13 | INFO | \"Processing: posts/top/2021-09-27/000000000020.parquet\"\n",
      "100%|#########################################| 115/115 [02:39<00:00,  1.39s/it]\n",
      "09:43:56 | INFO | \"  Saving to local: df_vect_posts/000000000020 | 298,379 Rows by 515 Cols\"\n",
      " 78%|#######7  | 21/27 [1:00:51<16:53, 168.97s/it]09:44:05 | INFO | \"Processing: posts/top/2021-09-27/000000000021.parquet\"\n",
      "100%|#########################################| 129/129 [02:59<00:00,  1.39s/it]\n",
      "09:47:08 | INFO | \"  Saving to local: df_vect_posts/000000000021 | 334,647 Rows by 515 Cols\"\n",
      " 81%|########1 | 22/27 [1:04:04<14:40, 176.05s/it]09:47:18 | INFO | \"Processing: posts/top/2021-09-27/000000000022.parquet\"\n",
      "100%|#########################################| 127/127 [02:55<00:00,  1.38s/it]\n",
      "09:50:17 | INFO | \"  Saving to local: df_vect_posts/000000000022 | 329,376 Rows by 515 Cols\"\n",
      " 85%|########5 | 23/27 [1:07:13<11:59, 179.89s/it]09:50:26 | INFO | \"Processing: posts/top/2021-09-27/000000000023.parquet\"\n",
      "100%|#########################################| 114/114 [02:25<00:00,  1.27s/it]\n",
      "09:58:39 | INFO | \"  Saving to local: df_vect_posts/000000000025 | 295,990 Rows by 515 Cols\"\n",
      " 96%|#########6| 26/27 [1:15:35<02:49, 169.68s/it]09:58:49 | INFO | \"Processing: posts/top/2021-09-27/000000000026.parquet\"\n",
      "100%|#########################################| 140/140 [02:54<00:00,  1.25s/it]\n",
      "10:01:47 | INFO | \"  Saving to local: df_vect_posts/000000000026 | 361,954 Rows by 515 Cols\"\n",
      "100%|##########| 27/27 [1:18:43<00:00, 174.95s/it]\n",
      "10:01:57 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "10:05:01 | INFO | \"  1:22:22.539304 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2600,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb49f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55223"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26115c01",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f271a5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:05:02 | INFO | \"Start vectorize function\"\n",
      "10:05:02 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_100502\"\n",
      "10:05:02 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "10:05:03 | INFO | \"  Saving config to local path...\"\n",
      "10:05:03 | INFO | \"  Logging config to mlflow...\"\n",
      "10:05:03 | INFO | \"Loading model use_multilingual...\"\n",
      "10:05:05 | INFO | \"  0:00:02.265257 <- Load TF HUB model time elapsed\"\n",
      "10:05:05 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "10:05:05 | INFO | \"Load subreddits df...\"\n",
      "10:05:06 | INFO | \"  0:00:00.683829 <- df_subs loading time elapsed\"\n",
      "10:05:06 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "10:05:06 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "10:05:06 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]10:05:18 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[568066,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_45494289]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:26<00:00,  3.33s/it]\n",
      "10:05:33 | INFO | \"  0:00:27.219040 <- df_subs vectorizing time elapsed\"\n",
      "10:05:33 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "10:05:33 | INFO | \"Converting pandas to dask...\"\n",
      "10:05:33 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "10:05:33 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "10:05:34 | INFO | \"  Logging to mlflow...\"\n",
      "10:05:36 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "10:05:36 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2600\"\n",
      "10:05:36 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]10:05:36 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      " 56%|#######################4                  | 63/113 [01:18<01:03,  1.27s/it]10:07:07 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[594158,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_45494289]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 113/113 [02:30<00:00,  1.33s/it]\n",
      "10:08:10 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:43<1:10:52, 163.57s/it]10:08:19 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "100%|#########################################| 158/158 [03:15<00:00,  1.24s/it]\n",
      "10:11:39 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:13<1:19:28, 190.76s/it]10:11:49 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "100%|#########################################| 127/127 [02:36<00:00,  1.23s/it]\n",
      "10:14:30 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [09:03<1:12:34, 181.44s/it]10:14:40 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "100%|###########################################| 98/98 [02:03<00:00,  1.26s/it]\n",
      "10:16:46 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [11:19<1:02:35, 163.27s/it]10:16:55 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "100%|#########################################| 106/106 [02:11<00:00,  1.24s/it]\n",
      "10:19:09 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [13:42<57:16, 156.19s/it]  10:19:19 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "100%|#########################################| 138/138 [02:56<00:00,  1.28s/it]\n",
      "10:22:19 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [16:53<58:46, 167.94s/it]10:22:29 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "100%|#########################################| 126/126 [02:46<00:00,  1.32s/it]\n",
      "10:25:20 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 326,821 Rows by 515 Cols\"\n",
      " 26%|##5       | 7/27 [19:53<57:18, 171.94s/it]10:25:30 | INFO | \"Processing: posts/top/2021-09-27/000000000007.parquet\"\n",
      "100%|#########################################| 107/107 [02:21<00:00,  1.32s/it]\n",
      "10:27:54 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 277,995 Rows by 515 Cols\"\n",
      " 30%|##9       | 8/27 [22:27<52:39, 166.31s/it]10:28:04 | INFO | \"Processing: posts/top/2021-09-27/000000000008.parquet\"\n",
      "100%|#########################################| 142/142 [03:12<00:00,  1.35s/it]\n",
      "10:31:20 | INFO | \"  Saving to local: df_vect_posts/000000000008 | 368,185 Rows by 515 Cols\"\n",
      " 33%|###3      | 9/27 [25:54<53:40, 178.90s/it]10:31:30 | INFO | \"Processing: posts/top/2021-09-27/000000000009.parquet\"\n",
      "100%|#########################################| 115/115 [02:36<00:00,  1.36s/it]\n",
      "10:34:10 | INFO | \"  Saving to local: df_vect_posts/000000000009 | 297,827 Rows by 515 Cols\"\n",
      " 37%|###7      | 10/27 [28:43<49:51, 175.98s/it]10:34:20 | INFO | \"Processing: posts/top/2021-09-27/000000000010.parquet\"\n",
      "100%|#########################################| 122/122 [02:44<00:00,  1.35s/it]\n",
      "10:37:09 | INFO | \"  Saving to local: df_vect_posts/000000000010 | 316,831 Rows by 515 Cols\"\n",
      " 41%|####      | 11/27 [31:43<47:10, 176.92s/it]10:37:19 | INFO | \"Processing: posts/top/2021-09-27/000000000011.parquet\"\n",
      "100%|#########################################| 117/117 [02:44<00:00,  1.40s/it]\n",
      "10:40:07 | INFO | \"  Saving to local: df_vect_posts/000000000011 | 303,271 Rows by 515 Cols\"\n",
      " 44%|####4     | 12/27 [34:40<44:18, 177.23s/it]10:40:17 | INFO | \"Processing: posts/top/2021-09-27/000000000012.parquet\"\n",
      "100%|#########################################| 113/113 [02:28<00:00,  1.31s/it]\n",
      "10:42:49 | INFO | \"  Saving to local: df_vect_posts/000000000012 | 291,892 Rows by 515 Cols\"\n",
      " 48%|####8     | 13/27 [37:23<40:18, 172.74s/it]10:42:59 | INFO | \"Processing: posts/top/2021-09-27/000000000013.parquet\"\n",
      "100%|#########################################| 117/117 [02:34<00:00,  1.32s/it]\n",
      "10:45:38 | INFO | \"  Saving to local: df_vect_posts/000000000013 | 302,591 Rows by 515 Cols\"\n",
      " 52%|#####1    | 14/27 [40:11<37:08, 171.42s/it]10:45:48 | INFO | \"Processing: posts/top/2021-09-27/000000000014.parquet\"\n",
      "100%|#########################################| 125/125 [02:53<00:00,  1.39s/it]\n",
      "10:48:45 | INFO | \"  Saving to local: df_vect_posts/000000000014 | 322,877 Rows by 515 Cols\"\n",
      " 56%|#####5    | 15/27 [43:19<35:15, 176.26s/it]10:48:55 | INFO | \"Processing: posts/top/2021-09-27/000000000015.parquet\"\n",
      "100%|#########################################| 124/124 [02:50<00:00,  1.38s/it]\n",
      "10:51:50 | INFO | \"  Saving to local: df_vect_posts/000000000015 | 322,124 Rows by 515 Cols\"\n",
      " 59%|#####9    | 16/27 [46:24<32:48, 178.91s/it]10:52:00 | INFO | \"Processing: posts/top/2021-09-27/000000000016.parquet\"\n",
      "100%|#########################################| 120/120 [02:41<00:00,  1.35s/it]\n",
      "10:54:46 | INFO | \"  Saving to local: df_vect_posts/000000000016 | 309,522 Rows by 515 Cols\"\n",
      " 63%|######2   | 17/27 [49:19<29:38, 177.87s/it]10:54:56 | INFO | \"Processing: posts/top/2021-09-27/000000000017.parquet\"\n",
      "100%|#########################################| 122/122 [02:50<00:00,  1.39s/it]\n",
      "10:57:50 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 315,380 Rows by 515 Cols\"\n",
      " 67%|######6   | 18/27 [52:23<26:57, 179.74s/it]10:58:00 | INFO | \"Processing: posts/top/2021-09-27/000000000018.parquet\"\n",
      "100%|#########################################| 116/116 [02:43<00:00,  1.41s/it]\n",
      "11:00:47 | INFO | \"  Saving to local: df_vect_posts/000000000018 | 300,659 Rows by 515 Cols\"\n",
      " 70%|#######   | 19/27 [55:21<23:52, 179.02s/it]11:00:57 | INFO | \"Processing: posts/top/2021-09-27/000000000019.parquet\"\n",
      "100%|###########################################| 96/96 [02:13<00:00,  1.39s/it]\n",
      "11:03:14 | INFO | \"  Saving to local: df_vect_posts/000000000019 | 248,516 Rows by 515 Cols\"\n",
      " 74%|#######4  | 20/27 [57:46<19:43, 169.01s/it]11:03:23 | INFO | \"Processing: posts/top/2021-09-27/000000000020.parquet\"\n",
      "100%|#########################################| 115/115 [02:39<00:00,  1.38s/it]\n",
      "11:06:05 | INFO | \"  Saving to local: df_vect_posts/000000000020 | 298,379 Rows by 515 Cols\"\n",
      " 78%|#######7  | 21/27 [1:00:39<17:00, 170.05s/it]11:06:15 | INFO | \"Processing: posts/top/2021-09-27/000000000021.parquet\"\n",
      "100%|#########################################| 129/129 [02:58<00:00,  1.39s/it]\n",
      "11:09:18 | INFO | \"  Saving to local: df_vect_posts/000000000021 | 334,647 Rows by 515 Cols\"\n",
      " 81%|########1 | 22/27 [1:03:51<14:43, 176.79s/it]11:09:28 | INFO | \"Processing: posts/top/2021-09-27/000000000022.parquet\"\n",
      "100%|#########################################| 127/127 [02:53<00:00,  1.36s/it]\n",
      "11:12:37 | INFO | \"  Saving to local: df_vect_posts/000000000022 | 329,376 Rows by 515 Cols\"\n",
      " 85%|########5 | 23/27 [1:07:10<12:13, 183.40s/it]11:12:46 | INFO | \"Processing: posts/top/2021-09-27/000000000023.parquet\"\n",
      " 42%|#################4                        | 52/125 [01:11<01:38,  1.35s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2600,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595870d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5b988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b9ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a875267",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a9c15",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d16a36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
