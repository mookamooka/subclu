{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a31207",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "\n",
    "2022-06-29:\n",
    "Use updated pandas function to get embeddings on VM machine with a ton of RAM.\n",
    "\n",
    "Because we embedded post & text as a single embedding and we didn't use MLflow to create those embeddings, it's easier to just run the embeddings in the notebook than to re-use or re-write the old `AggregateEmbeddings` class.\n",
    "\n",
    "Provenance:\n",
    "* `v0.4.1 / djb_03.01-2021-12-aggregate_v041_posts_and_comments_pandas.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de22cd",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9e17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ec025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from logging import info\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "from subclu.models import aggregate_embeddings_pd\n",
    "\n",
    "from subclu.utils import set_working_directory, get_project_subfolder\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric,\n",
    "    elapsed_time,\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import LoadSubredditsGCS\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f2fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512c900",
   "metadata": {},
   "source": [
    "# Set Local model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf2581c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/manual_v050_2022-07-01_114214')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_model_timestamp = datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')\n",
    "path_this_model = get_project_subfolder(\n",
    "    f\"data/models/aggregate_embeddings/manual_v050_{manual_model_timestamp}\"\n",
    ")\n",
    "Path.mkdir(path_this_model, parents=True, exist_ok=True)\n",
    "path_this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060e069",
   "metadata": {},
   "source": [
    "## Paths for embeddings\n",
    "\n",
    "For v0.5.0 embeddings I didn't use mlflow to track the embeddings inference. We'll need to get them from these folders in GCS:\n",
    "\n",
    "- [Subreddit metadata](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555`\n",
    "- [Post + Comment Text (already combined)](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DATE = '20220629'\n",
    "\n",
    "BUCKET_NAME = 'i18n-subreddit-clustering'\n",
    "EMBEDDINGS_SUB_ID = '2022-06-29_084555'\n",
    "EMBEDDINGS_POST_COMMENT_ID = '2022-06-29_091925'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea1061",
   "metadata": {},
   "source": [
    "# Start MLflow & Log base params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4129cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MlflowLogger(tracking_uri='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d34331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:42:15 | INFO | \"== Start run_aggregation() method ==\"\n",
      "11:42:15 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "11:42:16 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "11:42:16 | INFO | \"cpu_count: 96\"\n",
      "11:42:16 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '1.01%', 'memory_total': '1,444,961', 'memory_used': '14,599', 'memory_free': '1,381,009'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_total': 1444961,\n",
       " 'memory_used_percent': 0.010103386873417344,\n",
       " 'memory_used': 14599,\n",
       " 'memory_free': 1381009}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-sample posts for testing\n",
    "n_posts_sample = None\n",
    "\n",
    "mlflow_experiment = 'v0.5.0_mUSE_aggregates'\n",
    "# 'v0.5.0_mUSE_aggregates', 'v0.5.0_mUSE_aggregates_test'\n",
    "\n",
    "\n",
    "t_start_agg_embed = datetime.utcnow()\n",
    "info(f\"== Start run_aggregation() method ==\")\n",
    "\n",
    "\n",
    "\n",
    "info(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "mlf.set_experiment(mlflow_experiment)\n",
    "mlflow.start_run()\n",
    "mlf.add_git_hash_to_active_run()\n",
    "mlf.set_tag_hostname(key='host_name')\n",
    "mlf.log_param_hostname(key='host_name')\n",
    "mlf.log_cpu_count()\n",
    "mlf.log_ram_stats(param=True, only_memory_used=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748c93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weights\n",
    "WEIGHT_POST_COMMENT = 0.85\n",
    "WEIGHT_SUB_META = 0.15\n",
    "assert(1.0 == WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "\n",
    "\n",
    "gcs_sub_embeddings = f'i18n_topic_model_batch/runs/{RUN_DATE}/subreddits/text/embedding/{EMBEDDINGS_SUB_ID}'\n",
    "gcs_post_comment_embeddings = (\n",
    "    f'i18n_topic_model_batch/runs/{RUN_DATE}/post_and_comment_text_combined/text_subreddit_seeds/embedding/{EMBEDDINGS_POST_COMMENT_ID}'\n",
    ")\n",
    "\n",
    "\n",
    "mlflow.log_params(\n",
    "    {\n",
    "        'embeddings_bucket': BUCKET_NAME,\n",
    "        'embeddings_subreddit_path': gcs_sub_embeddings,\n",
    "        'embeddings_post_and_comments_path': gcs_post_comment_embeddings,\n",
    "        'weight_post_and_comments': WEIGHT_POST_COMMENT,\n",
    "        'weight_subreddit_meta': WEIGHT_SUB_META,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bf776",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7178065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:42:16 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555\"\n",
      "11:42:16 | INFO | \"  5 <- Files matching prefix\"\n",
      "11:42:16 | INFO | \"  5 <- Files to check\"\n",
      "11:42:16 | INFO | \"    000000000000-196371_by_514.parquet <- File already exists, not downloading\"\n",
      "11:42:16 | INFO | \"    2022-06-29_08-45-55_vectorize_text.log <- File already exists, not downloading\"\n",
      "11:42:16 | INFO | \"  Files already cached: 2\"\n",
      "11:42:16 | INFO | \"  Files already downloaded.\"\n",
      "11:42:16 | INFO | \"  df format: pandas\"\n",
      "11:42:17 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196,371 rows, 514 cols\n",
      "CPU times: user 1.66 s, sys: 1.77 s, total: 3.42 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_start_data_load_ = datetime.utcnow()\n",
    "\n",
    "subs_v = LoadSubredditsGCS(\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    gcs_path=gcs_sub_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    ")\n",
    "subs_v.local_cache()\n",
    "\n",
    "df_v_subs = subs_v.read_as_one_df()\n",
    "r_subs, c_subs = df_v_subs.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_subs-rows\": r_subs,\n",
    "        f\"df_v_subs-cols\": c_subs,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_subs:,.0f} rows, {c_subs:,.0f} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba25222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:42:17 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925\"\n",
      "11:42:17 | INFO | \"  39 <- Files matching prefix\"\n",
      "11:42:17 | INFO | \"  39 <- Files to check\"\n",
      "11:42:17 | INFO | \"    000000000000-539559_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000001-598576_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000002-444545_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000003-274825_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000004-288676_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000005-324438_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000006-376924_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000007-318149_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000008-355281_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000009-374258_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000010-322145_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000011-334639_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000012-337243_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000013-366718_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000014-385659_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000015-412694_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000016-406709_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000017-419703_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000018-327421_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000019-403352_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000020-407185_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000021-418088_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000022-569183_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000023-425209_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000024-502588_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000025-425530_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000026-464263_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000027-585623_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000028-678184_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000029-607617_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000030-762851_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000031-950783_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000032-834277_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000033-473803_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    000000000034-643616_by_515.parquet <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"    2022-06-29_09-19-25_vectorize_text.log <- File already exists, not downloading\"\n",
      "11:42:17 | INFO | \"  Files already cached: 36\"\n",
      "11:42:17 | INFO | \"  Files already downloaded.\"\n",
      "11:42:17 | INFO | \"  df format: pandas\"\n",
      "11:43:11 | INFO | \"  0:00:55.560496 <- Data Loading Time time elapsed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,360,314 rows, 515 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:43:12 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '5.37%', 'memory_used': '77,626'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 2min 4s, total: 3min 53s\n",
      "Wall time: 55.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.05372186515760633, 'memory_used': 77626}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pc_v = LoadSubredditsGCS(\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    gcs_path=gcs_post_comment_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='post_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    ")\n",
    "pc_v.local_cache()\n",
    "\n",
    "df_v_pc = pc_v.read_as_one_df()\n",
    "r_pc, c_pc = df_v_pc.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_post_comments-rows\": r_pc,\n",
    "        f\"df_v_post_comments-cols\": c_pc,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_pc:,.0f} rows, {c_pc:,.0f} cols\")\n",
    "\n",
    "t_data_load = elapsed_time(start_time=t_start_data_load_, log_label='Data Loading Time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-data_loading_time',\n",
    "                  t_data_load / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01879f3a",
   "metadata": {},
   "source": [
    "# Set weights & create copy dfs for new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb920326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "l_ix_sub_level = ['subreddit_id', 'subreddit_name']\n",
    "l_ix_post_level = l_ix_sub_level + ['post_id']\n",
    "\n",
    "l_embedding_cols = [c for c in df_v_pc if c.startswith('embeddings_')]\n",
    "print(len(l_embedding_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872c23f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.96 s, sys: 6 s, total: 13 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### UPDATE TO RESET TEST\n",
    "if n_posts_sample is not None:\n",
    "    df_v_pc_weighted = df_v_pc.head(n_posts_sample).copy()\n",
    "else:\n",
    "    df_v_pc_weighted = df_v_pc.copy()\n",
    "\n",
    "df_v_subs_weighted = df_v_subs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63dad671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be True\n",
    "np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "722d208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 157 ms, total: 436 ms\n",
      "Wall time: 435 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# apply weight to all posts & subreddit meta at once (vectorized)\n",
    "df_v_subs_weighted[l_embedding_cols] = df_v_subs_weighted[l_embedding_cols] * WEIGHT_SUB_META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93c01e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 13.7 s, total: 39.6 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# apply weight to all posts & subreddit meta at once (vectorized)\n",
    "df_v_pc_weighted[l_embedding_cols] = df_v_pc_weighted[l_embedding_cols] * WEIGHT_POST_COMMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83951571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be False\n",
    "np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c59ec3",
   "metadata": {},
   "source": [
    "# Aggregate to Post-Level: Post&Comments + Subreddit Meta\n",
    "\n",
    "It's better to let pandas handle the interations with `.groupby('subreddit_id')`. Otherwise we have to create masks for each subreddit that can take much longer (10+ hours).\n",
    "\n",
    "- ETA with masks: +17.6 hours\n",
    "- ETA with groupby ~2.5 hours\n",
    "\n",
    "```\n",
    "# mask:\n",
    "0%  329/81973 [04:18<17:42:36, 1.28it/s]\n",
    "\n",
    "# .groupby()\n",
    "6% 4751/81973 [09:56<2:35:06, 8.30it/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14e22bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:44:06 | INFO | \"Start C1 - posts + comments + sub descriptions\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1074c4beb48042bfbb82ba0b2dff0049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:24:56 | INFO | \"Create new C1 df\"\n",
      "14:25:37 | INFO | \"  2:41:31.360312 <- Total Agg fxn time time elapsed\"\n",
      "14:25:37 | INFO | \"C1 - post level complete\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,360,314 rows, 515 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:25:38 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '11.45%', 'memory_used': '165,383'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 40min 33s, sys: 1min 28s, total: 2h 42min 1s\n",
      "Wall time: 2h 41min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.11445499221086244, 'memory_used': 165383}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "info(f\"Start C1 - posts + comments + sub descriptions\")\n",
    "t_start_agg_post_c1 = datetime.utcnow()\n",
    "\n",
    "l_df_c1_weights = list()\n",
    "\n",
    "for s_id, df_ in tqdm(\n",
    "    df_v_pc_weighted.groupby('subreddit_id'),\n",
    "    ascii=True, mininterval=5,\n",
    "):\n",
    "    df_.loc[:, l_embedding_cols] = np.add(\n",
    "        df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "        df_[l_embedding_cols]\n",
    "    )\n",
    "    l_df_c1_weights.append(df_)\n",
    "    del df_\n",
    "\n",
    "\n",
    "info(f\"Create new C1 df\")\n",
    "df_posts_agg_c1 = pd.concat(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "r_, c_ = df_posts_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_posts_agg_c1-rows\": r_,\n",
    "        f\"df_posts_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_:,.0f} rows, {c_:,.0f} cols\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_pc_c1 = elapsed_time(start_time=t_start_agg_post_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_posts_agg_c1',\n",
    "                  t_agg_pc_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "info(f\"C1 - post level complete\")\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a112e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>post_id</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_1009a3</td>\n",
       "      <td>memesenespanol</td>\n",
       "      <td>t3_v00j0e</td>\n",
       "      <td>-0.038424</td>\n",
       "      <td>-0.001508</td>\n",
       "      <td>-0.044316</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>-0.078988</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.014201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_1009a3</td>\n",
       "      <td>memesenespanol</td>\n",
       "      <td>t3_v0eg7b</td>\n",
       "      <td>-0.031401</td>\n",
       "      <td>-0.023828</td>\n",
       "      <td>-0.038469</td>\n",
       "      <td>0.039142</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>-0.004657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_1009a3</td>\n",
       "      <td>memesenespanol</td>\n",
       "      <td>t3_v0l7ym</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>-0.024439</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>-0.021662</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.038604</td>\n",
       "      <td>-0.035386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5_1009a3</td>\n",
       "      <td>memesenespanol</td>\n",
       "      <td>t3_v0l8vu</td>\n",
       "      <td>-0.036525</td>\n",
       "      <td>-0.019641</td>\n",
       "      <td>-0.018195</td>\n",
       "      <td>-0.039854</td>\n",
       "      <td>-0.055660</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>0.009570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5_1009a3</td>\n",
       "      <td>memesenespanol</td>\n",
       "      <td>t3_v0qrxj</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>-0.025595</td>\n",
       "      <td>-0.042165</td>\n",
       "      <td>0.021455</td>\n",
       "      <td>-0.033216</td>\n",
       "      <td>0.068623</td>\n",
       "      <td>0.003232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id  subreddit_name    post_id  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6\n",
       "0    t5_1009a3  memesenespanol  t3_v00j0e     -0.038424     -0.001508     -0.044316      0.006019     -0.078988      0.012636      0.014201\n",
       "1    t5_1009a3  memesenespanol  t3_v0eg7b     -0.031401     -0.023828     -0.038469      0.039142      0.057682      0.033646     -0.004657\n",
       "2    t5_1009a3  memesenespanol  t3_v0l7ym      0.035448     -0.024439      0.040074     -0.021662      0.002018      0.038604     -0.035386\n",
       "3    t5_1009a3  memesenespanol  t3_v0l8vu     -0.036525     -0.019641     -0.018195     -0.039854     -0.055660      0.035754      0.009570\n",
       "4    t5_1009a3  memesenespanol  t3_v0qrxj     -0.015133     -0.025595     -0.042165      0.021455     -0.033216      0.068623      0.003232"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts_agg_c1.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc33fb",
   "metadata": {},
   "source": [
    "### Save post-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38b60683",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dfs_to_save = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18af9449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:25:38 | INFO | \"Converting pandas to dask...\"\n",
      "14:25:42 | INFO | \"  35,070.4 MB <- Memory usage\"\n",
      "14:25:42 | INFO | \"      64\t<- target Dask partitions\t  550.0 <- target MB partition size\"\n",
      "14:27:15 | INFO | \"  Logging df to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 9s, sys: 4min 25s, total: 35min 34s\n",
      "Wall time: 9min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_posts_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_posts_agg_c1_{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_posts_agg_c1,\n",
    "    d_dfs_to_save['df_posts_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_posts_agg_c1']['local'], artifact_path='df_posts_agg_c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8461ea",
   "metadata": {},
   "source": [
    "# Aggregate to Subreddit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5c4a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:18 | INFO | \"SUBREDDIT-LEVEL C1 - posts + comments + sub descriptions\"\n",
      "14:36:50 | INFO | \"  0:01:32.196986 <- Total Agg fxn time time elapsed\"\n",
      "14:36:50 | INFO | \"  <- df_subs_agg_c1.shape (posts + comments + sub description)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81,973 rows, 514 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:51 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '11.52%', 'memory_used': '166,480'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 26.2 s, total: 1min 32s\n",
      "Wall time: 1min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.11521418225128567, 'memory_used': 166480}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "info(f\"SUBREDDIT-LEVEL C1 - posts + comments + sub descriptions\")\n",
    "t_start_agg_subs_c1 = datetime.utcnow()\n",
    "\n",
    "df_subs_agg_c1 = (\n",
    "    df_posts_agg_c1\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "r_, c_ = df_subs_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_subs_agg_c1-rows\": r_,\n",
    "        f\"df_subs_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_:,.0f} rows, {c_:,.0f} cols\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_subs_c1 = elapsed_time(start_time=t_start_agg_subs_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_subs_agg_c1',\n",
    "                  t_agg_subs_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "info(f\"  <- df_subs_agg_c1.shape (posts + comments + sub description)\")\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affe90b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81965</th>\n",
       "      <td>t5_zyz1w</td>\n",
       "      <td>k_on_shuffle</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>0.023965</td>\n",
       "      <td>0.041733</td>\n",
       "      <td>-0.010157</td>\n",
       "      <td>0.035751</td>\n",
       "      <td>0.038720</td>\n",
       "      <td>-0.015874</td>\n",
       "      <td>-0.013620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81966</th>\n",
       "      <td>t5_zz27k</td>\n",
       "      <td>capcomhomearcade</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>-0.021336</td>\n",
       "      <td>0.015305</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>-0.015484</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>-0.017814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81967</th>\n",
       "      <td>t5_zz4jm</td>\n",
       "      <td>aithesomniumfiles</td>\n",
       "      <td>-0.017261</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.021436</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.002664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81968</th>\n",
       "      <td>t5_zz9nd</td>\n",
       "      <td>freyanightingale</td>\n",
       "      <td>0.041456</td>\n",
       "      <td>0.020634</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>-0.033700</td>\n",
       "      <td>0.048856</td>\n",
       "      <td>0.024756</td>\n",
       "      <td>-0.024959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81969</th>\n",
       "      <td>t5_zzebd</td>\n",
       "      <td>mk3supra</td>\n",
       "      <td>-0.029058</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>-0.010903</td>\n",
       "      <td>-0.053896</td>\n",
       "      <td>0.063239</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.009073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81970</th>\n",
       "      <td>t5_zzgss</td>\n",
       "      <td>epididymitis</td>\n",
       "      <td>-0.037259</td>\n",
       "      <td>-0.001366</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>-0.014607</td>\n",
       "      <td>-0.063600</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.030545</td>\n",
       "      <td>-0.028854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81971</th>\n",
       "      <td>t5_zzjup</td>\n",
       "      <td>morgonaut</td>\n",
       "      <td>-0.014793</td>\n",
       "      <td>-0.026270</td>\n",
       "      <td>0.038431</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>-0.024845</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.029336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81972</th>\n",
       "      <td>t5_zzszh</td>\n",
       "      <td>circumcisiongrief</td>\n",
       "      <td>-0.011436</td>\n",
       "      <td>0.027289</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.032805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit_id     subreddit_name  embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7\n",
       "81965     t5_zyz1w       k_on_shuffle     -0.011377      0.023965      0.041733     -0.010157      0.035751      0.038720     -0.015874     -0.013620\n",
       "81966     t5_zz27k   capcomhomearcade     -0.002962     -0.021336      0.015305      0.004804     -0.015484     -0.013002      0.009767     -0.017814\n",
       "81967     t5_zz4jm  aithesomniumfiles     -0.017261      0.007027      0.010723      0.001267      0.000868      0.021436      0.004496      0.002664\n",
       "81968     t5_zz9nd   freyanightingale      0.041456      0.020634      0.010827      0.029859     -0.033700      0.048856      0.024756     -0.024959\n",
       "81969     t5_zzebd           mk3supra     -0.029058     -0.003269      0.001232     -0.010903     -0.053896      0.063239      0.009007      0.009073\n",
       "81970     t5_zzgss       epididymitis     -0.037259     -0.001366      0.001502     -0.014607     -0.063600      0.052982      0.030545     -0.028854\n",
       "81971     t5_zzjup          morgonaut     -0.014793     -0.026270      0.038431      0.008339      0.005570     -0.024845      0.001907      0.029336\n",
       "81972     t5_zzszh  circumcisiongrief     -0.011436      0.027289      0.005682      0.006329     -0.014950      0.036290      0.007651      0.032805"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs_agg_c1.iloc[-8:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab00ca",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "This one we can save as a pandas df, no need to split it into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67b1d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 s, sys: 363 ms, total: 2.82 s\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_subs_agg_c1.to_parquet(\n",
    "    path_this_model / f\"df_subs_agg_c1-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db14971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:54 | INFO | \"Converting pandas to dask...\"\n",
      "14:36:54 | INFO | \"   171.2 MB <- Memory usage\"\n",
      "14:36:54 | INFO | \"       1\t<- target Dask partitions\t  350.0 <- target MB partition size\"\n",
      "14:36:57 | INFO | \"  Logging df to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.82 s, sys: 428 ms, total: 3.25 s\n",
      "Wall time: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_subs_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_subs_agg_c1{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_subs_agg_c1,\n",
    "    d_dfs_to_save['df_subs_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_subs_agg_c1']['local'], artifact_path='df_subs_agg_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e516d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:59 | INFO | \"  2:54:43.728899 <- Total Agg fxn time time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "# finish logging total time + end mlflow run\n",
    "total_fxn_time = elapsed_time(start_time=t_start_agg_embed, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-full_aggregation_fxn_minutes',\n",
    "                  total_fxn_time / timedelta(minutes=1)\n",
    "                  )\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84354e",
   "metadata": {},
   "source": [
    "# Test run on data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ad5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "353f48dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BREAK' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-66db536c6cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBREAK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BREAK' is not defined"
     ]
    }
   ],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000cafa",
   "metadata": {},
   "source": [
    "## Aggregate to Post Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DON'T DO THIS -- iterating per sub + mask takes waaaay longer. \n",
    "# for s_id in tqdm(\n",
    "#     df_v_pc_weighted['subreddit_id'].unique(),\n",
    "#     ascii=True, mininterval=2\n",
    "# ):\n",
    "#     mask_sub_posts = df_v_pc_weighted['subreddit_id'] == s_id\n",
    "    \n",
    "# #     df_v_pc_weighted.loc[mask_sub_posts, l_embedding_cols] = np.add(\n",
    "# #         df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "# #         df_v_pc_weighted[mask_sub_posts][l_embedding_cols]\n",
    "# #     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df_c1_weights_test = list()\n",
    "\n",
    "for s_id, df_ in tqdm(\n",
    "    df_v_pc_weighted.head(123000).groupby('subreddit_id'),\n",
    "    ascii=True, mininterval=5,\n",
    "):\n",
    "    df_.loc[:, l_embedding_cols] = np.add(\n",
    "        df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "        df_[l_embedding_cols]\n",
    "    )\n",
    "    l_df_c1_weights_test.append(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731de42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_df_c1_weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_posts_agg_c1_test = pd.concat(l_df_c1_weights_test, ignore_index=True)\n",
    "print(df_posts_agg_c1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42634cf",
   "metadata": {},
   "source": [
    "### Save post-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for post-level, we want to save using dask because it'll save to multiple files\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_posts_agg_c1_test,\n",
    "    path_this_model / f\"df_posts_agg_c1_test_{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\",\n",
    "    write_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97ef50",
   "metadata": {},
   "source": [
    "## Aggregate to Subreddit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29170f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ix_sub_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d3959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info(f\"C1 - posts + comments + sub descriptions\")\n",
    "df_subs_agg_c1_test = (\n",
    "    df_posts_agg_c1_test\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "info(f\"  {df_subs_agg_c1_test.shape} <- df_subs_agg_c1.shape (posts + comments + sub description)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_agg_c1_test.iloc[-8:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b6924",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "This one we can save as a pandas df, no need to split it into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36605ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_agg_c1_test.to_parquet(\n",
    "    path_this_model / f\"df_subs_agg_c1_test-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca70d96",
   "metadata": {},
   "source": [
    "## Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected it to be false\n",
    "np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_posts_agg_c1_test.iloc[:1000,3:515])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_agg_c1_test.iloc[-3:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c12300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_pc.iloc[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc80f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_pc_weighted.iloc[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_agg_c1_test.iloc[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_subs_weighted[df_v_subs_weighted['subreddit_name'] == 'memesenespanol'].iloc[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa18eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_subs[df_v_subs['subreddit_name'] == 'memesenespanol'].iloc[:, :10]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
