{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d69fc2a",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to test the new data-loader (GCS), configs, and embeddings class that we'll be using in kubeflow.\n",
    "\n",
    "For inference (getting embeddings) it might be better to read from GCS than from SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219e9fa",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10b4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d300ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.20.0\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "# from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "# new modules to test:\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_sql import (\n",
    "    convert_iter_to_sql_str,\n",
    "    LoadSubredditsSQL\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6727a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:39:54 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf51add",
   "metadata": {},
   "source": [
    "# Load data with new class\n",
    "\n",
    "Load data from a test folder where we have 2 parquet files. This way we can test slicing & sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da8583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import (\n",
    "    LoadSubredditsGCS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ec0cd",
   "metadata": {},
   "source": [
    "## Cache files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54dcfc",
   "metadata": {},
   "source": [
    "### All files, read as 1 pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4768a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:39:56 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:39:56 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:39:56 | INFO | \"  2 <- Files to check\"\n",
      "03:39:56 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "03:39:56 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "03:39:56 | INFO | \"  Files already cached: 2\"\n",
      "03:39:56 | INFO | \"0:00:01.380230  <- Downloading files elapsed time\"\n",
      "03:39:56 | INFO | \"  Files already downloaded.\"\n",
      "03:39:56 | INFO | \"  df format: pandas\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <- Local files\n",
      "2 <- Local parquet files\n",
      "(159874, 3)\n",
      "CPU times: user 364 ms, sys: 275 ms, total: 639 ms\n",
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_name_title_related_subs_and_clean_descriptions'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "subs._local_cache()\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "assert 2 == len(subs.local_files_)\n",
    "\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c617092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_name_title_related_subs_and_clean_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_2sxhs</td>\n",
       "      <td>place</td>\n",
       "      <td>place. r/place. \\nplace\\nplace\\n\\nSome have visited a canvas before. A place where togetherness created more. Now in numbers far greater, taking more space, It falls upon you to create a better place.\\n\\nThere is an empty canvas.\\n\\nYou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>AskReddit. r/AskReddit. \\nAsk Reddit...\\naskreddit, AskReddit\\n\\nAskReddit is the place to ask and answer thought provoking questions.\\n\\nSERIOUS askreddit true Serious \\n\\nRules AskReddit wikiwiki rules:\\n1. You must post a clear and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_2qhsa</td>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>interestingasfuck. r/interestingasfuck. \\nInteresting As Fuck\\ninterestingasfuck, TodayILearned, notinteresting, mildlyinteresting, offbeat, oddlysatisfying, damnthatsinteresting, Unexpected, wtf\\n\\nFor anything that is InterestingAsFuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5_2y77d</td>\n",
       "      <td>antiwork</td>\n",
       "      <td>antiwork. r/antiwork. \\nAntiwork: Unemployment for all, not just the rich!\\nantiwork, antitrampo, AntiTaff, antiarbeit, antiworkItaly, tegenwerken, antiwork_slovenija, Antiwork_UK, Anarchism, Anarchy101, IWW, LateStageCapitalism, lostge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5_2qh13</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>worldnews. r/worldnews. \\nWorld News\\nNews, PoliticalDiscussion, WorldEvents, GeoPolitics, IntheNews, GlobalTalk, Breakingnews, Business, Economics, Environment, History, HumanRights, Features, UpliftingNews, NewsOfTheWeird, FakeNews, I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id     subreddit_name                                                                                                                                                                                         subreddit_name_title_related_subs_and_clean_descriptions\n",
       "0     t5_2sxhs              place  place. r/place. \\nplace\\nplace\\n\\nSome have visited a canvas before. A place where togetherness created more. Now in numbers far greater, taking more space, It falls upon you to create a better place.\\n\\nThere is an empty canvas.\\n\\nYou...\n",
       "1     t5_2qh1i          askreddit  AskReddit. r/AskReddit. \\nAsk Reddit...\\naskreddit, AskReddit\\n\\nAskReddit is the place to ask and answer thought provoking questions.\\n\\nSERIOUS askreddit true Serious \\n\\nRules AskReddit wikiwiki rules:\\n1. You must post a clear and d...\n",
       "2     t5_2qhsa  interestingasfuck  interestingasfuck. r/interestingasfuck. \\nInteresting As Fuck\\ninterestingasfuck, TodayILearned, notinteresting, mildlyinteresting, offbeat, oddlysatisfying, damnthatsinteresting, Unexpected, wtf\\n\\nFor anything that is InterestingAsFuc...\n",
       "3     t5_2y77d           antiwork  antiwork. r/antiwork. \\nAntiwork: Unemployment for all, not just the rich!\\nantiwork, antitrampo, AntiTaff, antiarbeit, antiworkItaly, tegenwerken, antiwork_slovenija, Antiwork_UK, Anarchism, Anarchy101, IWW, LateStageCapitalism, lostge...\n",
       "4     t5_2qh13          worldnews  worldnews. r/worldnews. \\nWorld News\\nNews, PoliticalDiscussion, WorldEvents, GeoPolitics, IntheNews, GlobalTalk, Breakingnews, Business, Economics, Environment, History, HumanRights, Features, UpliftingNews, NewsOfTheWeird, FakeNews, I..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c409d12",
   "metadata": {},
   "source": [
    "### All files, read as 1 `dask` df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abc1602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:39:58 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:39:58 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:39:58 | INFO | \"  2 <- Files to check\"\n",
      "03:39:58 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "03:39:58 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "03:39:58 | INFO | \"  Files already cached: 2\"\n",
      "03:39:58 | INFO | \"0:00:01.337023  <- Downloading files elapsed time\"\n",
      "03:39:58 | INFO | \"  Files already downloaded.\"\n",
      "03:39:58 | INFO | \"  df format: dask\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <- Local files\n",
      "2 <- Local parquet files\n",
      "(Delayed('int-7a927216-326b-4f38-ad3c-7ec261434fe9'), 3)\n",
      "CPU times: user 39.6 ms, sys: 35.1 ms, total: 74.7 ms\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_name_title_related_subs_and_clean_descriptions'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='dask',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "subs._local_cache()\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "assert 2 == len(subs.local_files_)\n",
    "\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f93af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_name_title_related_subs_and_clean_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79932</th>\n",
       "      <td>t5_s3j8c</td>\n",
       "      <td>antianimearmy</td>\n",
       "      <td>AntiAnimeArmy. r/AntiAnimeArmy. \\nJoin the Army Today!\\nAnimeAddictsAnonymous, NoAnimePolice, AnimeHate\\n\\nThe Anti Anime Army is to destroy the evil that is Anime.\\n\\nSeeking help with your addiction?\\ntry our spin off subreddit:\\nAnim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79933</th>\n",
       "      <td>t5_3stsd4</td>\n",
       "      <td>poorlymadepolicememes</td>\n",
       "      <td>poorlymadepolicememes. r/poorlymadepolicememes. \\npoorlymadepolicememes\\nThe sub about the very poorly made memer and podcaster\\n\\nFeel free to discuss the podcast episodes and memes\\n\\nA bunch of police memes that aren’t really that gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79934</th>\n",
       "      <td>t5_48te5o</td>\n",
       "      <td>svampedyrkning</td>\n",
       "      <td>Svampedyrkning. r/Svampedyrkning. \\nSvampedyrkning\\nEt dansk subreddit dedikeret til svampedyrkning. Her kan man diskutere dyrkning af gourmet , psilocybin og medicinsvampe, med henblik på at hjælpe hinanden, dele sine erfaringer og udv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79935</th>\n",
       "      <td>t5_4ua7kp</td>\n",
       "      <td>meditieren</td>\n",
       "      <td>meditieren. r/meditieren. \\nMeditieren - Entspannung von Kopf bis Fuß.\\nmeditieren\\n\\nWillkommen auf meditieren! Dieser Sub dient dem Austausch von Erfahrungen, Geschichten und Anleitungen in Bezug auf die Praxis der Meditation.\\n\\nWILK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79936</th>\n",
       "      <td>t5_4cefbk</td>\n",
       "      <td>handball_de</td>\n",
       "      <td>handball_de. r/handball_de. \\nHandball\\nFür alles rund um den Profi und Amateurhandball!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit_id         subreddit_name                                                                                                                                                                                         subreddit_name_title_related_subs_and_clean_descriptions\n",
       "79932     t5_s3j8c          antianimearmy  AntiAnimeArmy. r/AntiAnimeArmy. \\nJoin the Army Today!\\nAnimeAddictsAnonymous, NoAnimePolice, AnimeHate\\n\\nThe Anti Anime Army is to destroy the evil that is Anime.\\n\\nSeeking help with your addiction?\\ntry our spin off subreddit:\\nAnim...\n",
       "79933    t5_3stsd4  poorlymadepolicememes  poorlymadepolicememes. r/poorlymadepolicememes. \\npoorlymadepolicememes\\nThe sub about the very poorly made memer and podcaster\\n\\nFeel free to discuss the podcast episodes and memes\\n\\nA bunch of police memes that aren’t really that gr...\n",
       "79934    t5_48te5o         svampedyrkning  Svampedyrkning. r/Svampedyrkning. \\nSvampedyrkning\\nEt dansk subreddit dedikeret til svampedyrkning. Her kan man diskutere dyrkning af gourmet , psilocybin og medicinsvampe, med henblik på at hjælpe hinanden, dele sine erfaringer og udv...\n",
       "79935    t5_4ua7kp             meditieren  meditieren. r/meditieren. \\nMeditieren - Entspannung von Kopf bis Fuß.\\nmeditieren\\n\\nWillkommen auf meditieren! Dieser Sub dient dem Austausch von Erfahrungen, Geschichten und Anleitungen in Bezug auf die Praxis der Meditation.\\n\\nWILK...\n",
       "79936    t5_4cefbk            handball_de                                                                                                                                                         handball_de. r/handball_de. \\nHandball\\nFür alles rund um den Profi und Amateurhandball!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca976ada",
   "metadata": {},
   "source": [
    "### All files, yield each file as separate df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ddd224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:39:59 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:39:59 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:39:59 | INFO | \"  2 <- Files to check\"\n",
      "03:39:59 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "03:39:59 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "03:39:59 | INFO | \"  Files already cached: 2\"\n",
      "03:39:59 | INFO | \"0:00:01.351215  <- Downloading files elapsed time\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]03:39:59 | INFO | \"  Files already downloaded.\"\n",
      " 50%|█████     | 1/2 [00:00<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79937, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79937, 3)\n",
      "2 <- Local files\n",
      "2 <- Local parquet files\n",
      "CPU times: user 323 ms, sys: 270 ms, total: 593 ms\n",
      "Wall time: 1.85 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_name_title_related_subs_and_clean_descriptions'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "subs._local_cache()\n",
    "for df_y_ in tqdm(subs.yield_each_file_as_df(), total=subs.n_local_parquet_files_):\n",
    "    print(df_y_.shape)\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "assert 2 == len(subs.local_files_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae3410",
   "metadata": {},
   "source": [
    "### Sample file (1st file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e52ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:40:01 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:40:01 | INFO | \"  1 <- Files matching prefix\"\n",
      "03:40:01 | INFO | \"  1 <- Files to check\"\n",
      "03:40:01 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "03:40:01 | INFO | \"  Files already cached: 1\"\n",
      "03:40:01 | INFO | \"0:00:01.394998  <- Downloading files elapsed time\"\n",
      "03:40:01 | INFO | \"  df format: pandas\"\n",
      "03:40:01 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79937, 3)\n",
      "1 <- Local files\n",
      "1 <- Local parquet files\n",
      "CPU times: user 174 ms, sys: 77.9 ms, total: 252 ms\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_description'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=1,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    ")\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "\n",
    "assert(1 == len(subs.local_files_)), \"Expected to sample only 1 file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6b0b2",
   "metadata": {},
   "source": [
    "### Slice -- last file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adafdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:40:03 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:40:03 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:40:03 | INFO | \"  1 <- Files to check\"\n",
      "03:40:03 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "03:40:03 | INFO | \"  Files already cached: 1\"\n",
      "03:40:03 | INFO | \"0:00:01.361892  <- Downloading files elapsed time\"\n",
      "03:40:03 | INFO | \"  df format: pandas\"\n",
      "03:40:03 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79937, 3)\n",
      "1 <- Local files\n",
      "1 <- Local parquet files\n",
      "CPU times: user 184 ms, sys: 136 ms, total: 321 ms\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_description'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=-1,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    ")\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "\n",
    "assert(1 == len(subs.local_files_)), \"Expected slice with only 1 file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2772c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:40:04 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:40:04 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:40:04 | INFO | \"  1 <- Files to check\"\n",
      "03:40:04 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "03:40:04 | INFO | \"  Files already cached: 1\"\n",
      "03:40:04 | INFO | \"0:00:01.336908  <- Downloading files elapsed time\"\n",
      "03:40:04 | INFO | \"  df format: pandas\"\n",
      "03:40:05 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79937, 3)\n",
      "1 <- Local files\n",
      "1 <- Local parquet files\n",
      "CPU times: user 184 ms, sys: 141 ms, total: 326 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_description'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "  \n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=1,\n",
    "    n_files_slice_end=2,\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    ")\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "\n",
    "assert(1 == len(subs.local_files_)), \"Expected slice with only 1 file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af1203",
   "metadata": {},
   "source": [
    "### Slice first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495f8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:40:06 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:40:06 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:40:06 | INFO | \"  1 <- Files to check\"\n",
      "03:40:06 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "03:40:06 | INFO | \"  Files already cached: 1\"\n",
      "03:40:06 | INFO | \"0:00:01.384844  <- Downloading files elapsed time\"\n",
      "03:40:06 | INFO | \"  df format: pandas\"\n",
      "03:40:06 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79937, 3)\n",
      "1 <- Local files\n",
      "1 <- Local parquet files\n",
      "CPU times: user 191 ms, sys: 126 ms, total: 317 ms\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_description'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=1,\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    ")\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "\n",
    "assert(1 == len(subs.local_files_)), \"Expected slice with 1 file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4edbfc",
   "metadata": {},
   "source": [
    "### Slice first 2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a026ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:40:08 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "03:40:08 | INFO | \"  2 <- Files matching prefix\"\n",
      "03:40:08 | INFO | \"  2 <- Files to check\"\n",
      "03:40:08 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "03:40:08 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "03:40:08 | INFO | \"  Files already cached: 2\"\n",
      "03:40:08 | INFO | \"0:00:01.365880  <- Downloading files elapsed time\"\n",
      "03:40:08 | INFO | \"  df format: pandas\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159874, 3)\n",
      "2 <- Local files\n",
      "2 <- Local parquet files\n",
      "CPU times: user 321 ms, sys: 240 ms, total: 561 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_description'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=2,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "\n",
    "assert(2 == len(subs.local_files_)), \"Expected 2 files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b5230",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d311718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddits_test_local\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_subreddits_test_local'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b77fb",
   "metadata": {},
   "source": [
    "## All files as single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458ce8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 04:19:25.399262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'output_folder', 'gcs_path', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'get_embeddings_verbose', 'cols_index'])\n",
      "Error executing job with overrides: []\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 78, in vectorize_text\n",
      "    **{k: v for k, v in cfg.items() if k not in ['data_test', 'data_loader_kwargs']}\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 135, in __init__\n",
      "    **data_loader_kwargs\n",
      "TypeError: __init__() got an unexpected keyword argument 'process_individual_files'\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca2d55",
   "metadata": {},
   "source": [
    "## Files sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131e979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
