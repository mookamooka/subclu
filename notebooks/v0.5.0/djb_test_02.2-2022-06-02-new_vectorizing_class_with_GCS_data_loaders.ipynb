{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1105c3c6",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to test the new data-loader (GCS), configs, and embeddings class that we'll be using in kubeflow.\n",
    "\n",
    "For inference (getting embeddings) it might be better to read from GCS than from SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def3cb1",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407bedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a9f541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.20.0\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "# from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "# new modules to test:\n",
    "from google.cloud import bigquery\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import (\n",
    "    LoadSubredditsGCS\n",
    ")\n",
    "from subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf import(\n",
    "    get_embeddings_as_df,\n",
    "    upload_folder_to_gcs,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a524bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:30:03 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc952dda",
   "metadata": {},
   "source": [
    "# Auth note\n",
    "This notebook assumes you have authenticated using the gcloud CLI. Example</br>\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef912d",
   "metadata": {},
   "source": [
    "# Test embeddings function on plain df\n",
    "\n",
    "To make sure that the function itself is fine.\n",
    "\n",
    "#### Update: 2022-06-02\n",
    "The name for the meta column has changed to: `subreddit_meta_for_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d974e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:07:30 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text\"\n",
      "18:07:30 | INFO | \"  1 <- Files matching prefix\"\n",
      "18:07:30 | INFO | \"  1 <- Files to check\"\n",
      "18:07:30 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "18:07:30 | INFO | \"  Files already cached: 1\"\n",
      "18:07:30 | INFO | \"  Files already downloaded.\"\n",
      "18:07:30 | INFO | \"  df format: pandas\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <- Local files\n",
      "1 <- Local parquet files\n",
      "(84480, 4)\n",
      "CPU times: user 168 ms, sys: 173 ms, total: 341 ms\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RUN_DATE = '20220602'\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path=f'i18n_topic_model_batch/runs/{RUN_DATE}/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_meta_for_embeddings', 'subreddit_meta_for_embeddings_len'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "subs.local_cache()\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)\n",
    "\n",
    "assert 1 == len(subs.local_files_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000bd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text\n",
    "# model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4501ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Get embeddings\n",
    "# df_vect_ = get_embeddings_as_df(\n",
    "#     model=model,\n",
    "#     df=df_,\n",
    "#     col_text='subreddit_name_title_related_subs_and_clean_descriptions',\n",
    "#     cols_index=['subreddit_id', 'subreddit_name'],\n",
    "#     verbose_init=True,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae3bcb",
   "metadata": {},
   "source": [
    "# Test upload folder to GCS\n",
    "\n",
    "We need this if we're not using mlflow to track a model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f48f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:42:30 | INFO | \"dry_run=True\"\n",
      "17:42:32 | INFO | \"Uploading file\n",
      "  from: /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text/000000000000.parquet\n",
      "  to: gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text/TEST_UPLOAD/000000000000.parquet\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text\n",
      "CPU times: user 25.3 ms, sys: 17.8 ms, total: 43.1 ms\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upload_folder_to_gcs(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_output_root=f'i18n_topic_model_batch/runs/{RUN_DATE}/subreddits/text/TEST_UPLOAD',\n",
    "    local_dir=f'/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/{RUN_DATE}/subreddits/text',\n",
    "    gcs_new_subfolder=None,\n",
    "    verbose=True,\n",
    "    dry_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81466a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:43:35 | INFO | \"dry_run=True\"\n",
      "17:43:36 | INFO | \"Uploading file\n",
      "  from: /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text/000000000000.parquet\n",
      "  to: gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text/TEST_UPLOAD/subfolder_test/000000000000.parquet\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text\n",
      "CPU times: user 28.5 ms, sys: 20.4 ms, total: 48.9 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upload_folder_to_gcs(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_output_root=f'i18n_topic_model_batch/runs/{RUN_DATE}/subreddits/text/TEST_UPLOAD',\n",
    "    local_dir=f'/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/{RUN_DATE}/subreddits/text',\n",
    "    gcs_new_subfolder='subfolder_test',\n",
    "    verbose=True,\n",
    "    dry_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aafc58",
   "metadata": {},
   "source": [
    "# Check meta text len so that we know the length needed to cover ~90% of subreddits\n",
    "\n",
    "This length will change based on what text we add. For example, adding taxonomy mature topics should increase text length for some subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "908272ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_f6e80_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >subreddit_meta_for_embeddings_len</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f6e80_level0_row0\" class=\"row_heading level0 row0\" >count</th>\n",
       "                        <td id=\"T_f6e80_row0_col0\" class=\"data row0 col0\" >84,480.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row1\" class=\"row_heading level0 row1\" >mean</th>\n",
       "                        <td id=\"T_f6e80_row1_col0\" class=\"data row1 col0\" >758.48</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row2\" class=\"row_heading level0 row2\" >std</th>\n",
       "                        <td id=\"T_f6e80_row2_col0\" class=\"data row2 col0\" >1,112.89</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row3\" class=\"row_heading level0 row3\" >min</th>\n",
       "                        <td id=\"T_f6e80_row3_col0\" class=\"data row3 col0\" >9.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row4\" class=\"row_heading level0 row4\" >10%</th>\n",
       "                        <td id=\"T_f6e80_row4_col0\" class=\"data row4 col0\" >71.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row5\" class=\"row_heading level0 row5\" >25%</th>\n",
       "                        <td id=\"T_f6e80_row5_col0\" class=\"data row5 col0\" >135.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row6\" class=\"row_heading level0 row6\" >50%</th>\n",
       "                        <td id=\"T_f6e80_row6_col0\" class=\"data row6 col0\" >298.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row7\" class=\"row_heading level0 row7\" >70%</th>\n",
       "                        <td id=\"T_f6e80_row7_col0\" class=\"data row7 col0\" >681.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row8\" class=\"row_heading level0 row8\" >75%</th>\n",
       "                        <td id=\"T_f6e80_row8_col0\" class=\"data row8 col0\" >887.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row9\" class=\"row_heading level0 row9\" >85%</th>\n",
       "                        <td id=\"T_f6e80_row9_col0\" class=\"data row9 col0\" >1,524.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row10\" class=\"row_heading level0 row10\" >90%</th>\n",
       "                        <td id=\"T_f6e80_row10_col0\" class=\"data row10 col0\" >2,077.10</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row11\" class=\"row_heading level0 row11\" >95%</th>\n",
       "                        <td id=\"T_f6e80_row11_col0\" class=\"data row11 col0\" >3,078.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row12\" class=\"row_heading level0 row12\" >99%</th>\n",
       "                        <td id=\"T_f6e80_row12_col0\" class=\"data row12 col0\" >5,456.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f6e80_level0_row13\" class=\"row_heading level0 row13\" >max</th>\n",
       "                        <td id=\"T_f6e80_row13_col0\" class=\"data row13 col0\" >11,253.00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6a679dc550>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_df_numeric(\n",
    "    df_['subreddit_meta_for_embeddings_len']\n",
    "    .describe(percentiles=[.1, .25, .5, .70, .75, .85, .90, .95, .99])\n",
    "    .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf935d4",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize \n",
    "\n",
    "When we call the vectorizing function, it calls the data loader under the hood.\n",
    "See the configs in:\n",
    "- `subclu2/config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b98f50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddits_test_local\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_subreddits_test_local'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e93420",
   "metadata": {},
   "source": [
    "## All files as single DF\n",
    "\n",
    "For subreddit-metadata, aim for:\n",
    "- `first_n_chars` ~ 2,000 (~90th percentile)\n",
    "- `batch_inference_rows` below 1,500 (fewer OOM errors => fewer retries)\n",
    "\n",
    "\n",
    "*NOTE*: in previous test we had ~140k subreddits in 2 files:\n",
    "```bash\n",
    "`lowercase_text: False`\n",
    "`limit_first_n_chars: 2200`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 2000`\n",
    "0:03:58.241812 <- df_subs vectorizing time elapsed\n",
    "# large batch & too many characters makes it slow b/c it runs OOM often, so we spend a lot of time retrying.\n",
    "\n",
    "\n",
    "`limit_first_n_chars: 2000`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1700`\n",
    "0:02:21.720878 <- df_subs vectorizing time elapsed\n",
    "\n",
    "`limit_first_n_chars: 1900`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1600`\n",
    "0:02:04.692077 <- df_subs vectorizing time elapsed\n",
    "```\n",
    "\n",
    "```bash\n",
    "# single file with 84k subreddits\n",
    "`Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text/embedding/2022-06-02_183302/df-84480_by_514.parquet`\n",
    "`limit_first_n_chars: 2000`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1500`\n",
    "0:01:15.324874 <- Total vectorize fxn time elapsed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3139954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'local_model_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "Data Loader kwags:\n",
      "  columns: ['subreddit_id', 'subreddit_name', 'subreddit_meta_for_embeddings']\n",
      "  df_format: pandas\n",
      "  unique_check: False\n",
      "  verbose: True\n",
      "  bucket_name: gazette-models-temp\n",
      "  gcs_path: i18n_topic_model_batch/runs/20220602/subreddits/text\n",
      "  local_cache_path: /home/jupyter/subreddit_clustering_i18n/data/local_cache/\n",
      "  n_sample_files: None\n",
      "  n_files_slice_start: None\n",
      "  n_files_slice_end: None\n",
      "`2022-06-02 18:33:02,080` | `INFO` | `Using hydra's path`\n",
      "`2022-06-02 18:33:02,080` | `INFO` | `  Log file created at: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-06-02/18-33-02/logs/2022-06-02_18-33-02_vectorize_text.log`\n",
      "`2022-06-02 18:33:02,080` | `INFO` | `Start vectorize function`\n",
      "`2022-06-02 18:33:02,081` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-06-02 18:33:03,677` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-06-02 18:33:07,878` | `INFO` | `Model loaded`\n",
      "`2022-06-02 18:33:07,878` | `INFO` | `Loading all files as a single df...`\n",
      "`2022-06-02 18:33:09,125` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text`\n",
      "`2022-06-02 18:33:09,197` | `INFO` | `  1 <- Files matching prefix`\n",
      "`2022-06-02 18:33:09,197` | `INFO` | `  1 <- Files to check`\n",
      "`2022-06-02 18:33:09,198` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-06-02 18:33:09,198` | `INFO` | `  Files already cached: 1`\n",
      "`2022-06-02 18:33:09,198` | `INFO` | `0:00:01.319586  <- Downloading files elapsed time`\n",
      "`2022-06-02 18:33:09,198` | `INFO` | `  df format: pandas`\n",
      "`2022-06-02 18:33:09,445` | `INFO` | `Vectorizing column: subreddit_meta_for_embeddings`\n",
      "`2022-06-02 18:33:09,449` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-06-02 18:33:09,449` | `INFO` | `col_text: subreddit_meta_for_embeddings`\n",
      "`2022-06-02 18:33:09,449` | `INFO` | `lowercase_text: False`\n",
      "`2022-06-02 18:33:09,449` | `INFO` | `limit_first_n_chars: 2000`\n",
      "`2022-06-02 18:33:09,449` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-06-02 18:33:09,658` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "2022-06-02 18:33:21.753266: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.85GiB (rounded to 3059394560)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-02 18:33:21.753877: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************_______*****________*****___**********************************______________\n",
      "2022-06-02 18:33:21.753912: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[597538,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-02 18:33:21,756` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[597538,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-02 18:33:22,854` | `INFO` | `  Vectorizing:   2%|5                            | 1/57 [00:13<12:18, 13.20s/it]`\n",
      "`2022-06-02 18:33:34,843` | `INFO` | `  Vectorizing:  19%|#####4                      | 11/57 [00:25<01:31,  1.98s/it]`\n",
      "`2022-06-02 18:33:46,502` | `INFO` | `  Vectorizing:  44%|############2               | 25/57 [00:36<00:39,  1.23s/it]`\n",
      "`2022-06-02 18:33:57,902` | `INFO` | `  Vectorizing:  74%|####################6       | 42/57 [00:48<00:14,  1.07it/s]`\n",
      "`2022-06-02 18:34:06,088` | `INFO` | `  Vectorizing: 100%|############################| 57/57 [00:56<00:00,  1.01it/s]`\n",
      "\n",
      "`2022-06-02 18:34:06,520` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220602/subreddits/text/embedding/2022-06-02_183302/df-84480_by_514.parquet`\n",
      "`2022-06-02 18:34:14,066` | `INFO` | `  0:01:06.187933 <- df_subs vectorizing time elapsed`\n",
      "`2022-06-02 18:34:15,390` | `INFO` | `Saving log file...`\n",
      "`2022-06-02 18:34:15,572` | `INFO` | `Saving hydra config...`\n",
      "/home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-06-02/18-33-02/.hydra\n",
      "`2022-06-02 18:34:17,405` | `INFO` | `  0:01:15.324874 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=false \\\n",
    "    limit_first_n_chars=2000 \\\n",
    "    batch_inference_rows=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc268b1f",
   "metadata": {},
   "source": [
    "## Run in bucket owned my i18n\n",
    "This bucket retains data longer than the gazette temp bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f0a35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'local_model_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "Data Loader kwags:\n",
      "  columns: ['subreddit_id', 'subreddit_name', 'subreddit_meta_for_embeddings']\n",
      "  df_format: pandas\n",
      "  unique_check: False\n",
      "  verbose: True\n",
      "  bucket_name: i18n-subreddit-clustering\n",
      "  gcs_path: i18n_topic_model_batch/runs/20220602/subreddits/text\n",
      "  local_cache_path: /home/jupyter/subreddit_clustering_i18n/data/local_cache/\n",
      "  n_sample_files: None\n",
      "  n_files_slice_start: None\n",
      "  n_files_slice_end: None\n",
      "`2022-06-02 19:21:32,090` | `INFO` | `Using hydra's path`\n",
      "`2022-06-02 19:21:32,090` | `INFO` | `  Log file created at: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-06-02/19-21-32/logs/2022-06-02_19-21-32_vectorize_text.log`\n",
      "`2022-06-02 19:21:32,090` | `INFO` | `Start vectorize function`\n",
      "`2022-06-02 19:21:32,090` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-06-02 19:21:33,640` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-06-02 19:21:37,737` | `INFO` | `Model loaded`\n",
      "`2022-06-02 19:21:37,737` | `INFO` | `Loading all files as a single df...`\n",
      "`2022-06-02 19:21:38,988` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220602/subreddits/text`\n",
      "`2022-06-02 19:21:39,066` | `INFO` | `  1 <- Files matching prefix`\n",
      "`2022-06-02 19:21:39,067` | `INFO` | `  1 <- Files to check`\n",
      "`2022-06-02 19:21:40,593` | `INFO` | `  Files already cached: 0`\n",
      "`2022-06-02 19:21:40,594` | `INFO` | `0:00:02.856167  <- Downloading files elapsed time`\n",
      "`2022-06-02 19:21:40,594` | `INFO` | `  df format: pandas`\n",
      "`2022-06-02 19:21:40,847` | `INFO` | `Vectorizing column: subreddit_meta_for_embeddings`\n",
      "`2022-06-02 19:21:40,851` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-06-02 19:21:40,851` | `INFO` | `col_text: subreddit_meta_for_embeddings`\n",
      "`2022-06-02 19:21:40,851` | `INFO` | `lowercase_text: False`\n",
      "`2022-06-02 19:21:40,851` | `INFO` | `limit_first_n_chars: 2000`\n",
      "`2022-06-02 19:21:40,851` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-06-02 19:21:41,088` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "2022-06-02 19:21:52.677847: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.85GiB (rounded to 3059394560)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-02 19:21:52.678420: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************_______*****________*****___**********************************______________\n",
      "2022-06-02 19:21:52.678458: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[597538,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-02 19:21:52,679` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[597538,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-02 19:21:53,744` | `INFO` | `  Vectorizing:   2%|5                            | 1/57 [00:12<11:48, 12.65s/it]`\n",
      "`2022-06-02 19:22:05,500` | `INFO` | `  Vectorizing:  19%|#####4                      | 11/57 [00:24<01:28,  1.93s/it]`\n",
      "`2022-06-02 19:22:16,843` | `INFO` | `  Vectorizing:  44%|############2               | 25/57 [00:35<00:38,  1.20s/it]`\n",
      "`2022-06-02 19:22:27,967` | `INFO` | `  Vectorizing:  74%|####################6       | 42/57 [00:46<00:13,  1.10it/s]`\n",
      "`2022-06-02 19:22:35,952` | `INFO` | `  Vectorizing: 100%|############################| 57/57 [00:54<00:00,  1.04it/s]`\n",
      "\n",
      "`2022-06-02 19:22:36,364` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220602/subreddits/text/embedding/2022-06-02_192132/df-84480_by_514.parquet`\n",
      "`2022-06-02 19:22:44,117` | `INFO` | `  0:01:06.379400 <- df_subs vectorizing time elapsed`\n",
      "`2022-06-02 19:22:45,444` | `INFO` | `Saving log file...`\n",
      "`2022-06-02 19:22:45,648` | `INFO` | `Saving hydra config...`\n",
      "/home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-06-02/19-21-32/.hydra\n",
      "`2022-06-02 19:22:47,427` | `INFO` | `  0:01:15.337494 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=false \\\n",
    "    limit_first_n_chars=2000 \\\n",
    "    batch_inference_rows=1500 \\\n",
    "    data_text.bucket_name=\"i18n-subreddit-clustering\" \\\n",
    "    output_bucket=\"i18n-subreddit-clustering\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df878c97",
   "metadata": {},
   "source": [
    "## Rough time projections\n",
    "Based on the file(s) processed above. Here are some rough projections for how long it might take to process all posts needed for the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "395f37c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_8679e_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >n_rows</th>        <th class=\"col_heading level0 col1\" >n_jobs</th>        <th class=\"col_heading level0 col2\" >projected_hours</th>        <th class=\"col_heading level0 col3\" >projected_days</th>        <th class=\"col_heading level0 col4\" >projected_mins</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8679e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8679e_row0_col0\" class=\"data row0 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "                        <td id=\"T_8679e_row0_col2\" class=\"data row0 col2\" >24.68</td>\n",
       "                        <td id=\"T_8679e_row0_col3\" class=\"data row0 col3\" >1.03</td>\n",
       "                        <td id=\"T_8679e_row0_col4\" class=\"data row0 col4\" >1,481.04</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8679e_row1_col0\" class=\"data row1 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "                        <td id=\"T_8679e_row1_col2\" class=\"data row1 col2\" >12.34</td>\n",
       "                        <td id=\"T_8679e_row1_col3\" class=\"data row1 col3\" >0.51</td>\n",
       "                        <td id=\"T_8679e_row1_col4\" class=\"data row1 col4\" >740.52</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_8679e_row2_col0\" class=\"data row2 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row2_col1\" class=\"data row2 col1\" >3</td>\n",
       "                        <td id=\"T_8679e_row2_col2\" class=\"data row2 col2\" >8.23</td>\n",
       "                        <td id=\"T_8679e_row2_col3\" class=\"data row2 col3\" >0.34</td>\n",
       "                        <td id=\"T_8679e_row2_col4\" class=\"data row2 col4\" >493.68</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_8679e_row3_col0\" class=\"data row3 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row3_col1\" class=\"data row3 col1\" >4</td>\n",
       "                        <td id=\"T_8679e_row3_col2\" class=\"data row3 col2\" >6.17</td>\n",
       "                        <td id=\"T_8679e_row3_col3\" class=\"data row3 col3\" >0.26</td>\n",
       "                        <td id=\"T_8679e_row3_col4\" class=\"data row3 col4\" >370.26</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_8679e_row4_col0\" class=\"data row4 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row4_col1\" class=\"data row4 col1\" >5</td>\n",
       "                        <td id=\"T_8679e_row4_col2\" class=\"data row4 col2\" >4.94</td>\n",
       "                        <td id=\"T_8679e_row4_col3\" class=\"data row4 col3\" >0.21</td>\n",
       "                        <td id=\"T_8679e_row4_col4\" class=\"data row4 col4\" >296.21</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_8679e_row5_col0\" class=\"data row5 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row5_col1\" class=\"data row5 col1\" >6</td>\n",
       "                        <td id=\"T_8679e_row5_col2\" class=\"data row5 col2\" >4.11</td>\n",
       "                        <td id=\"T_8679e_row5_col3\" class=\"data row5 col3\" >0.17</td>\n",
       "                        <td id=\"T_8679e_row5_col4\" class=\"data row5 col4\" >246.84</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_8679e_row6_col0\" class=\"data row6 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row6_col1\" class=\"data row6 col1\" >7</td>\n",
       "                        <td id=\"T_8679e_row6_col2\" class=\"data row6 col2\" >3.53</td>\n",
       "                        <td id=\"T_8679e_row6_col3\" class=\"data row6 col3\" >0.15</td>\n",
       "                        <td id=\"T_8679e_row6_col4\" class=\"data row6 col4\" >211.58</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8679e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_8679e_row7_col0\" class=\"data row7 col0\" >100,000,000</td>\n",
       "                        <td id=\"T_8679e_row7_col1\" class=\"data row7 col1\" >8</td>\n",
       "                        <td id=\"T_8679e_row7_col2\" class=\"data row7 col2\" >3.09</td>\n",
       "                        <td id=\"T_8679e_row7_col3\" class=\"data row7 col3\" >0.13</td>\n",
       "                        <td id=\"T_8679e_row7_col4\" class=\"data row7 col4\" >185.13</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6a6756a710>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Projections\n",
    "l_estimates_ = list()\n",
    "\n",
    "time_mins = 1.25\n",
    "rows_embedded_ = 84400\n",
    "\n",
    "rows_to_embed_ = int(100e6)\n",
    "projected_time_mins = time_mins * (rows_to_embed_ / rows_embedded_)\n",
    "\n",
    "l_estimates_.append(\n",
    "    {\n",
    "        'n_rows': rows_to_embed_,\n",
    "        'n_jobs': 1,\n",
    "        'projected_hours': projected_time_mins / 60,\n",
    "        'projected_days': projected_time_mins / (60 * 24),\n",
    "        'projected_mins': projected_time_mins,\n",
    "    }\n",
    ")\n",
    "\n",
    "for n_parallel_jobs_ in range(2, 9):\n",
    "    proj_mins_parallel = projected_time_mins / n_parallel_jobs_\n",
    "    l_estimates_.append(\n",
    "        {\n",
    "            'n_rows': rows_to_embed_,\n",
    "            'n_jobs': n_parallel_jobs_,\n",
    "            'projected_hours': proj_mins_parallel / 60, \n",
    "            'projected_days': proj_mins_parallel / (60 * 24), \n",
    "            'projected_mins': proj_mins_parallel,\n",
    "        }\n",
    "    )\n",
    "\n",
    "style_df_numeric(\n",
    "    pd.DataFrame(l_estimates_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42392180",
   "metadata": {},
   "source": [
    "## Files sequentially\n",
    "\n",
    "For these tests, see the previous notebook (02.2). We can't test multiple files with the latest subreddit descriptions/meta because it all fits in a single file now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271517d",
   "metadata": {},
   "source": [
    "### only 1 file (sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00269801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run on sample data\n",
    "\n",
    "# !cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "#     --config-name $config_vectorize \\\n",
    "#     process_individual_files=true \\\n",
    "#     limit_first_n_chars=500 \\\n",
    "#     batch_inference_rows=2600 \\\n",
    "#     n_sample_files=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b22d3",
   "metadata": {},
   "source": [
    "### all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f829cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run on sample data\n",
    "\n",
    "# !cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "#     --config-name $config_vectorize \\\n",
    "#     process_individual_files=true \\\n",
    "#     limit_first_n_chars=900 \\\n",
    "#     batch_inference_rows=1600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b8ec6",
   "metadata": {},
   "source": [
    "### Only files in slice (first 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402fc961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run on sample data\n",
    "\n",
    "# !cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "#     --config-name $config_vectorize \\\n",
    "#     process_individual_files=true \\\n",
    "#     limit_first_n_chars=500 \\\n",
    "#     batch_inference_rows=2600 \\\n",
    "#     n_files_slice_end=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b292222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run on sample data\n",
    "\n",
    "# !cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "#     --config-name $config_vectorize \\\n",
    "#     process_individual_files=true \\\n",
    "#     limit_first_n_chars=500 \\\n",
    "#     batch_inference_rows=2600 \\\n",
    "#     n_files_slice_start=0 \\\n",
    "#     n_files_slice_end=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e7391",
   "metadata": {},
   "source": [
    "### Only last file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run on sample data\n",
    "\n",
    "# !cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "#     --config-name $config_vectorize \\\n",
    "#     process_individual_files=true \\\n",
    "#     limit_first_n_chars=500 \\\n",
    "#     batch_inference_rows=2600 \\\n",
    "#     n_files_slice_start=1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
