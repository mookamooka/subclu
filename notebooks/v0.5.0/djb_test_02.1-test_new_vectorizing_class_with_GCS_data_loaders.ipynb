{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a86c2d9",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to test the new data-loader (GCS), configs, and embeddings class that we'll be using in kubeflow.\n",
    "\n",
    "For inference (getting embeddings) it might be better to read from GCS than from SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df9695",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8a3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd19f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.20.0\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "# from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "# new modules to test:\n",
    "from google.cloud import bigquery\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import (\n",
    "    LoadSubredditsGCS\n",
    ")\n",
    "from subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf import(\n",
    "    get_embeddings_as_df,\n",
    "    upload_folder_to_gcs,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edf190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:56:49 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8855a",
   "metadata": {},
   "source": [
    "# Auth note\n",
    "This notebook assumes you have authenticated using the gcloud CLI. Example</br>\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2bfaaa",
   "metadata": {},
   "source": [
    "# Test embeddings function on plain df\n",
    "\n",
    "To make sure that the function itself is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c370630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:57:29 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "16:57:29 | INFO | \"  20 <- Files matching prefix\"\n",
      "16:57:29 | INFO | \"  20 <- Files to check\"\n",
      "16:57:29 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "16:57:29 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "16:57:29 | INFO | \"  Files already cached: 2\"\n",
      "16:57:29 | INFO | \"  Files already downloaded.\"\n",
      "16:57:29 | INFO | \"  df format: pandas\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <- Local files\n",
      "2 <- Local parquet files\n",
      "(159874, 3)\n",
      "CPU times: user 401 ms, sys: 255 ms, total: 656 ms\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_name_title_related_subs_and_clean_descriptions'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "subs.local_cache()\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "assert 2 == len(subs.local_files_)\n",
    "\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f92a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text\n",
    "# model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68ffc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Get embeddings\n",
    "# df_vect_ = get_embeddings_as_df(\n",
    "#     model=model,\n",
    "#     df=df_,\n",
    "#     col_text='subreddit_name_title_related_subs_and_clean_descriptions',\n",
    "#     cols_index=['subreddit_id', 'subreddit_name'],\n",
    "#     verbose_init=True,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5e595",
   "metadata": {},
   "source": [
    "# Test upload folder to GCS\n",
    "\n",
    "We need this if we're not using mlflow to track a model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55ad7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:10:56 | INFO | \"dry_run=True\"\n",
      "17:10:57 | INFO | \"Uploading file\n",
      "  from: /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/000000000000.parquet\n",
      "  to: gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/TEST_UPLOAD/000000000000.parquet\"\n",
      "17:10:57 | INFO | \"Uploading file\n",
      "  from: /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/000000000001_test.parquet\n",
      "  to: gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/TEST_UPLOAD/000000000001_test.parquet\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 24.1 ms, total: 41.5 ms\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upload_folder_to_gcs(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_output_root='i18n_topic_model_batch/runs/20220412/subreddits/text/TEST_UPLOAD',\n",
    "    local_dir='/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    gcs_new_subfolder=None,\n",
    "    verbose=True,\n",
    "    dry_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dc7721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:52:08 | INFO | \"dry_run=True\"\n",
      "17:52:10 | INFO | \"Uploading file\n",
      "  from: /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/000000000000.parquet\n",
      "  to: gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/TEST_UPLOAD/subfolder_test/000000000000.parquet\"\n",
      "17:52:10 | INFO | \"Uploading file\n",
      "  from: /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/000000000001_test.parquet\n",
      "  to: gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/TEST_UPLOAD/subfolder_test/000000000001_test.parquet\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\n",
      "CPU times: user 24 ms, sys: 16.4 ms, total: 40.4 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upload_folder_to_gcs(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_output_root='i18n_topic_model_batch/runs/20220412/subreddits/text/TEST_UPLOAD',\n",
    "    local_dir='/home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    gcs_new_subfolder='subfolder_test',\n",
    "    verbose=True,\n",
    "    dry_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ac986",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c800e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddits_test_local\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_subreddits_test_local'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae99fe",
   "metadata": {},
   "source": [
    "## All files as single DF\n",
    "\n",
    "For subreddit-metadata, aim for:\n",
    "- `first_n_chars` below 1,900\n",
    "- `batch_inference_rows` below 1,600\n",
    "\n",
    "\n",
    "```bash\n",
    "`lowercase_text: False`\n",
    "`limit_first_n_chars: 2200`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 2000`\n",
    "0:03:58.241812 <- df_subs vectorizing time elapsed\n",
    "# large batch & too many characters makes it slow b/c it runs OOM often, so we spend a lot of time retrying.\n",
    "\n",
    "\n",
    "`limit_first_n_chars: 2000`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1700`\n",
    "0:02:21.720878 <- df_subs vectorizing time elapsed\n",
    "\n",
    "`limit_first_n_chars: 1900`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1600`\n",
    "0:02:04.692077 <- df_subs vectorizing time elapsed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c5949bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 22:04:55,525` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 22:04:55,525` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 22:04:57,101` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 22:05:01,105` | `INFO` | `Model loaded`\n",
      "`2022-04-14 22:05:01,105` | `INFO` | `Loading text...`\n",
      "`2022-04-14 22:05:02,357` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `  19 <- Files matching prefix`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `  19 <- Files to check`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `0:00:01.324496  <- Downloading files elapsed time`\n",
      "`2022-04-14 22:05:02,430` | `INFO` | `  df format: pandas`\n",
      "`2022-04-14 22:05:02,874` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:05:02,879` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:05:02,879` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:05:02,879` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:05:02,879` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 22:05:02,879` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:05:03,105` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 22:05:14,327` | `INFO` | `  Vectorizing:  18%|####9                       | 11/62 [00:11<00:52,  1.02s/it]`\n",
      "`2022-04-14 22:05:25,402` | `INFO` | `  Vectorizing:  42%|###########7                | 26/62 [00:22<00:30,  1.20it/s]`\n",
      "`2022-04-14 22:05:37,855` | `INFO` | `  Vectorizing:  66%|##################5         | 41/62 [00:34<00:17,  1.20it/s]`\n",
      "`2022-04-14 22:05:49,517` | `INFO` | `  Vectorizing:  92%|#########################7  | 57/62 [00:46<00:03,  1.27it/s]`\n",
      "`2022-04-14 22:05:52,591` | `INFO` | `  Vectorizing: 100%|############################| 62/62 [00:49<00:00,  1.25it/s]`\n",
      "\n",
      "`2022-04-14 22:05:53,183` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_220455/df-159874_by_514.parquet`\n",
      "`2022-04-14 22:06:07,772` | `INFO` | `  0:01:06.666625 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 22:06:07,772` | `INFO` | `  0:01:12.247719 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=false \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c990612c",
   "metadata": {},
   "source": [
    "## Files sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff52c8",
   "metadata": {},
   "source": [
    "### only 1 file (sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3255ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'local_model_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-15 18:02:12,129` | `INFO` | `Using hydra's path`\n",
      "`2022-04-15 18:02:12,129` | `INFO` | `  Log file created at: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-04-15/18-02-12/logs/2022-04-15_18-02-12_vectorize_text.log`\n",
      "`2022-04-15 18:02:12,129` | `INFO` | `Start vectorize function`\n",
      "`2022-04-15 18:02:12,130` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-15 18:02:13,638` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-15 18:02:17,654` | `INFO` | `Model loaded`\n",
      "`2022-04-15 18:02:18,899` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-15 18:02:18,970` | `INFO` | `  1 <- Files matching prefix`\n",
      "`2022-04-15 18:02:18,970` | `INFO` | `  1 <- Files to check`\n",
      "`2022-04-15 18:02:18,970` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-15 18:02:18,971` | `INFO` | `  Files already cached: 1`\n",
      "`2022-04-15 18:02:18,971` | `INFO` | `0:00:01.316144  <- Downloading files elapsed time`\n",
      "`2022-04-15 18:02:18,972` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-15 18:02:19,366` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-04-15 18:02:19,366` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-15 18:02:19,369` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-15 18:02:19,369` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-15 18:02:19,369` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-15 18:02:19,369` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-15 18:02:19,369` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-15 18:02:19,579` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-15 18:02:30,973` | `INFO` | `  Vectorizing:  39%|##########8                 | 12/31 [00:11<00:18,  1.05it/s]`\n",
      "`2022-04-15 18:02:42,457` | `INFO` | `  Vectorizing:  94%|##########################1 | 29/31 [00:22<00:01,  1.31it/s]`\n",
      "`2022-04-15 18:02:43,568` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:23<00:00,  1.29it/s]`\n",
      "\n",
      "`2022-04-15 18:02:43,934` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-15_180212/000000000000-79937_by_514.parquet`\n",
      "`2022-04-15 18:02:50,462` | `INFO` | `Files in batch: 100%|##############| 1/1 [00:31<00:00, 31.49s/it]`\n",
      "`2022-04-15 18:02:50,463` | `INFO` | `Files in batch: 100%|##############| 1/1 [00:31<00:00, 31.49s/it]`\n",
      "\n",
      "`2022-04-15 18:02:50,463` | `INFO` | `  0:00:32.808434 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-15 18:02:51,744` | `INFO` | `Saving log file...`\n",
      "`2022-04-15 18:02:51,932` | `INFO` | `Saving hydra config...`\n",
      "/home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-04-15/18-02-12/.hydra\n",
      "`2022-04-15 18:02:53,893` | `INFO` | `  0:00:41.763970 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_sample_files=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f142cee",
   "metadata": {},
   "source": [
    "### all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee73434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 21:59:22,813` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 21:59:22,813` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 21:59:24,383` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 21:59:28,412` | `INFO` | `Model loaded`\n",
      "`2022-04-14 21:59:29,716` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 21:59:29,801` | `INFO` | `  11 <- Files matching prefix`\n",
      "`2022-04-14 21:59:29,801` | `INFO` | `  11 <- Files to check`\n",
      "`2022-04-14 21:59:29,801` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 21:59:29,801` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 21:59:29,801` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 21:59:29,801` | `INFO` | `0:00:01.389164  <- Downloading files elapsed time`\n",
      "`2022-04-14 21:59:29,803` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 21:59:30,189` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-04-14 21:59:30,189` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 21:59:30,192` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 21:59:30,193` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 21:59:30,193` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 21:59:30,193` | `INFO` | `limit_first_n_chars: 900`\n",
      "`2022-04-14 21:59:30,193` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 21:59:30,386` | `INFO` | `Getting embeddings in batches of size: 1600`\n",
      "`2022-04-14 21:59:41,435` | `INFO` | `  Vectorizing:  22%|######1                     | 11/50 [00:11<00:39,  1.00s/it]`\n",
      "`2022-04-14 21:59:53,007` | `INFO` | `  Vectorizing:  56%|###############6            | 28/50 [00:22<00:17,  1.28it/s]`\n",
      "`2022-04-14 22:00:04,064` | `INFO` | `  Vectorizing:  94%|##########################3 | 47/50 [00:33<00:02,  1.48it/s]`\n",
      "`2022-04-14 22:00:05,681` | `INFO` | `  Vectorizing: 100%|############################| 50/50 [00:35<00:00,  1.42it/s]`\n",
      "\n",
      "`2022-04-14 22:00:06,067` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_215922/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 22:00:14,017` | `INFO` | `Files in batch:  50%|#######       | 1/2 [00:44<00:44, 44.21s/it]`\n",
      "`2022-04-14 22:00:14,438` | `INFO` | `  Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 22:00:14,438` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:00:14,441` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:00:14,441` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:00:14,441` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:00:14,441` | `INFO` | `limit_first_n_chars: 900`\n",
      "`2022-04-14 22:00:14,441` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:00:14,684` | `INFO` | `Getting embeddings in batches of size: 1600`\n",
      "`2022-04-14 22:00:26,043` | `INFO` | `  Vectorizing:  26%|#######2                    | 13/50 [00:11<00:32,  1.14it/s]`\n",
      "`2022-04-14 22:00:37,650` | `INFO` | `  Vectorizing:  60%|################8           | 30/50 [00:22<00:14,  1.33it/s]`\n",
      "`2022-04-14 22:00:48,793` | `INFO` | `  Vectorizing:  98%|###########################4| 49/50 [00:34<00:00,  1.50it/s]`\n",
      "`2022-04-14 22:00:49,305` | `INFO` | `  Vectorizing: 100%|############################| 50/50 [00:34<00:00,  1.44it/s]`\n",
      "\n",
      "`2022-04-14 22:00:49,689` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_215922/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 22:00:56,557` | `INFO` | `Files in batch: 100%|##############| 2/2 [01:26<00:00, 43.23s/it]`\n",
      "`2022-04-14 22:00:56,557` | `INFO` | `Files in batch: 100%|##############| 2/2 [01:26<00:00, 43.38s/it]`\n",
      "\n",
      "`2022-04-14 22:00:56,557` | `INFO` | `  0:01:28.145107 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 22:00:56,557` | `INFO` | `  0:01:33.744494 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=900 \\\n",
    "    batch_inference_rows=1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95996be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e36e77f9",
   "metadata": {},
   "source": [
    "### Only files in slice (first 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d42010e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 22:01:42,913` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 22:01:42,913` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 22:01:44,469` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 22:01:48,603` | `INFO` | `Model loaded`\n",
      "`2022-04-14 22:01:49,889` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 22:01:49,964` | `INFO` | `  14 <- Files matching prefix`\n",
      "`2022-04-14 22:01:49,964` | `INFO` | `  14 <- Files to check`\n",
      "`2022-04-14 22:01:49,965` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:01:49,965` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:01:49,965` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 22:01:49,965` | `INFO` | `0:00:01.361151  <- Downloading files elapsed time`\n",
      "`2022-04-14 22:01:49,966` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 22:01:50,367` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-04-14 22:01:50,367` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:01:50,370` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:01:50,370` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:01:50,370` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:01:50,370` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 22:01:50,370` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:01:50,598` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 22:02:01,732` | `INFO` | `  Vectorizing:  35%|#########9                  | 11/31 [00:11<00:20,  1.01s/it]`\n",
      "`2022-04-14 22:02:12,746` | `INFO` | `  Vectorizing:  84%|#######################4    | 26/31 [00:22<00:04,  1.21it/s]`\n",
      "`2022-04-14 22:02:15,977` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:25<00:00,  1.22it/s]`\n",
      "\n",
      "`2022-04-14 22:02:16,351` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_220142/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 22:02:23,360` | `INFO` | `Files in batch:  50%|#######       | 1/2 [00:33<00:33, 33.39s/it]`\n",
      "`2022-04-14 22:02:23,764` | `INFO` | `  Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 22:02:23,764` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:02:23,767` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:02:23,767` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:02:23,767` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:02:23,767` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 22:02:23,767` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:02:23,974` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 22:02:35,479` | `INFO` | `  Vectorizing:  42%|###########7                | 13/31 [00:11<00:15,  1.13it/s]`\n",
      "`2022-04-14 22:02:47,086` | `INFO` | `  Vectorizing:  94%|##########################1 | 29/31 [00:23<00:01,  1.28it/s]`\n",
      "`2022-04-14 22:02:48,284` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:24<00:00,  1.28it/s]`\n",
      "\n",
      "`2022-04-14 22:02:48,669` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_220142/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 22:02:55,209` | `INFO` | `Files in batch: 100%|##############| 2/2 [01:05<00:00, 32.49s/it]`\n",
      "`2022-04-14 22:02:55,210` | `INFO` | `Files in batch: 100%|##############| 2/2 [01:05<00:00, 32.62s/it]`\n",
      "\n",
      "`2022-04-14 22:02:55,210` | `INFO` | `  0:01:06.606386 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 22:02:55,210` | `INFO` | `  0:01:12.296792 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_files_slice_end=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1febd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 22:02:58,149` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 22:02:58,149` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 22:02:59,672` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 22:03:03,807` | `INFO` | `Model loaded`\n",
      "`2022-04-14 22:03:05,083` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 22:03:05,157` | `INFO` | `  16 <- Files matching prefix`\n",
      "`2022-04-14 22:03:05,158` | `INFO` | `  16 <- Files to check`\n",
      "`2022-04-14 22:03:05,158` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:03:05,158` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:03:05,158` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 22:03:05,158` | `INFO` | `0:00:01.350942  <- Downloading files elapsed time`\n",
      "`2022-04-14 22:03:05,160` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 22:03:05,545` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-04-14 22:03:05,545` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:03:05,548` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:03:05,548` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:03:05,548` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:03:05,548` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 22:03:05,548` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:03:05,725` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 22:03:16,797` | `INFO` | `  Vectorizing:  35%|#########9                  | 11/31 [00:11<00:20,  1.01s/it]`\n",
      "`2022-04-14 22:03:28,275` | `INFO` | `  Vectorizing:  87%|########################3   | 27/31 [00:22<00:03,  1.23it/s]`\n",
      "`2022-04-14 22:03:30,787` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:25<00:00,  1.24it/s]`\n",
      "\n",
      "`2022-04-14 22:03:31,165` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_220258/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 22:03:38,018` | `INFO` | `Files in batch:  50%|#######       | 1/2 [00:32<00:32, 32.86s/it]`\n",
      "`2022-04-14 22:03:38,434` | `INFO` | `  Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 22:03:38,434` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:03:38,437` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:03:38,437` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:03:38,437` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:03:38,437` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 22:03:38,437` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:03:38,667` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 22:03:50,207` | `INFO` | `  Vectorizing:  42%|###########7                | 13/31 [00:11<00:15,  1.13it/s]`\n",
      "`2022-04-14 22:04:01,725` | `INFO` | `  Vectorizing:  94%|##########################1 | 29/31 [00:23<00:01,  1.28it/s]`\n",
      "`2022-04-14 22:04:02,927` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:24<00:00,  1.28it/s]`\n",
      "\n",
      "`2022-04-14 22:04:03,315` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_220258/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 22:04:09,712` | `INFO` | `Files in batch: 100%|##############| 2/2 [01:04<00:00, 32.17s/it]`\n",
      "`2022-04-14 22:04:09,713` | `INFO` | `Files in batch: 100%|##############| 2/2 [01:04<00:00, 32.28s/it]`\n",
      "\n",
      "`2022-04-14 22:04:09,713` | `INFO` | `  0:01:05.905996 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 22:04:09,713` | `INFO` | `  0:01:11.563723 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_files_slice_start=0 \\\n",
    "    n_files_slice_end=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d67bc",
   "metadata": {},
   "source": [
    "### Only last file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf8ce09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 22:04:12,714` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 22:04:12,714` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 22:04:14,256` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 22:04:18,291` | `INFO` | `Model loaded`\n",
      "`2022-04-14 22:04:19,670` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 22:04:19,741` | `INFO` | `  18 <- Files matching prefix`\n",
      "`2022-04-14 22:04:19,741` | `INFO` | `  17 <- Files to check`\n",
      "`2022-04-14 22:04:19,741` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 22:04:19,741` | `INFO` | `  Files already cached: 1`\n",
      "`2022-04-14 22:04:19,741` | `INFO` | `0:00:01.449830  <- Downloading files elapsed time`\n",
      "`2022-04-14 22:04:19,743` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 22:04:20,131` | `INFO` | `  Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 22:04:20,132` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:04:20,135` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 22:04:20,135` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 22:04:20,135` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 22:04:20,135` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 22:04:20,135` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 22:04:20,322` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 22:04:31,975` | `INFO` | `  Vectorizing:  39%|##########8                 | 12/31 [00:11<00:18,  1.03it/s]`\n",
      "`2022-04-14 22:04:43,397` | `INFO` | `  Vectorizing:  90%|#########################2  | 28/31 [00:23<00:02,  1.25it/s]`\n",
      "`2022-04-14 22:04:45,213` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:24<00:00,  1.25it/s]`\n",
      "\n",
      "`2022-04-14 22:04:45,590` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_220412/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 22:04:52,663` | `INFO` | `Files in batch: 100%|##############| 1/1 [00:32<00:00, 32.92s/it]`\n",
      "`2022-04-14 22:04:52,664` | `INFO` | `Files in batch: 100%|##############| 1/1 [00:32<00:00, 32.92s/it]`\n",
      "\n",
      "`2022-04-14 22:04:52,664` | `INFO` | `  0:00:34.372433 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 22:04:52,664` | `INFO` | `  0:00:39.950001 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_files_slice_start=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c911cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
