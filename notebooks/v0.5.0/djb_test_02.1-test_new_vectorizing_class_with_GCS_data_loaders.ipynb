{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27518d33",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to test the new data-loader (GCS), configs, and embeddings class that we'll be using in kubeflow.\n",
    "\n",
    "For inference (getting embeddings) it might be better to read from GCS than from SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ada3d",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980fce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a9097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.20.0\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "# from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "# new modules to test:\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2eb6ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:39:41 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc515c6a",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069e386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddits_test_local\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_subreddits_test_local'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b97279",
   "metadata": {},
   "source": [
    "## All files as single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6bccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'output_folder', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 05:55:50,674` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 05:55:50,674` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 05:55:52,276` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 05:55:56,426` | `INFO` | `Model loaded`\n",
      "`2022-04-14 05:55:56,426` | `INFO` | `Loading text...`\n",
      "`2022-04-14 05:55:57,732` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 05:55:57,795` | `INFO` | `  2 <- Files matching prefix`\n",
      "`2022-04-14 05:55:57,795` | `INFO` | `  2 <- Files to check`\n",
      "`2022-04-14 05:55:57,795` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 05:55:57,795` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 05:55:57,795` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 05:55:57,795` | `INFO` | `0:00:01.369054  <- Downloading files elapsed time`\n",
      "`2022-04-14 05:55:57,796` | `INFO` | `  df format: pandas`\n",
      "0    place. r/place. \\nplace\\nplace\\n\\nSome have vi...\n",
      "1    AskReddit. r/AskReddit. \\nAsk Reddit...\\naskre...\n",
      "2    interestingasfuck. r/interestingasfuck. \\nInte...\n",
      "3    antiwork. r/antiwork. \\nAntiwork: Unemployment...\n",
      "4    worldnews. r/worldnews. \\nWorld News\\nNews, Po...\n",
      "Name: subreddit_name_title_related_subs_and_clean_descriptions, dtype: object\n",
      "`2022-04-14 05:55:58,254` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 05:55:58,258` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 05:55:58,258` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 05:55:58,259` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 05:55:58,259` | `INFO` | `limit_first_n_chars: 2500`\n",
      "`2022-04-14 05:55:58,259` | `INFO` | `limit_first_n_chars_retry: 600`\n",
      "`2022-04-14 05:55:58,500` | `INFO` | `Getting embeddings in batches of size: 2000`\n",
      "2022-04-14 05:56:10.830992: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.72GiB (rounded to 1851478016)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_2/Ngram-2-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-04-14 05:56:10.831543: W tensorflow/core/common_runtime/bfc_allocator.cc:439] **************************************************************************************************__\n",
      "2022-04-14 05:56:10.831610: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[904042,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-04-14 05:56:20.831830: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.59GiB (rounded to 2777217024)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_3/Ngram-3-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-04-14 05:56:20.832369: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************************_____**************_____***************************************__\n",
      "2022-04-14 05:56:20.832405: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[904042,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-04-14 05:56:30.832687: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.31GiB (rounded to 4628695040)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-04-14 05:56:30.833260: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************************___________********___________********_____********************__\n",
      "2022-04-14 05:56:30.833300: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[904042,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-04-14 05:56:30,834` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[904042,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_2/Ngram-2-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "\n",
      "Error executing job with overrides: ['process_individual_files=false']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 384, in get_embeddings_as_df\n",
      "    limit_first_n_chars=limit_first_n_chars,\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 343, in get_embeddings_as_df\n",
      "    model(series_text.to_list()).numpy()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 509, in _call_attribute\n",
      "    return instance.__call__(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\n",
      "    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\n",
      "    cancellation_manager=cancellation_manager)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\n",
      "    ctx=ctx)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[904042,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_2/Ngram-2-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 89, in vectorize_text\n",
      "    vect.get_embeddings()\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 182, in get_embeddings\n",
      "    model=model,\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 238, in _vectorize_single_df\n",
      "    verbose_init=self.get_embeddings_verbose,\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 399, in get_embeddings_as_df\n",
      "    limit_first_n_chars=limit_first_n_chars_retry,\n",
      "  File \"/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/i18n_topic_model_batch/subclu2/get_embeddings/vectorize_text_tf.py\", line 351, in get_embeddings_as_df\n",
      "    ).set_index(cols_index)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\", line 4727, in set_index\n",
      "    raise KeyError(f\"None of {missing} are in the columns\")\n",
      "KeyError: \"None of [['subreddit_id', 'subreddit_name']] are in the columns\"\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0752c7",
   "metadata": {},
   "source": [
    "## Files sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d0da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
