{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f679f7",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to test the new data-loader (GCS), configs, and embeddings class that we'll be using in kubeflow.\n",
    "\n",
    "For inference (getting embeddings) it might be better to read from GCS than from SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e81eec",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6f617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6813835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.20.0\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "# from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "# new modules to test:\n",
    "from google.cloud import bigquery\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import (\n",
    "    LoadSubredditsGCS\n",
    ")\n",
    "from subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf import(\n",
    "    get_embeddings_as_df\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b57516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:54:05 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3575f5",
   "metadata": {},
   "source": [
    "# Test embeddings function on plain df\n",
    "\n",
    "To make sure that the function itself is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f36df001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:13:59 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text\"\n",
      "08:13:59 | INFO | \"  2 <- Files matching prefix\"\n",
      "08:13:59 | INFO | \"  2 <- Files to check\"\n",
      "08:13:59 | INFO | \"    000000000000.parquet <- File already exists, not downloading\"\n",
      "08:13:59 | INFO | \"    000000000001_test.parquet <- File already exists, not downloading\"\n",
      "08:13:59 | INFO | \"  Files already cached: 2\"\n",
      "08:13:59 | INFO | \"0:00:01.398938  <- Downloading files elapsed time\"\n",
      "08:13:59 | INFO | \"  Files already downloaded.\"\n",
      "08:13:59 | INFO | \"  df format: pandas\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 <- Local files\n",
      "2 <- Local parquet files\n",
      "(159874, 3)\n",
      "CPU times: user 374 ms, sys: 310 ms, total: 685 ms\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "subs = LoadSubredditsGCS(\n",
    "    bucket_name='gazette-models-temp',\n",
    "    gcs_path='i18n_topic_model_batch/runs/20220412/subreddits/text',\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=['subreddit_id', 'subreddit_name', 'subreddit_name_title_related_subs_and_clean_descriptions'],\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    ")\n",
    "subs.local_cache()\n",
    "\n",
    "print(f\"{len(subs.local_files_)} <- Local files\")\n",
    "print(f\"{len(subs.local_parquet_files_)} <- Local parquet files\")\n",
    "assert 2 == len(subs.local_files_)\n",
    "\n",
    "df_ = subs.read_as_one_df()\n",
    "print(df_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1572993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text\n",
    "# model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffaf76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Get embeddings\n",
    "# df_vect_ = get_embeddings_as_df(\n",
    "#     model=model,\n",
    "#     df=df_,\n",
    "#     col_text='subreddit_name_title_related_subs_and_clean_descriptions',\n",
    "#     cols_index=['subreddit_id', 'subreddit_name'],\n",
    "#     verbose_init=True,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199ab9f",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697df39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddits_test_local\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_subreddits_test_local'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a27cf7",
   "metadata": {},
   "source": [
    "## All files as single DF\n",
    "\n",
    "For subreddit-metadata, aim for:\n",
    "- `first_n_chars` below 1,900\n",
    "- `batch_inference_rows` below 1,600\n",
    "\n",
    "\n",
    "```bash\n",
    "`lowercase_text: False`\n",
    "`limit_first_n_chars: 2200`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 2000`\n",
    "0:03:58.241812 <- df_subs vectorizing time elapsed\n",
    "# large batch & too many characters makes it slow b/c it runs OOM often, so we spend a lot of time retrying.\n",
    "\n",
    "\n",
    "`limit_first_n_chars: 2000`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1700`\n",
    "0:02:21.720878 <- df_subs vectorizing time elapsed\n",
    "\n",
    "`limit_first_n_chars: 1900`\n",
    "`limit_first_n_chars_retry: 700`\n",
    "`Getting embeddings in batches of size: 1600`\n",
    "0:02:04.692077 <- df_subs vectorizing time elapsed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6744c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'output_folder', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 07:46:59,786` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 07:46:59,786` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 07:47:01,328` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 07:47:05,455` | `INFO` | `Model loaded`\n",
      "`2022-04-14 07:47:05,455` | `INFO` | `Loading text...`\n",
      "`2022-04-14 07:47:06,737` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `  2 <- Files matching prefix`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `  2 <- Files to check`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `0:00:01.359208  <- Downloading files elapsed time`\n",
      "`2022-04-14 07:47:06,814` | `INFO` | `  df format: pandas`\n",
      "`2022-04-14 07:47:07,277` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 07:47:07,282` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 07:47:07,282` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 07:47:07,282` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 07:47:07,282` | `INFO` | `limit_first_n_chars: 1900`\n",
      "`2022-04-14 07:47:07,282` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 07:47:07,523` | `INFO` | `Getting embeddings in batches of size: 1600`\n",
      "2022-04-14 07:47:19.159858: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.93GiB (rounded to 3143736320)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-04-14 07:47:19.160528: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *************************_______*****________*****____***********************************___________\n",
      "2022-04-14 07:47:19.160572: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[614011,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-04-14 07:47:19,162` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[614011,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-04-14 07:47:20,324` | `INFO` | `progress:   1%|3                                | 1/100 [00:12<21:07, 12.80s/it]`\n",
      "`2022-04-14 07:47:32,002` | `INFO` | `progress:  10%|###2                            | 10/100 [00:24<03:11,  2.13s/it]`\n",
      "`2022-04-14 07:47:43,508` | `INFO` | `progress:  23%|#######3                        | 23/100 [00:35<01:40,  1.31s/it]`\n",
      "`2022-04-14 07:47:55,061` | `INFO` | `progress:  39%|############4                   | 39/100 [00:47<01:00,  1.00it/s]`\n",
      "`2022-04-14 07:48:07,759` | `INFO` | `progress:  39%|############4                   | 39/100 [01:00<01:00,  1.00it/s]`\n",
      "2022-04-14 07:48:12.073818: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.90GiB (rounded to 3115489280)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-04-14 07:48:12.074509: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************________*****________*****___***********************************____________\n",
      "2022-04-14 07:48:12.074557: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[608494,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-04-14 07:48:12,075` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[608494,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-04-14 07:48:13,270` | `INFO` | `progress:  51%|################3               | 51/100 [01:05<00:58,  1.19s/it]`\n",
      "`2022-04-14 07:48:25,189` | `INFO` | `progress:  60%|###################2            | 60/100 [01:17<00:48,  1.22s/it]`\n",
      "`2022-04-14 07:48:36,969` | `INFO` | `progress:  73%|#######################3        | 73/100 [01:29<00:29,  1.11s/it]`\n",
      "`2022-04-14 07:48:47,814` | `INFO` | `progress:  73%|#######################3        | 73/100 [01:40<00:29,  1.11s/it]`\n",
      "`2022-04-14 07:48:48,089` | `INFO` | `progress:  88%|############################1   | 88/100 [01:40<00:11,  1.03it/s]`\n",
      "`2022-04-14 07:48:55,593` | `INFO` | `progress: 100%|###############################| 100/100 [01:48<00:00,  1.08s/it]`\n",
      "\n",
      "`2022-04-14 07:48:56,194` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220413/subreddits/embeddings/2022-04-14_074659/df-159874_by_514.parquet`\n",
      "`2022-04-14 07:49:10,147` | `INFO` | `  0:02:04.692077 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 07:49:10,148` | `INFO` | `  0:02:10.361288 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5914cc8",
   "metadata": {},
   "source": [
    "## Files sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1531d",
   "metadata": {},
   "source": [
    "### all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f93bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 08:42:48,738` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 08:42:48,738` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 08:42:50,309` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 08:42:54,761` | `INFO` | `Model loaded`\n",
      "`2022-04-14 08:42:56,079` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 08:42:56,146` | `INFO` | `  2 <- Files matching prefix`\n",
      "`2022-04-14 08:42:56,147` | `INFO` | `  2 <- Files to check`\n",
      "`2022-04-14 08:42:56,147` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:42:56,147` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:42:56,147` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 08:42:56,147` | `INFO` | `0:00:01.385721  <- Downloading files elapsed time`\n",
      "`2022-04-14 08:42:56,149` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 08:42:56,553` | `INFO` | `Processing: 000000000000.parquet`\n",
      "`2022-04-14 08:42:56,553` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:42:56,556` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:42:56,556` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:42:56,556` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:42:56,556` | `INFO` | `limit_first_n_chars: 900`\n",
      "`2022-04-14 08:42:56,556` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:42:56,761` | `INFO` | `Getting embeddings in batches of size: 1600`\n",
      "`2022-04-14 08:43:08,252` | `INFO` | `progress:  24%|#######9                         | 12/50 [00:11<00:36,  1.04it/s]`\n",
      "`2022-04-14 08:43:19,416` | `INFO` | `progress:  58%|###################1             | 29/50 [00:22<00:15,  1.32it/s]`\n",
      "`2022-04-14 08:43:30,611` | `INFO` | `progress:  98%|################################3| 49/50 [00:33<00:00,  1.53it/s]`\n",
      "`2022-04-14 08:43:31,124` | `INFO` | `progress: 100%|#################################| 50/50 [00:34<00:00,  1.46it/s]`\n",
      "\n",
      "`2022-04-14 08:43:31,514` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_084248/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 08:43:38,399` | `INFO` | `progress: : 1it [00:42, 42.25s/it]`\n",
      "`2022-04-14 08:43:38,811` | `INFO` | `Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 08:43:38,811` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:43:38,814` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:43:38,814` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:43:38,814` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:43:38,814` | `INFO` | `limit_first_n_chars: 900`\n",
      "`2022-04-14 08:43:38,814` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:43:39,017` | `INFO` | `Getting embeddings in batches of size: 1600`\n",
      "`2022-04-14 08:43:50,177` | `INFO` | `progress:  26%|########5                        | 13/50 [00:11<00:31,  1.16it/s]`\n",
      "`2022-04-14 08:44:01,463` | `INFO` | `progress:  60%|###################8             | 30/50 [00:22<00:14,  1.37it/s]`\n",
      "`2022-04-14 08:44:12,823` | `INFO` | `progress: 100%|#################################| 50/50 [00:33<00:00,  1.55it/s]`\n",
      "`2022-04-14 08:44:12,823` | `INFO` | `progress: 100%|#################################| 50/50 [00:33<00:00,  1.48it/s]`\n",
      "\n",
      "`2022-04-14 08:44:13,218` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_084248/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 08:44:19,345` | `INFO` | `progress: : 2it [01:23, 41.48s/it]`\n",
      "`2022-04-14 08:44:19,345` | `INFO` | `progress: : 2it [01:23, 41.60s/it]`\n",
      "\n",
      "`2022-04-14 08:44:19,345` | `INFO` | `  0:01:24.583756 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 08:44:19,345` | `INFO` | `  0:01:30.607392 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=900 \\\n",
    "    batch_inference_rows=1600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7307baf",
   "metadata": {},
   "source": [
    "### only 1 file (sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16d0694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 08:51:52,441` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 08:51:52,441` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 08:51:54,641` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 08:51:58,922` | `INFO` | `Model loaded`\n",
      "`2022-04-14 08:52:00,240` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 08:52:00,315` | `INFO` | `  1 <- Files matching prefix`\n",
      "`2022-04-14 08:52:00,315` | `INFO` | `  1 <- Files to check`\n",
      "`2022-04-14 08:52:00,316` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:52:00,316` | `INFO` | `  Files already cached: 1`\n",
      "`2022-04-14 08:52:00,316` | `INFO` | `0:00:01.393749  <- Downloading files elapsed time`\n",
      "`2022-04-14 08:52:00,317` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 08:52:00,717` | `INFO` | `Processing: 000000000000.parquet`\n",
      "`2022-04-14 08:52:00,717` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:52:00,720` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:52:00,720` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:52:00,720` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:52:00,720` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 08:52:00,721` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:52:00,934` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 08:52:12,634` | `INFO` | `progress:  39%|############7                    | 12/31 [00:11<00:18,  1.03it/s]`\n",
      "`2022-04-14 08:52:23,799` | `INFO` | `progress:  90%|#############################8   | 28/31 [00:22<00:02,  1.26it/s]`\n",
      "`2022-04-14 08:52:25,592` | `INFO` | `progress: 100%|#################################| 31/31 [00:24<00:00,  1.26it/s]`\n",
      "\n",
      "`2022-04-14 08:52:25,974` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_085152/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 08:52:32,764` | `INFO` | `progress: 100%|####################| 1/1 [00:32<00:00, 32.45s/it]`\n",
      "`2022-04-14 08:52:32,765` | `INFO` | `progress: 100%|####################| 1/1 [00:32<00:00, 32.45s/it]`\n",
      "\n",
      "`2022-04-14 08:52:32,765` | `INFO` | `  0:00:33.843387 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 08:52:32,765` | `INFO` | `  0:00:40.324109 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_sample_files=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c5f4c",
   "metadata": {},
   "source": [
    "### only files in slice (first 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cac83c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 08:53:53,278` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 08:53:53,278` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 08:53:54,815` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 08:53:59,055` | `INFO` | `Model loaded`\n",
      "`2022-04-14 08:54:00,323` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 08:54:00,394` | `INFO` | `  5 <- Files matching prefix`\n",
      "`2022-04-14 08:54:00,395` | `INFO` | `  5 <- Files to check`\n",
      "`2022-04-14 08:54:00,395` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:54:00,395` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:54:00,395` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 08:54:00,395` | `INFO` | `0:00:01.340269  <- Downloading files elapsed time`\n",
      "`2022-04-14 08:54:00,397` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 08:54:00,802` | `INFO` | `Processing: 000000000000.parquet`\n",
      "`2022-04-14 08:54:00,802` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:54:00,805` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:54:00,805` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:54:00,805` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:54:00,805` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 08:54:00,805` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:54:01,027` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 08:54:12,639` | `INFO` | `progress:  39%|############7                    | 12/31 [00:11<00:18,  1.03it/s]`\n",
      "`2022-04-14 08:54:23,823` | `INFO` | `progress:  90%|#############################8   | 28/31 [00:22<00:02,  1.26it/s]`\n",
      "`2022-04-14 08:54:25,617` | `INFO` | `progress: 100%|#################################| 31/31 [00:24<00:00,  1.26it/s]`\n",
      "\n",
      "`2022-04-14 08:54:25,998` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_085353/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 08:54:33,420` | `INFO` | `progress:  50%|##########          | 1/2 [00:33<00:33, 33.02s/it]`\n",
      "`2022-04-14 08:54:33,834` | `INFO` | `Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 08:54:33,834` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:54:33,837` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:54:33,837` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:54:33,837` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:54:33,838` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 08:54:33,838` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:54:34,046` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 08:54:45,332` | `INFO` | `progress:  42%|#############8                   | 13/31 [00:11<00:15,  1.15it/s]`\n",
      "`2022-04-14 08:54:56,649` | `INFO` | `progress:  94%|##############################8  | 29/31 [00:22<00:01,  1.31it/s]`\n",
      "`2022-04-14 08:54:57,826` | `INFO` | `progress: 100%|#################################| 31/31 [00:23<00:00,  1.30it/s]`\n",
      "\n",
      "`2022-04-14 08:54:58,209` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_085353/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 08:55:04,615` | `INFO` | `progress: 100%|####################| 2/2 [01:04<00:00, 31.95s/it]`\n",
      "`2022-04-14 08:55:04,615` | `INFO` | `progress: 100%|####################| 2/2 [01:04<00:00, 32.11s/it]`\n",
      "\n",
      "`2022-04-14 08:55:04,616` | `INFO` | `  0:01:05.560757 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 08:55:04,616` | `INFO` | `  0:01:11.337333 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_files_slice_end=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb54a775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 08:59:16,861` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 08:59:16,861` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 08:59:18,414` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 08:59:22,773` | `INFO` | `Model loaded`\n",
      "`2022-04-14 08:59:24,047` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 08:59:24,121` | `INFO` | `  7 <- Files matching prefix`\n",
      "`2022-04-14 08:59:24,121` | `INFO` | `  7 <- Files to check`\n",
      "`2022-04-14 08:59:24,121` | `INFO` | `    000000000000.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:59:24,121` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 08:59:24,121` | `INFO` | `  Files already cached: 2`\n",
      "`2022-04-14 08:59:24,121` | `INFO` | `0:00:01.347950  <- Downloading files elapsed time`\n",
      "`2022-04-14 08:59:24,123` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 08:59:24,518` | `INFO` | `Processing: 000000000000.parquet`\n",
      "`2022-04-14 08:59:24,518` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:59:24,521` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:59:24,521` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:59:24,521` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:59:24,521` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 08:59:24,521` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:59:24,738` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 08:59:36,321` | `INFO` | `vectorizing:  39%|###########6                  | 12/31 [00:11<00:18,  1.04it/s]`\n",
      "`2022-04-14 08:59:47,383` | `INFO` | `vectorizing:  90%|###########################   | 28/31 [00:22<00:02,  1.27it/s]`\n",
      "`2022-04-14 08:59:49,215` | `INFO` | `vectorizing: 100%|##############################| 31/31 [00:24<00:00,  1.27it/s]`\n",
      "\n",
      "`2022-04-14 08:59:49,583` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_085916/000000000000-79937_by_514.parquet`\n",
      "`2022-04-14 08:59:56,505` | `INFO` | `files in batch:  50%|#######       | 1/2 [00:32<00:32, 32.38s/it]`\n",
      "`2022-04-14 08:59:56,925` | `INFO` | `Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 08:59:56,925` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:59:56,928` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 08:59:56,928` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 08:59:56,928` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 08:59:56,928` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 08:59:56,928` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 08:59:57,164` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 09:00:08,386` | `INFO` | `vectorizing:  42%|############5                 | 13/31 [00:11<00:15,  1.16it/s]`\n",
      "`2022-04-14 09:00:19,667` | `INFO` | `vectorizing:  94%|############################  | 29/31 [00:22<00:01,  1.31it/s]`\n",
      "`2022-04-14 09:00:20,882` | `INFO` | `vectorizing: 100%|##############################| 31/31 [00:23<00:00,  1.31it/s]`\n",
      "\n",
      "`2022-04-14 09:00:21,262` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_085916/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 09:00:28,020` | `INFO` | `files in batch: 100%|##############| 2/2 [01:03<00:00, 31.87s/it]`\n",
      "`2022-04-14 09:00:28,021` | `INFO` | `files in batch: 100%|##############| 2/2 [01:03<00:00, 31.95s/it]`\n",
      "\n",
      "`2022-04-14 09:00:28,021` | `INFO` | `  0:01:05.247596 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 09:00:28,021` | `INFO` | `  0:01:11.159798 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_files_slice_start=0 \\\n",
    "    n_files_slice_end=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d57d78",
   "metadata": {},
   "source": [
    "### Only last filem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c432fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "`2022-04-14 09:02:53,998` | `INFO` | `Start vectorize function`\n",
      "`2022-04-14 09:02:53,998` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-04-14 09:02:55,638` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-04-14 09:02:59,934` | `INFO` | `Model loaded`\n",
      "`2022-04-14 09:03:01,207` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text`\n",
      "`2022-04-14 09:03:01,277` | `INFO` | `  9 <- Files matching prefix`\n",
      "`2022-04-14 09:03:01,277` | `INFO` | `  8 <- Files to check`\n",
      "`2022-04-14 09:03:01,277` | `INFO` | `    000000000001_test.parquet <- File already exists, not downloading`\n",
      "`2022-04-14 09:03:01,278` | `INFO` | `  Files already cached: 1`\n",
      "`2022-04-14 09:03:01,278` | `INFO` | `0:00:01.342976  <- Downloading files elapsed time`\n",
      "`2022-04-14 09:03:01,279` | `INFO` | `  Files already downloaded.`\n",
      "`2022-04-14 09:03:01,683` | `INFO` | `  Processing: 000000000001_test.parquet`\n",
      "`2022-04-14 09:03:01,683` | `INFO` | `Vectorizing column: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 09:03:01,686` | `INFO` | `cols_index: ['subreddit_id', 'subreddit_name']`\n",
      "`2022-04-14 09:03:01,686` | `INFO` | `col_text: subreddit_name_title_related_subs_and_clean_descriptions`\n",
      "`2022-04-14 09:03:01,686` | `INFO` | `lowercase_text: False`\n",
      "`2022-04-14 09:03:01,686` | `INFO` | `limit_first_n_chars: 500`\n",
      "`2022-04-14 09:03:01,686` | `INFO` | `limit_first_n_chars_retry: 700`\n",
      "`2022-04-14 09:03:01,888` | `INFO` | `Getting embeddings in batches of size: 2600`\n",
      "`2022-04-14 09:03:13,582` | `INFO` | `  Vectorizing:  39%|##########8                 | 12/31 [00:11<00:18,  1.03it/s]`\n",
      "`2022-04-14 09:03:24,714` | `INFO` | `  Vectorizing:  90%|#########################2  | 28/31 [00:22<00:02,  1.26it/s]`\n",
      "`2022-04-14 09:03:26,527` | `INFO` | `  Vectorizing: 100%|############################| 31/31 [00:24<00:00,  1.26it/s]`\n",
      "\n",
      "`2022-04-14 09:03:26,912` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model_batch/runs/20220412/subreddits/text/embedding/2022-04-14_090253/000000000001_test-79937_by_514.parquet`\n",
      "`2022-04-14 09:03:33,991` | `INFO` | `Files in batch: 100%|##############| 1/1 [00:32<00:00, 32.71s/it]`\n",
      "`2022-04-14 09:03:33,991` | `INFO` | `Files in batch: 100%|##############| 1/1 [00:32<00:00, 32.71s/it]`\n",
      "\n",
      "`2022-04-14 09:03:33,992` | `INFO` | `  0:00:34.057028 <- df_subs vectorizing time elapsed`\n",
      "`2022-04-14 09:03:33,992` | `INFO` | `  0:00:39.993473 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize \\\n",
    "    process_individual_files=true \\\n",
    "    limit_first_n_chars=500 \\\n",
    "    batch_inference_rows=2600 \\\n",
    "    n_files_slice_start=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59031f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
