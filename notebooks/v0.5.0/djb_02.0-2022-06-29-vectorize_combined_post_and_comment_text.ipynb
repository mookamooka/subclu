{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b95d61",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to vectorize the text of combined Post + Comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d91ff4",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bd4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76fb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([np, pd, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f920670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:04:44 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519aae53",
   "metadata": {},
   "source": [
    "# Auth note\n",
    "This notebook assumes you have authenticated using the gcloud CLI. Example</br>\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb157d00",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize \n",
    "\n",
    "When we call the vectorizing function, it calls the data loader under the hood.\n",
    "See the configs in:\n",
    "- `subclu2/config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9abf7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_post_and_comments_combined_seed_v0.5.0\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_post_and_comments_combined_seed_v0.5.0'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce4a68",
   "metadata": {},
   "source": [
    "## Run in bucket owned by i18n\n",
    "This bucket retains data longer than the gazette temp bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cbdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text', 'config_description', 'local_cache_path', 'local_model_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "Data Loader kwags:\n",
      "  columns: ['subreddit_id', 'subreddit_name', 'post_id', 'post_and_comment_text_clean']\n",
      "  df_format: pandas\n",
      "  unique_check: False\n",
      "  verbose: True\n",
      "  bucket_name: i18n-subreddit-clustering\n",
      "  gcs_path: i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds\n",
      "  local_cache_path: /home/jupyter/subreddit_clustering_i18n/data/local_cache/\n",
      "  n_sample_files: None\n",
      "  n_files_slice_start: None\n",
      "  n_files_slice_end: None\n",
      "`2022-06-29 09:19:25,489` | `INFO` | `Using hydra's path`\n",
      "`2022-06-29 09:19:25,490` | `INFO` | `  Log file created at: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-06-29/09-19-25/logs/2022-06-29_09-19-25_vectorize_text.log`\n",
      "`2022-06-29 09:19:25,490` | `INFO` | `Start vectorize function`\n",
      "`2022-06-29 09:19:25,490` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-06-29 09:19:27,018` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-06-29 09:19:31,289` | `INFO` | `Model loaded`\n",
      "`2022-06-29 09:19:31,289` | `INFO` | `  Loading & Processing each file independently`\n",
      "`2022-06-29 09:19:31,381` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds`\n",
      "`2022-06-29 09:19:31,455` | `INFO` | `  35 <- Files matching prefix`\n",
      "`2022-06-29 09:19:31,455` | `INFO` | `  35 <- Files to check`\n",
      "`2022-06-29 09:20:35,668` | `INFO` | `  Files already cached: 0`\n",
      "`2022-06-29 09:20:35,668` | `INFO` | `0:01:04.379243  <- Downloading files elapsed time`\n",
      "`2022-06-29 09:20:35,670` | `INFO` | `  Files already downloaded.`\n",
      "`2022-06-29 09:20:37,011` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-06-29 09:20:37,011` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:20:37,224` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:20:48,752` | `INFO` | `  Vectorizing:   4%|#1                         | 15/360 [00:11<04:25,  1.30it/s]`\n",
      "`2022-06-29 09:21:00,253` | `INFO` | `  Vectorizing:  10%|##6                        | 35/360 [00:23<03:28,  1.56it/s]`\n",
      "`2022-06-29 09:21:11,353` | `INFO` | `  Vectorizing:  15%|####1                      | 55/360 [00:34<03:02,  1.67it/s]`\n",
      "`2022-06-29 09:21:23,538` | `INFO` | `  Vectorizing:  22%|#####9                     | 79/360 [00:46<02:36,  1.79it/s]`\n",
      "`2022-06-29 09:21:35,839` | `INFO` | `  Vectorizing:  22%|#####9                     | 79/360 [00:58<02:36,  1.79it/s]`\n",
      "`2022-06-29 09:21:35,839` | `INFO` | `  Vectorizing:  27%|#######2                   | 96/360 [00:58<02:41,  1.64it/s]`\n",
      "`2022-06-29 09:21:45,866` | `INFO` | `  Vectorizing:  27%|#######2                   | 96/360 [01:08<02:41,  1.64it/s]`\n",
      "`2022-06-29 09:21:46,935` | `INFO` | `  Vectorizing:  31%|########                  | 112/360 [01:09<02:37,  1.58it/s]`\n",
      "`2022-06-29 09:21:58,638` | `INFO` | `  Vectorizing:  36%|#########2                | 128/360 [01:21<02:33,  1.51it/s]`\n",
      "`2022-06-29 09:22:09,819` | `INFO` | `  Vectorizing:  41%|##########6               | 148/360 [01:32<02:12,  1.59it/s]`\n",
      "`2022-06-29 09:22:21,656` | `INFO` | `  Vectorizing:  47%|############1             | 168/360 [01:44<01:58,  1.62it/s]`\n",
      "`2022-06-29 09:22:34,965` | `INFO` | `  Vectorizing:  52%|#############5            | 187/360 [01:57<01:51,  1.56it/s]`\n",
      "`2022-06-29 09:22:46,093` | `INFO` | `  Vectorizing:  52%|#############5            | 187/360 [02:08<01:51,  1.56it/s]`\n",
      "`2022-06-29 09:22:46,802` | `INFO` | `  Vectorizing:  56%|##############6           | 203/360 [02:09<01:44,  1.50it/s]`\n",
      "`2022-06-29 09:22:58,231` | `INFO` | `  Vectorizing:  62%|################1         | 224/360 [02:21<01:25,  1.60it/s]`\n",
      "`2022-06-29 09:23:10,918` | `INFO` | `  Vectorizing:  68%|#################6        | 245/360 [02:33<01:11,  1.61it/s]`\n",
      "`2022-06-29 09:23:22,176` | `INFO` | `  Vectorizing:  74%|###################2      | 266/360 [02:44<00:55,  1.69it/s]`\n",
      "`2022-06-29 09:23:33,828` | `INFO` | `  Vectorizing:  80%|####################7     | 287/360 [02:56<00:42,  1.72it/s]`\n",
      "`2022-06-29 09:23:45,407` | `INFO` | `  Vectorizing:  85%|######################1   | 307/360 [03:08<00:30,  1.72it/s]`\n",
      "`2022-06-29 09:23:56,222` | `INFO` | `  Vectorizing:  85%|######################1   | 307/360 [03:18<00:30,  1.72it/s]`\n",
      "`2022-06-29 09:23:57,021` | `INFO` | `  Vectorizing:  90%|#######################4  | 324/360 [03:19<00:21,  1.65it/s]`\n",
      "`2022-06-29 09:24:08,427` | `INFO` | `  Vectorizing:  95%|########################7 | 343/360 [03:31<00:10,  1.65it/s]`\n",
      "`2022-06-29 09:24:18,012` | `INFO` | `  Vectorizing: 100%|##########################| 360/360 [03:40<00:00,  1.63it/s]`\n",
      "\n",
      "`2022-06-29 09:24:20,155` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000000-539559_by_515.parquet`\n",
      "`2022-06-29 09:24:50,723` | `INFO` | `Files in batch:   3%|2         | 1/35 [04:15<2:24:31, 255.05s/it]`\n",
      "`2022-06-29 09:24:51,967` | `INFO` | `  Processing: 000000000001.parquet`\n",
      "`2022-06-29 09:24:51,967` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:24:52,220` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:25:03,605` | `INFO` | `  Vectorizing:   4%|#1                         | 17/400 [00:11<04:16,  1.49it/s]`\n",
      "`2022-06-29 09:25:14,675` | `INFO` | `  Vectorizing:   9%|##3                        | 35/400 [00:22<03:52,  1.57it/s]`\n",
      "`2022-06-29 09:25:25,742` | `INFO` | `  Vectorizing:  13%|###5                       | 53/400 [00:33<03:37,  1.60it/s]`\n",
      "`2022-06-29 09:25:36,226` | `INFO` | `  Vectorizing:  13%|###5                       | 53/400 [00:44<03:37,  1.60it/s]`\n",
      "`2022-06-29 09:25:37,521` | `INFO` | `  Vectorizing:  18%|####8                      | 72/400 [00:45<03:24,  1.60it/s]`\n",
      "`2022-06-29 09:25:48,597` | `INFO` | `  Vectorizing:  23%|######1                    | 91/400 [00:56<03:08,  1.64it/s]`\n",
      "`2022-06-29 09:26:01,587` | `INFO` | `  Vectorizing:  28%|#######1                  | 110/400 [01:09<03:04,  1.58it/s]`\n",
      "`2022-06-29 09:26:13,102` | `INFO` | `  Vectorizing:  32%|########3                 | 128/400 [01:20<02:53,  1.57it/s]`\n",
      "`2022-06-29 09:26:24,382` | `INFO` | `  Vectorizing:  37%|#########6                | 148/400 [01:32<02:34,  1.63it/s]`\n",
      "`2022-06-29 09:26:35,601` | `INFO` | `  Vectorizing:  42%|##########9               | 168/400 [01:43<02:18,  1.68it/s]`\n",
      "`2022-06-29 09:26:46,291` | `INFO` | `  Vectorizing:  42%|##########9               | 168/400 [01:54<02:18,  1.68it/s]`\n",
      "`2022-06-29 09:26:46,627` | `INFO` | `  Vectorizing:  46%|###########8              | 183/400 [01:54<02:17,  1.58it/s]`\n",
      "`2022-06-29 09:26:57,808` | `INFO` | `  Vectorizing:  50%|#############1            | 202/400 [02:05<02:02,  1.62it/s]`\n",
      "`2022-06-29 09:27:09,481` | `INFO` | `  Vectorizing:  55%|##############3           | 221/400 [02:17<01:50,  1.62it/s]`\n",
      "`2022-06-29 09:27:20,977` | `INFO` | `  Vectorizing:  60%|###############5          | 239/400 [02:28<01:40,  1.60it/s]`\n",
      "`2022-06-29 09:27:32,270` | `INFO` | `  Vectorizing:  65%|################8         | 259/400 [02:40<01:25,  1.65it/s]`\n",
      "`2022-06-29 09:27:43,790` | `INFO` | `  Vectorizing:  70%|##################3       | 282/400 [02:51<01:07,  1.76it/s]`\n",
      "`2022-06-29 09:27:56,280` | `INFO` | `  Vectorizing:  76%|###################7      | 304/400 [03:04<00:54,  1.76it/s]`\n",
      "`2022-06-29 09:28:06,389` | `INFO` | `  Vectorizing:  76%|###################7      | 304/400 [03:14<00:54,  1.76it/s]`\n",
      "`2022-06-29 09:28:07,405` | `INFO` | `  Vectorizing:  81%|#####################     | 324/400 [03:15<00:42,  1.77it/s]`\n",
      "`2022-06-29 09:28:18,452` | `INFO` | `  Vectorizing:  86%|######################4   | 345/400 [03:26<00:30,  1.81it/s]`\n",
      "`2022-06-29 09:28:29,836` | `INFO` | `  Vectorizing:  92%|#######################7  | 366/400 [03:37<00:18,  1.82it/s]`\n",
      "`2022-06-29 09:28:42,098` | `INFO` | `  Vectorizing:  97%|#########################1| 387/400 [03:49<00:07,  1.79it/s]`\n",
      "`2022-06-29 09:28:48,510` | `INFO` | `  Vectorizing: 100%|##########################| 400/400 [03:56<00:00,  1.69it/s]`\n",
      "\n",
      "`2022-06-29 09:28:50,953` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000001-598576_by_515.parquet`\n",
      "`2022-06-29 09:29:21,816` | `INFO` | `Files in batch:   6%|5         | 2/35 [08:46<2:25:28, 264.49s/it]`\n",
      "`2022-06-29 09:29:22,976` | `INFO` | `  Processing: 000000000002.parquet`\n",
      "`2022-06-29 09:29:22,976` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:29:23,190` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:29:34,264` | `INFO` | `  Vectorizing:   7%|#8                         | 20/297 [00:11<02:33,  1.81it/s]`\n",
      "`2022-06-29 09:29:45,345` | `INFO` | `  Vectorizing:  15%|####                       | 45/297 [00:22<02:01,  2.07it/s]`\n",
      "`2022-06-29 09:29:56,790` | `INFO` | `  Vectorizing:  15%|####                       | 45/297 [00:33<02:01,  2.07it/s]`\n",
      "`2022-06-29 09:29:57,111` | `INFO` | `  Vectorizing:  22%|######                     | 66/297 [00:33<01:59,  1.94it/s]`\n",
      "`2022-06-29 09:30:08,696` | `INFO` | `  Vectorizing:  30%|########                   | 88/297 [00:45<01:48,  1.92it/s]`\n",
      "`2022-06-29 09:30:21,297` | `INFO` | `  Vectorizing:  37%|#########5                | 109/297 [00:58<01:43,  1.82it/s]`\n",
      "`2022-06-29 09:30:32,451` | `INFO` | `  Vectorizing:  44%|###########4              | 131/297 [01:09<01:28,  1.87it/s]`\n",
      "`2022-06-29 09:30:43,574` | `INFO` | `  Vectorizing:  52%|#############3            | 153/297 [01:20<01:15,  1.91it/s]`\n",
      "`2022-06-29 09:30:56,664` | `INFO` | `  Vectorizing:  59%|###############3          | 175/297 [01:33<01:06,  1.83it/s]`\n",
      "`2022-06-29 09:31:06,890` | `INFO` | `  Vectorizing:  59%|###############3          | 175/297 [01:43<01:06,  1.83it/s]`\n",
      "`2022-06-29 09:31:08,005` | `INFO` | `  Vectorizing:  64%|################5         | 189/297 [01:44<01:05,  1.65it/s]`\n",
      "`2022-06-29 09:31:19,245` | `INFO` | `  Vectorizing:  71%|##################3       | 210/297 [01:56<00:50,  1.71it/s]`\n",
      "`2022-06-29 09:31:33,801` | `INFO` | `  Vectorizing:  78%|####################2     | 231/297 [02:10<00:40,  1.62it/s]`\n",
      "`2022-06-29 09:31:47,131` | `INFO` | `  Vectorizing:  78%|####################2     | 231/297 [02:23<00:40,  1.62it/s]`\n",
      "`2022-06-29 09:31:47,569` | `INFO` | `  Vectorizing:  83%|#####################5    | 246/297 [02:24<00:35,  1.45it/s]`\n",
      "`2022-06-29 09:31:58,922` | `INFO` | `  Vectorizing:  87%|######################6   | 259/297 [02:35<00:27,  1.36it/s]`\n",
      "`2022-06-29 09:32:12,356` | `INFO` | `  Vectorizing:  92%|#######################8  | 272/297 [02:49<00:20,  1.24it/s]`\n",
      "2022-06-29 09:32:25.285646: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.75GiB (rounded to 2954501120)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:32:25.286254: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****_______*****___*********************************_________________\n",
      "2022-06-29 09:32:25.286299: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[577051,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:32:25,287` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[577051,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:32:27,203` | `INFO` | `  Vectorizing:  92%|#######################8  | 272/297 [03:04<00:20,  1.24it/s]`\n",
      "2022-06-29 09:32:36.973153: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.76GiB (rounded to 2966599680)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:32:36.973732: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****_______*****___**********************************________________\n",
      "2022-06-29 09:32:36.973766: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[579414,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:32:36,974` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[579414,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:32:38,419` | `INFO` | `  Vectorizing:  93%|########################1 | 276/297 [03:15<00:28,  1.37s/it]`\n",
      "`2022-06-29 09:32:50,506` | `INFO` | `  Vectorizing:  96%|########################9 | 285/297 [03:27<00:16,  1.37s/it]`\n",
      "`2022-06-29 09:33:02,017` | `INFO` | `  Vectorizing:  99%|#########################8| 295/297 [03:38<00:02,  1.31s/it]`\n",
      "`2022-06-29 09:33:03,965` | `INFO` | `  Vectorizing: 100%|##########################| 297/297 [03:40<00:00,  1.35it/s]`\n",
      "\n",
      "`2022-06-29 09:33:05,807` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000002-444545_by_515.parquet`\n",
      "`2022-06-29 09:33:33,487` | `INFO` | `Files in batch:   9%|8         | 3/35 [12:57<2:17:56, 258.64s/it]`\n",
      "`2022-06-29 09:33:34,414` | `INFO` | `  Processing: 000000000003.parquet`\n",
      "`2022-06-29 09:33:34,415` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:33:34,619` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:33:46,315` | `INFO` | `  Vectorizing:   8%|##2                        | 15/184 [00:11<02:11,  1.28it/s]`\n",
      "`2022-06-29 09:33:57,277` | `INFO` | `  Vectorizing:   8%|##2                        | 15/184 [00:22<02:11,  1.28it/s]`\n",
      "`2022-06-29 09:33:58,172` | `INFO` | `  Vectorizing:  14%|###6                       | 25/184 [00:23<02:35,  1.02it/s]`\n",
      "`2022-06-29 09:34:10,597` | `INFO` | `  Vectorizing:  23%|######1                    | 42/184 [00:35<01:59,  1.19it/s]`\n",
      "`2022-06-29 09:34:27,326` | `INFO` | `  Vectorizing:  23%|######1                    | 42/184 [00:52<01:59,  1.19it/s]`\n",
      "2022-06-29 09:34:35.598204: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB (rounded to 1126664192)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/Window-0/Select\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:34:35.598768: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************************************************************************************_____\n",
      "2022-06-29 09:34:35.598836: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_op_select.cc:105 : Resource exhausted: OOM when allocating tensor with shape[1100258,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-06-29 09:34:45.600607: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB (rounded to 1126664192)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/Window-1/Select\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:34:45.601201: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************************************************************************************_____\n",
      "2022-06-29 09:34:45.601237: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_op_select.cc:105 : Resource exhausted: OOM when allocating tensor with shape[1100258,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-06-29 09:34:55.601563: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB (rounded to 1126664192)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/Window-3/Select\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:34:55.602117: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************************************************************************************_____\n",
      "2022-06-29 09:34:55.602162: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cwise_op_select.cc:105 : Resource exhausted: OOM when allocating tensor with shape[1100258,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-06-29 09:35:05.602478: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.10GiB (rounded to 2253328384)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_2/Ngram-2-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:35:05.603073: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************************************************************************************_____\n",
      "2022-06-29 09:35:05.603112: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[1100258,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-06-29 09:35:15.603415: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.15GiB (rounded to 3379992576)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_3/Ngram-3-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:35:15.603996: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *****************************************_______****************_______************************_____\n",
      "2022-06-29 09:35:15.604032: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[1100258,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:35:15,604` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[1100258,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/Window-0/Select}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "2022-06-29 09:35:25.792418: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.60GiB (rounded to 1714796544)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_2/Ngram-2-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:35:25.792989: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *******************************************************************************************_________\n",
      "2022-06-29 09:35:25.793026: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[837303,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-06-29 09:35:35.793302: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.40GiB (rounded to 2572194816)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_3/Ngram-3-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:35:35.793902: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ********************************_____*************_____************************************_________\n",
      "2022-06-29 09:35:35.793940: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[837303,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-06-29 09:35:45.794285: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.99GiB (rounded to 4286991360)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:35:45.794873: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ********************************___________*******__________*******_____*******************_________\n",
      "2022-06-29 09:35:45.794911: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[837303,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:35:45,795` | `INFO` | `  Vectorizing:  30%|########                   | 55/184 [02:11<05:07,  2.39s/it]`\n",
      "\n",
      "`2022-06-29 09:35:45,795` | `ERROR` | `Failed to vectorize comments`\n",
      "`2022-06-29 09:35:45,796` | `ERROR` | ` OOM when allocating tensor with shape[837303,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_2/Ngram-2-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "`\n",
      "`2022-06-29 09:35:45,796` | `INFO` | `*** Retrying with smaller batch size 1125***`\n",
      "`2022-06-29 09:35:46,011` | `INFO` | `Getting embeddings in batches of size: 1125`\n",
      "`2022-06-29 09:35:57,787` | `INFO` | `  Vectorizing:   8%|##                         | 19/245 [00:11<02:20,  1.61it/s]`\n",
      "`2022-06-29 09:36:13,628` | `INFO` | `  Vectorizing:  15%|####                       | 37/245 [00:27<02:40,  1.30it/s]`\n",
      "`2022-06-29 09:36:25,388` | `INFO` | `  Vectorizing:  23%|######2                    | 57/245 [00:39<02:08,  1.46it/s]`\n",
      "`2022-06-29 09:36:37,339` | `INFO` | `  Vectorizing:  23%|######2                    | 57/245 [00:51<02:08,  1.46it/s]`\n",
      "`2022-06-29 09:36:38,301` | `INFO` | `  Vectorizing:  29%|#######9                   | 72/245 [00:52<02:08,  1.35it/s]`\n",
      "2022-06-29 09:36:49.510625: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.12GiB (rounded to 2275206144)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_3/Ngram-3-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:36:49.511224: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *****************************____***********_____******************************************_________\n",
      "2022-06-29 09:36:49.511264: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[740627,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:36:57,340` | `INFO` | `  Vectorizing:  29%|#######9                   | 72/245 [01:11<02:08,  1.35it/s]`\n",
      "2022-06-29 09:36:59.511549: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.53GiB (rounded to 3792010240)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:36:59.512117: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *****************************_________******__________******____***************************_________\n",
      "2022-06-29 09:36:59.512154: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[740627,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:36:59,513` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[740627,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_3/Ngram-3-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "2022-06-29 09:37:09.737898: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.70GiB (rounded to 2904120320)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:37:09.738487: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****______*****___*********************************__________________\n",
      "2022-06-29 09:37:09.738525: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[567211,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:37:09,740` | `INFO` | `  Vectorizing:  30%|########                   | 73/245 [01:23<03:17,  1.15s/it]`\n",
      "\n",
      "`2022-06-29 09:37:09,740` | `ERROR` | `Failed to vectorize comments`\n",
      "`2022-06-29 09:37:09,740` | `ERROR` | ` OOM when allocating tensor with shape[567211,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "`\n",
      "`2022-06-29 09:37:09,740` | `INFO` | `*** Retrying with smaller batch size 750***`\n",
      "`2022-06-29 09:37:09,985` | `INFO` | `Getting embeddings in batches of size: 750`\n",
      "`2022-06-29 09:37:21,066` | `INFO` | `  Vectorizing:   7%|#7                         | 24/367 [00:11<02:38,  2.17it/s]`\n",
      "`2022-06-29 09:37:37,041` | `INFO` | `  Vectorizing:  13%|###5                       | 48/367 [00:27<03:05,  1.72it/s]`\n",
      "`2022-06-29 09:37:47,591` | `INFO` | `  Vectorizing:  13%|###5                       | 48/367 [00:37<03:05,  1.72it/s]`\n",
      "`2022-06-29 09:37:48,153` | `INFO` | `  Vectorizing:  20%|#####5                     | 75/367 [00:38<02:25,  2.00it/s]`\n",
      "`2022-06-29 09:38:05,477` | `INFO` | `  Vectorizing:  28%|#######2                  | 102/367 [00:55<02:27,  1.79it/s]`\n",
      "`2022-06-29 09:38:17,695` | `INFO` | `  Vectorizing:  28%|#######2                  | 102/367 [01:07<02:27,  1.79it/s]`\n",
      "`2022-06-29 09:38:17,905` | `INFO` | `  Vectorizing:  32%|########2                 | 117/367 [01:07<02:35,  1.60it/s]`\n",
      "`2022-06-29 09:38:29,127` | `INFO` | `  Vectorizing:  37%|#########6                | 136/367 [01:19<02:21,  1.63it/s]`\n",
      "`2022-06-29 09:38:41,840` | `INFO` | `  Vectorizing:  42%|##########9               | 155/367 [01:31<02:13,  1.59it/s]`\n",
      "`2022-06-29 09:38:54,041` | `INFO` | `  Vectorizing:  47%|############1             | 172/367 [01:44<02:07,  1.53it/s]`\n",
      "`2022-06-29 09:39:05,552` | `INFO` | `  Vectorizing:  51%|#############3            | 189/367 [01:55<01:57,  1.51it/s]`\n",
      "`2022-06-29 09:39:17,149` | `INFO` | `  Vectorizing:  56%|##############6           | 207/367 [02:07<01:44,  1.52it/s]`\n",
      "`2022-06-29 09:39:27,824` | `INFO` | `  Vectorizing:  56%|##############6           | 207/367 [02:17<01:44,  1.52it/s]`\n",
      "`2022-06-29 09:39:28,572` | `INFO` | `  Vectorizing:  61%|###############8          | 224/367 [02:18<01:34,  1.51it/s]`\n",
      "`2022-06-29 09:39:39,822` | `INFO` | `  Vectorizing:  66%|#################         | 241/367 [02:29<01:23,  1.51it/s]`\n",
      "`2022-06-29 09:39:51,217` | `INFO` | `  Vectorizing:  71%|##################3       | 259/367 [02:41<01:10,  1.53it/s]`\n",
      "`2022-06-29 09:40:02,626` | `INFO` | `  Vectorizing:  76%|###################8      | 280/367 [02:52<00:53,  1.62it/s]`\n",
      "`2022-06-29 09:40:17,951` | `INFO` | `  Vectorizing:  76%|###################8      | 280/367 [03:07<00:53,  1.62it/s]`\n",
      "`2022-06-29 09:40:18,498` | `INFO` | `  Vectorizing:  82%|#####################3    | 301/367 [03:08<00:43,  1.51it/s]`\n",
      "`2022-06-29 09:40:29,810` | `INFO` | `  Vectorizing:  86%|######################4   | 317/367 [03:19<00:33,  1.49it/s]`\n",
      "`2022-06-29 09:40:41,368` | `INFO` | `  Vectorizing:  92%|#######################8  | 337/367 [03:31<00:19,  1.56it/s]`\n",
      "`2022-06-29 09:40:54,678` | `INFO` | `  Vectorizing:  97%|#########################2| 357/367 [03:44<00:06,  1.54it/s]`\n",
      "`2022-06-29 09:41:01,141` | `INFO` | `  Vectorizing: 100%|##########################| 367/367 [03:51<00:00,  1.59it/s]`\n",
      "\n",
      "`2022-06-29 09:41:02,267` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000003-274825_by_515.parquet`\n",
      "`2022-06-29 09:41:24,608` | `INFO` | `Files in batch:  11%|#1        | 4/35 [20:48<2:56:58, 342.52s/it]`\n",
      "`2022-06-29 09:41:25,795` | `INFO` | `  Processing: 000000000004.parquet`\n",
      "`2022-06-29 09:41:25,796` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:41:26,002` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:41:37,733` | `INFO` | `  Vectorizing:   6%|#6                         | 12/193 [00:11<02:56,  1.02it/s]`\n",
      "`2022-06-29 09:41:47,994` | `INFO` | `  Vectorizing:   6%|#6                         | 12/193 [00:21<02:56,  1.02it/s]`\n",
      "`2022-06-29 09:41:49,380` | `INFO` | `  Vectorizing:  12%|###2                       | 23/193 [00:23<02:54,  1.02s/it]`\n",
      "`2022-06-29 09:42:00,738` | `INFO` | `  Vectorizing:  18%|####7                      | 34/193 [00:34<02:43,  1.03s/it]`\n",
      "`2022-06-29 09:42:11,949` | `INFO` | `  Vectorizing:  25%|######7                    | 48/193 [00:45<02:14,  1.08it/s]`\n",
      "`2022-06-29 09:42:23,993` | `INFO` | `  Vectorizing:  33%|########8                  | 63/193 [00:57<01:53,  1.14it/s]`\n",
      "`2022-06-29 09:42:38,070` | `INFO` | `  Vectorizing:  33%|########8                  | 63/193 [01:12<01:53,  1.14it/s]`\n",
      "`2022-06-29 09:42:38,668` | `INFO` | `  Vectorizing:  39%|##########6                | 76/193 [01:12<01:52,  1.04it/s]`\n",
      "`2022-06-29 09:42:50,342` | `INFO` | `  Vectorizing:  46%|############3              | 88/193 [01:24<01:41,  1.04it/s]`\n",
      "`2022-06-29 09:43:02,258` | `INFO` | `  Vectorizing:  52%|#############4            | 100/193 [01:36<01:30,  1.03it/s]`\n",
      "`2022-06-29 09:43:18,050` | `INFO` | `  Vectorizing:  58%|###############           | 112/193 [01:52<01:27,  1.08s/it]`\n",
      "`2022-06-29 09:43:28,151` | `INFO` | `  Vectorizing:  58%|###############           | 112/193 [02:02<01:27,  1.08s/it]`\n",
      "`2022-06-29 09:43:29,433` | `INFO` | `  Vectorizing:  63%|################3         | 121/193 [02:03<01:20,  1.12s/it]`\n",
      "`2022-06-29 09:43:40,913` | `INFO` | `  Vectorizing:  70%|##################1       | 135/193 [02:14<00:58,  1.02s/it]`\n",
      "`2022-06-29 09:43:52,973` | `INFO` | `  Vectorizing:  77%|####################      | 149/193 [02:26<00:42,  1.04it/s]`\n",
      "2022-06-29 09:44:05.762548: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.83GiB (rounded to 3043031040)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:44:05.763179: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************_______*****_______******___**********************************______________\n",
      "2022-06-29 09:44:05.763222: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[594342,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:44:05,764` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[594342,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:44:08,152` | `INFO` | `  Vectorizing:  77%|####################      | 149/193 [02:42<00:42,  1.04it/s]`\n",
      "2022-06-29 09:44:17.368408: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.83GiB (rounded to 3036892160)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:44:17.368983: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************_______*****_______******___**********************************______________\n",
      "2022-06-29 09:44:17.369022: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[593143,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:44:17,370` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[593143,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:44:18,802` | `INFO` | `  Vectorizing:  79%|####################6     | 153/193 [02:52<01:05,  1.63s/it]`\n",
      "`2022-06-29 09:44:30,411` | `INFO` | `  Vectorizing:  85%|######################2   | 165/193 [03:04<00:39,  1.40s/it]`\n",
      "`2022-06-29 09:44:41,805` | `INFO` | `  Vectorizing:  92%|#######################8  | 177/193 [03:15<00:20,  1.25s/it]`\n",
      "2022-06-29 09:44:51.990830: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.89GiB (rounded to 3098956800)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:44:51.991446: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************________*****_______*****____**********************************_____________\n",
      "2022-06-29 09:44:51.991487: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[605265,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:44:51,992` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[605265,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:44:58,228` | `INFO` | `  Vectorizing:  92%|#######################8  | 177/193 [03:32<00:20,  1.25s/it]`\n",
      "2022-06-29 09:45:06.310276: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.01GiB (rounded to 3231160320)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:45:06.310849: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *************************________*****________*****____************************************_________\n",
      "2022-06-29 09:45:06.310887: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[631086,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:45:06,312` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[631086,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:45:07,818` | `INFO` | `  Vectorizing:  94%|########################3 | 181/193 [03:41<00:23,  1.96s/it]`\n",
      "2022-06-29 09:45:18.041102: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.99GiB (rounded to 3211008000)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:45:18.041671: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *************************________*****________*****___************************************__________\n",
      "2022-06-29 09:45:18.041708: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[627150,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:45:18,043` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[627150,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:45:18,229` | `INFO` | `  Vectorizing:  94%|########################3 | 181/193 [03:52<00:23,  1.96s/it]`\n",
      "`2022-06-29 09:45:19,532` | `INFO` | `  Vectorizing:  94%|########################5 | 182/193 [03:53<00:26,  2.41s/it]`\n",
      "2022-06-29 09:45:29.731103: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.00GiB (rounded to 3216762880)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:45:29.731807: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *************************________*****________*****____************************************_________\n",
      "2022-06-29 09:45:29.731847: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[628274,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:45:29,732` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[628274,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:45:31,225` | `INFO` | `  Vectorizing:  95%|########################6 | 183/193 [04:05<00:29,  2.98s/it]`\n",
      "`2022-06-29 09:45:39,281` | `INFO` | `  Vectorizing: 100%|##########################| 193/193 [04:13<00:00,  1.31s/it]`\n",
      "\n",
      "`2022-06-29 09:45:40,376` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000004-288676_by_515.parquet`\n",
      "`2022-06-29 09:46:03,013` | `INFO` | `Files in batch:  14%|#4        | 5/35 [25:27<2:39:42, 319.40s/it]`\n",
      "`2022-06-29 09:46:04,215` | `INFO` | `  Processing: 000000000005.parquet`\n",
      "`2022-06-29 09:46:04,215` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:46:04,447` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:46:15,564` | `INFO` | `  Vectorizing:   6%|#4                         | 12/217 [00:11<03:09,  1.08it/s]`\n",
      "`2022-06-29 09:46:27,494` | `INFO` | `  Vectorizing:  12%|###1                       | 25/217 [00:23<02:56,  1.09it/s]`\n",
      "`2022-06-29 09:46:38,331` | `INFO` | `  Vectorizing:  12%|###1                       | 25/217 [00:33<02:56,  1.09it/s]`\n",
      "`2022-06-29 09:46:38,855` | `INFO` | `  Vectorizing:  17%|####4                      | 36/217 [00:34<02:55,  1.03it/s]`\n",
      "`2022-06-29 09:46:50,882` | `INFO` | `  Vectorizing:  23%|######2                    | 50/217 [00:46<02:33,  1.09it/s]`\n",
      "`2022-06-29 09:47:05,512` | `INFO` | `  Vectorizing:  29%|#######8                   | 63/217 [01:01<02:33,  1.00it/s]`\n",
      "`2022-06-29 09:47:17,258` | `INFO` | `  Vectorizing:  35%|#########5                 | 77/217 [01:12<02:11,  1.06it/s]`\n",
      "`2022-06-29 09:47:28,459` | `INFO` | `  Vectorizing:  35%|#########5                 | 77/217 [01:24<02:11,  1.06it/s]`\n",
      "`2022-06-29 09:47:28,614` | `INFO` | `  Vectorizing:  42%|###########4               | 92/217 [01:24<01:49,  1.14it/s]`\n",
      "`2022-06-29 09:47:44,192` | `INFO` | `  Vectorizing:  49%|############8             | 107/217 [01:39<01:42,  1.08it/s]`\n",
      "`2022-06-29 09:47:55,442` | `INFO` | `  Vectorizing:  55%|##############2           | 119/217 [01:50<01:31,  1.07it/s]`\n",
      "`2022-06-29 09:48:08,460` | `INFO` | `  Vectorizing:  55%|##############2           | 119/217 [02:04<01:31,  1.07it/s]`\n",
      "`2022-06-29 09:48:08,809` | `INFO` | `  Vectorizing:  60%|###############6          | 131/217 [02:04<01:24,  1.02it/s]`\n",
      "`2022-06-29 09:48:20,995` | `INFO` | `  Vectorizing:  65%|################8         | 141/217 [02:16<01:19,  1.04s/it]`\n",
      "`2022-06-29 09:48:32,203` | `INFO` | `  Vectorizing:  71%|##################4       | 154/217 [02:27<01:02,  1.01it/s]`\n",
      "`2022-06-29 09:48:43,416` | `INFO` | `  Vectorizing:  77%|####################      | 167/217 [02:38<00:47,  1.06it/s]`\n",
      "`2022-06-29 09:48:55,988` | `INFO` | `  Vectorizing:  83%|#####################5    | 180/217 [02:51<00:35,  1.05it/s]`\n",
      "`2022-06-29 09:49:07,297` | `INFO` | `  Vectorizing:  89%|#######################1  | 193/217 [03:02<00:22,  1.08it/s]`\n",
      "`2022-06-29 09:49:18,514` | `INFO` | `  Vectorizing:  89%|#######################1  | 193/217 [03:14<00:22,  1.08it/s]`\n",
      "`2022-06-29 09:49:18,813` | `INFO` | `  Vectorizing:  94%|########################4 | 204/217 [03:14<00:12,  1.04it/s]`\n",
      "`2022-06-29 09:49:32,443` | `INFO` | `  Vectorizing:  99%|#########################7| 215/217 [03:27<00:02,  1.04s/it]`\n",
      "`2022-06-29 09:49:34,018` | `INFO` | `  Vectorizing: 100%|##########################| 217/217 [03:29<00:00,  1.04it/s]`\n",
      "\n",
      "`2022-06-29 09:49:35,257` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000005-324438_by_515.parquet`\n",
      "`2022-06-29 09:49:59,790` | `INFO` | `Files in batch:  17%|#7        | 6/35 [29:24<2:20:47, 291.31s/it]`\n",
      "`2022-06-29 09:50:00,853` | `INFO` | `  Processing: 000000000006.parquet`\n",
      "`2022-06-29 09:50:00,853` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:50:01,095` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:50:13,018` | `INFO` | `  Vectorizing:   4%|#                          | 10/252 [00:11<04:48,  1.19s/it]`\n",
      "`2022-06-29 09:50:24,133` | `INFO` | `  Vectorizing:  11%|##8                        | 27/252 [00:23<03:02,  1.23it/s]`\n",
      "`2022-06-29 09:50:37,301` | `INFO` | `  Vectorizing:  17%|####7                      | 44/252 [00:36<02:44,  1.26it/s]`\n",
      "`2022-06-29 09:50:48,560` | `INFO` | `  Vectorizing:  17%|####7                      | 44/252 [00:47<02:44,  1.26it/s]`\n",
      "`2022-06-29 09:50:49,115` | `INFO` | `  Vectorizing:  23%|######1                    | 57/252 [00:48<02:42,  1.20it/s]`\n",
      "`2022-06-29 09:51:00,628` | `INFO` | `  Vectorizing:  28%|#######5                   | 70/252 [00:59<02:34,  1.17it/s]`\n",
      "`2022-06-29 09:51:14,612` | `INFO` | `  Vectorizing:  33%|########8                  | 83/252 [01:13<02:36,  1.08it/s]`\n",
      "`2022-06-29 09:51:25,744` | `INFO` | `  Vectorizing:  38%|##########1                | 95/252 [01:24<02:25,  1.08it/s]`\n",
      "`2022-06-29 09:51:38,565` | `INFO` | `  Vectorizing:  38%|##########1                | 95/252 [01:37<02:25,  1.08it/s]`\n",
      "`2022-06-29 09:51:38,839` | `INFO` | `  Vectorizing:  42%|###########               | 107/252 [01:37<02:21,  1.03it/s]`\n",
      "`2022-06-29 09:51:51,418` | `INFO` | `  Vectorizing:  47%|############1             | 118/252 [01:50<02:17,  1.02s/it]`\n",
      "`2022-06-29 09:52:03,169` | `INFO` | `  Vectorizing:  53%|#############7            | 133/252 [02:02<01:51,  1.07it/s]`\n",
      "`2022-06-29 09:52:15,321` | `INFO` | `  Vectorizing:  59%|###############3          | 149/252 [02:14<01:30,  1.14it/s]`\n",
      "`2022-06-29 09:52:26,616` | `INFO` | `  Vectorizing:  65%|################9         | 164/252 [02:25<01:13,  1.20it/s]`\n",
      "`2022-06-29 09:52:38,728` | `INFO` | `  Vectorizing:  65%|################9         | 164/252 [02:37<01:13,  1.20it/s]`\n",
      "`2022-06-29 09:52:39,072` | `INFO` | `  Vectorizing:  70%|##################1       | 176/252 [02:37<01:07,  1.12it/s]`\n",
      "`2022-06-29 09:52:50,828` | `INFO` | `  Vectorizing:  75%|###################6      | 190/252 [02:49<00:54,  1.14it/s]`\n",
      "`2022-06-29 09:53:08,730` | `INFO` | `  Vectorizing:  75%|###################6      | 190/252 [03:07<00:54,  1.14it/s]`\n",
      "2022-06-29 09:53:10.814254: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.69GiB (rounded to 2884520960)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:53:10.814829: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************______*****_______*****___*********************************__________________\n",
      "2022-06-29 09:53:10.814867: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[563383,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:53:10,816` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[563383,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:53:12,166` | `INFO` | `  Vectorizing:  79%|####################6     | 200/252 [03:11<01:01,  1.18s/it]`\n",
      "2022-06-29 09:53:22.359961: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.69GiB (rounded to 2887429120)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:53:22.360522: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************______*****_______*****___*********************************__________________\n",
      "2022-06-29 09:53:22.360624: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[563951,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:53:22,361` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[563951,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:53:28,588` | `INFO` | `  Vectorizing:  82%|#####################2    | 206/252 [03:27<01:06,  1.44s/it]`\n",
      "`2022-06-29 09:53:38,731` | `INFO` | `  Vectorizing:  82%|#####################2    | 206/252 [03:37<01:06,  1.44s/it]`\n",
      "`2022-06-29 09:53:39,699` | `INFO` | `  Vectorizing:  87%|######################4   | 218/252 [03:38<00:43,  1.27s/it]`\n",
      "`2022-06-29 09:53:51,297` | `INFO` | `  Vectorizing:  93%|########################1 | 234/252 [03:50<00:19,  1.06s/it]`\n",
      "2022-06-29 09:54:06.881296: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.68GiB (rounded to 2875914240)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:54:06.881909: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************______*****_______*****___********************************___________________\n",
      "2022-06-29 09:54:06.881945: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[561702,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:54:06,883` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[561702,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:54:08,837` | `INFO` | `  Vectorizing:  93%|########################1 | 234/252 [04:07<00:19,  1.06s/it]`\n",
      "`2022-06-29 09:54:09,143` | `INFO` | `  Vectorizing:  95%|########################7 | 240/252 [04:08<00:16,  1.39s/it]`\n",
      "`2022-06-29 09:54:18,434` | `INFO` | `  Vectorizing: 100%|##########################| 252/252 [04:17<00:00,  1.02s/it]`\n",
      "\n",
      "`2022-06-29 09:54:19,940` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000006-376924_by_515.parquet`\n",
      "`2022-06-29 09:54:48,844` | `INFO` | `Files in batch:  20%|##        | 7/35 [34:13<2:15:36, 290.57s/it]`\n",
      "`2022-06-29 09:54:49,785` | `INFO` | `  Processing: 000000000007.parquet`\n",
      "`2022-06-29 09:54:49,786` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:54:49,984` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 09:55:01,173` | `INFO` | `  Vectorizing:   7%|#7                         | 14/213 [00:11<02:39,  1.25it/s]`\n",
      "`2022-06-29 09:55:16,890` | `INFO` | `  Vectorizing:  13%|###5                       | 28/213 [00:26<03:03,  1.01it/s]`\n",
      "`2022-06-29 09:55:28,508` | `INFO` | `  Vectorizing:  20%|#####3                     | 42/213 [00:38<02:36,  1.09it/s]`\n",
      "`2022-06-29 09:55:38,918` | `INFO` | `  Vectorizing:  20%|#####3                     | 42/213 [00:48<02:36,  1.09it/s]`\n",
      "`2022-06-29 09:55:40,066` | `INFO` | `  Vectorizing:  25%|######8                    | 54/213 [00:50<02:28,  1.07it/s]`\n",
      "`2022-06-29 09:55:52,882` | `INFO` | `  Vectorizing:  31%|########3                  | 66/213 [01:02<02:23,  1.02it/s]`\n",
      "`2022-06-29 09:56:05,337` | `INFO` | `  Vectorizing:  36%|#########7                 | 77/213 [01:15<02:19,  1.03s/it]`\n",
      "`2022-06-29 09:56:17,037` | `INFO` | `  Vectorizing:  43%|###########5               | 91/213 [01:27<01:56,  1.04it/s]`\n",
      "`2022-06-29 09:56:28,063` | `INFO` | `  Vectorizing:  50%|############9             | 106/213 [01:38<01:34,  1.14it/s]`\n",
      "`2022-06-29 09:56:39,096` | `INFO` | `  Vectorizing:  50%|############9             | 106/213 [01:49<01:34,  1.14it/s]`\n",
      "`2022-06-29 09:56:39,935` | `INFO` | `  Vectorizing:  55%|##############2           | 117/213 [01:49<01:29,  1.07it/s]`\n",
      "`2022-06-29 09:56:51,118` | `INFO` | `  Vectorizing:  61%|###############8          | 130/213 [02:01<01:15,  1.10it/s]`\n",
      "2022-06-29 09:57:05.915670: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.75GiB (rounded to 2958376960)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:57:05.916248: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****_______*****___**********************************________________\n",
      "2022-06-29 09:57:05.916289: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[577808,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:57:05,917` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[577808,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:57:09,101` | `INFO` | `  Vectorizing:  61%|###############8          | 130/213 [02:19<01:15,  1.10it/s]`\n",
      "`2022-06-29 09:57:09,405` | `INFO` | `  Vectorizing:  64%|################7         | 137/213 [02:19<01:34,  1.24s/it]`\n",
      "`2022-06-29 09:57:21,697` | `INFO` | `  Vectorizing:  70%|##################3       | 150/213 [02:31<01:11,  1.14s/it]`\n",
      "2022-06-29 09:57:34.751110: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.23GiB (rounded to 3470771200)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:57:34.751740: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***************************________******________******____***************************************__\n",
      "2022-06-29 09:57:34.751775: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[677885,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:57:34,753` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[677885,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:57:39,103` | `INFO` | `  Vectorizing:  70%|##################3       | 150/213 [02:49<01:11,  1.14s/it]`\n",
      "2022-06-29 09:57:46.502881: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.26GiB (rounded to 3505889280)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:57:46.503507: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***************************________******_________******___****************************************_\n",
      "2022-06-29 09:57:46.503544: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[684744,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:57:46,504` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[684744,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:57:48,036` | `INFO` | `  Vectorizing:  72%|##################7       | 154/213 [02:58<01:49,  1.85s/it]`\n",
      "`2022-06-29 09:57:59,104` | `INFO` | `  Vectorizing:  72%|##################7       | 154/213 [03:09<01:49,  1.85s/it]`\n",
      "`2022-06-29 09:57:59,842` | `INFO` | `  Vectorizing:  77%|####################      | 164/213 [03:09<01:20,  1.64s/it]`\n",
      "`2022-06-29 09:58:11,597` | `INFO` | `  Vectorizing:  85%|#####################9    | 180/213 [03:21<00:41,  1.26s/it]`\n",
      "`2022-06-29 09:58:23,286` | `INFO` | `  Vectorizing:  92%|#######################8  | 195/213 [03:33<00:19,  1.09s/it]`\n",
      "`2022-06-29 09:58:37,640` | `INFO` | `  Vectorizing:  99%|#########################6| 210/213 [03:47<00:03,  1.04s/it]`\n",
      "`2022-06-29 09:58:39,442` | `INFO` | `  Vectorizing: 100%|##########################| 213/213 [03:49<00:00,  1.08s/it]`\n",
      "\n",
      "`2022-06-29 09:58:40,645` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000007-318149_by_515.parquet`\n",
      "`2022-06-29 09:59:05,390` | `INFO` | `Files in batch:  23%|##2       | 8/35 [38:29<2:05:52, 279.74s/it]`\n",
      "`2022-06-29 09:59:06,445` | `INFO` | `  Processing: 000000000008.parquet`\n",
      "`2022-06-29 09:59:06,445` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 09:59:06,651` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "2022-06-29 09:59:17.881714: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.67GiB (rounded to 2870062080)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 09:59:17.882289: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************______*****_______*****___********************************___________________\n",
      "2022-06-29 09:59:17.882326: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[560559,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 09:59:17,883` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[560559,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 09:59:19,247` | `INFO` | `  Vectorizing:   1%|2                           | 2/237 [00:12<24:39,  6.30s/it]`\n",
      "`2022-06-29 09:59:30,496` | `INFO` | `  Vectorizing:   8%|##                         | 18/237 [00:23<04:12,  1.15s/it]`\n",
      "`2022-06-29 09:59:46,913` | `INFO` | `  Vectorizing:  14%|###8                       | 34/237 [00:40<03:39,  1.08s/it]`\n",
      "`2022-06-29 09:59:59,303` | `INFO` | `  Vectorizing:  14%|###8                       | 34/237 [00:52<03:39,  1.08s/it]`\n",
      "`2022-06-29 09:59:59,303` | `INFO` | `  Vectorizing:  19%|#####1                     | 45/237 [00:52<03:30,  1.10s/it]`\n",
      "`2022-06-29 10:00:09,349` | `INFO` | `  Vectorizing:  19%|#####1                     | 45/237 [01:02<03:30,  1.10s/it]`\n",
      "`2022-06-29 10:00:10,313` | `INFO` | `  Vectorizing:  24%|######3                    | 56/237 [01:03<03:12,  1.06s/it]`\n",
      "`2022-06-29 10:00:21,724` | `INFO` | `  Vectorizing:  30%|#######9                   | 70/237 [01:15<02:41,  1.03it/s]`\n",
      "`2022-06-29 10:00:33,331` | `INFO` | `  Vectorizing:  35%|#########5                 | 84/237 [01:26<02:20,  1.09it/s]`\n",
      "`2022-06-29 10:00:46,713` | `INFO` | `  Vectorizing:  41%|###########1               | 98/237 [01:40<02:09,  1.07it/s]`\n",
      "`2022-06-29 10:00:58,803` | `INFO` | `  Vectorizing:  46%|############              | 110/237 [01:52<02:01,  1.05it/s]`\n",
      "`2022-06-29 10:01:09,389` | `INFO` | `  Vectorizing:  46%|############              | 110/237 [02:02<02:01,  1.05it/s]`\n",
      "`2022-06-29 10:01:09,874` | `INFO` | `  Vectorizing:  51%|#############2            | 121/237 [02:03<01:52,  1.03it/s]`\n",
      "`2022-06-29 10:01:21,465` | `INFO` | `  Vectorizing:  56%|##############5           | 133/237 [02:14<01:40,  1.03it/s]`\n",
      "`2022-06-29 10:01:33,347` | `INFO` | `  Vectorizing:  61%|###############9          | 145/237 [02:26<01:29,  1.03it/s]`\n",
      "`2022-06-29 10:01:44,680` | `INFO` | `  Vectorizing:  66%|#################2        | 157/237 [02:38<01:17,  1.04it/s]`\n",
      "`2022-06-29 10:01:56,209` | `INFO` | `  Vectorizing:  71%|##################5       | 169/237 [02:49<01:05,  1.04it/s]`\n",
      "`2022-06-29 10:02:07,740` | `INFO` | `  Vectorizing:  77%|###################9      | 182/237 [03:01<00:51,  1.06it/s]`\n",
      "`2022-06-29 10:02:19,590` | `INFO` | `  Vectorizing:  77%|###################9      | 182/237 [03:12<00:51,  1.06it/s]`\n",
      "`2022-06-29 10:02:20,798` | `INFO` | `  Vectorizing:  82%|#####################2    | 194/237 [03:14<00:42,  1.02it/s]`\n",
      "`2022-06-29 10:02:34,579` | `INFO` | `  Vectorizing:  86%|######################4   | 205/237 [03:27<00:33,  1.06s/it]`\n",
      "`2022-06-29 10:02:45,620` | `INFO` | `  Vectorizing:  92%|#######################8  | 217/237 [03:38<00:20,  1.02s/it]`\n",
      "`2022-06-29 10:02:56,885` | `INFO` | `  Vectorizing:  97%|#########################1| 229/237 [03:50<00:07,  1.01it/s]`\n",
      "`2022-06-29 10:03:03,044` | `INFO` | `  Vectorizing: 100%|##########################| 237/237 [03:56<00:00,  1.00it/s]`\n",
      "\n",
      "`2022-06-29 10:03:04,396` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000008-355281_by_515.parquet`\n",
      "`2022-06-29 10:03:29,138` | `INFO` | `Files in batch:  26%|##5       | 9/35 [42:53<1:59:03, 274.74s/it]`\n",
      "`2022-06-29 10:03:30,067` | `INFO` | `  Processing: 000000000009.parquet`\n",
      "`2022-06-29 10:03:30,068` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 10:03:30,270` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 10:03:42,013` | `INFO` | `  Vectorizing:   5%|#2                         | 12/250 [00:11<03:52,  1.02it/s]`\n",
      "`2022-06-29 10:03:53,412` | `INFO` | `  Vectorizing:  10%|##7                        | 25/250 [00:23<03:26,  1.09it/s]`\n",
      "`2022-06-29 10:04:06,053` | `INFO` | `  Vectorizing:  15%|####1                      | 38/250 [00:35<03:19,  1.06it/s]`\n",
      "`2022-06-29 10:04:17,481` | `INFO` | `  Vectorizing:  21%|#####6                     | 52/250 [00:47<02:56,  1.12it/s]`\n",
      "`2022-06-29 10:04:28,918` | `INFO` | `  Vectorizing:  26%|#######1                   | 66/250 [00:58<02:38,  1.16it/s]`\n",
      "`2022-06-29 10:04:40,058` | `INFO` | `  Vectorizing:  26%|#######1                   | 66/250 [01:09<02:38,  1.16it/s]`\n",
      "`2022-06-29 10:04:40,663` | `INFO` | `  Vectorizing:  32%|########6                  | 80/250 [01:10<02:25,  1.17it/s]`\n",
      "`2022-06-29 10:04:53,077` | `INFO` | `  Vectorizing:  38%|##########1                | 94/250 [01:22<02:14,  1.16it/s]`\n",
      "`2022-06-29 10:05:05,607` | `INFO` | `  Vectorizing:  43%|###########1              | 107/250 [01:35<02:08,  1.12it/s]`\n",
      "`2022-06-29 10:05:16,643` | `INFO` | `  Vectorizing:  48%|############4             | 120/250 [01:46<01:54,  1.13it/s]`\n",
      "`2022-06-29 10:05:28,886` | `INFO` | `  Vectorizing:  53%|#############8            | 133/250 [01:58<01:45,  1.11it/s]`\n",
      "`2022-06-29 10:05:40,093` | `INFO` | `  Vectorizing:  53%|#############8            | 133/250 [02:09<01:45,  1.11it/s]`\n",
      "`2022-06-29 10:05:40,094` | `INFO` | `  Vectorizing:  59%|###############3          | 148/250 [02:09<01:26,  1.18it/s]`\n",
      "`2022-06-29 10:05:50,094` | `INFO` | `  Vectorizing:  59%|###############3          | 148/250 [02:19<01:26,  1.18it/s]`\n",
      "`2022-06-29 10:05:51,146` | `INFO` | `  Vectorizing:  65%|################8         | 162/250 [02:20<01:13,  1.20it/s]`\n",
      "`2022-06-29 10:06:03,126` | `INFO` | `  Vectorizing:  70%|##################3       | 176/250 [02:32<01:02,  1.19it/s]`\n",
      "`2022-06-29 10:06:14,205` | `INFO` | `  Vectorizing:  76%|###################6      | 189/250 [02:43<00:51,  1.19it/s]`\n",
      "`2022-06-29 10:06:26,493` | `INFO` | `  Vectorizing:  81%|#####################     | 202/250 [02:56<00:41,  1.15it/s]`\n",
      "`2022-06-29 10:06:37,802` | `INFO` | `  Vectorizing:  87%|######################5   | 217/250 [03:07<00:27,  1.20it/s]`\n",
      "`2022-06-29 10:06:50,231` | `INFO` | `  Vectorizing:  87%|######################5   | 217/250 [03:19<00:27,  1.20it/s]`\n",
      "`2022-06-29 10:06:50,353` | `INFO` | `  Vectorizing:  92%|########################  | 231/250 [03:20<00:16,  1.17it/s]`\n",
      "`2022-06-29 10:07:01,757` | `INFO` | `  Vectorizing:  98%|#########################3| 244/250 [03:31<00:05,  1.16it/s]`\n",
      "`2022-06-29 10:07:07,180` | `INFO` | `  Vectorizing: 100%|##########################| 250/250 [03:36<00:00,  1.15it/s]`\n",
      "\n",
      "`2022-06-29 10:07:08,615` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000009-374258_by_515.parquet`\n",
      "`2022-06-29 10:07:33,340` | `INFO` | `Files in batch:  29%|##5      | 10/35 [46:57<1:50:32, 265.31s/it]`\n",
      "`2022-06-29 10:07:34,234` | `INFO` | `  Processing: 000000000010.parquet`\n",
      "`2022-06-29 10:07:34,235` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 10:07:34,433` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 10:07:45,999` | `INFO` | `  Vectorizing:   7%|#7                         | 14/215 [00:11<02:46,  1.21it/s]`\n",
      "`2022-06-29 10:08:00,331` | `INFO` | `  Vectorizing:   7%|#7                         | 14/215 [00:25<02:46,  1.21it/s]`\n",
      "2022-06-29 10:08:01.686145: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.74GiB (rounded to 2947466240)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 10:08:01.686753: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****_______*****___*********************************_________________\n",
      "2022-06-29 10:08:01.686796: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[575677,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 10:08:01,688` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[575677,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 10:08:03,187` | `INFO` | `  Vectorizing:   9%|##3                        | 19/215 [00:28<05:34,  1.71s/it]`\n",
      "`2022-06-29 10:08:15,097` | `INFO` | `  Vectorizing:  15%|####                       | 32/215 [00:40<03:51,  1.27s/it]`\n",
      "`2022-06-29 10:08:27,347` | `INFO` | `  Vectorizing:  21%|#####6                     | 45/215 [00:52<03:10,  1.12s/it]`\n",
      "`2022-06-29 10:08:38,888` | `INFO` | `  Vectorizing:  27%|#######4                   | 59/215 [01:04<02:36,  1.00s/it]`\n",
      "`2022-06-29 10:08:50,348` | `INFO` | `  Vectorizing:  27%|#######4                   | 59/215 [01:15<02:36,  1.00s/it]`\n",
      "`2022-06-29 10:08:50,632` | `INFO` | `  Vectorizing:  33%|########9                  | 71/215 [01:16<02:23,  1.01it/s]`\n",
      "`2022-06-29 10:09:02,155` | `INFO` | `  Vectorizing:  41%|###########                | 88/215 [01:27<01:50,  1.15it/s]`\n",
      "`2022-06-29 10:09:14,841` | `INFO` | `  Vectorizing:  49%|############6             | 105/215 [01:40<01:30,  1.21it/s]`\n",
      "`2022-06-29 10:09:26,488` | `INFO` | `  Vectorizing:  56%|##############6           | 121/215 [01:52<01:14,  1.26it/s]`\n",
      "`2022-06-29 10:09:39,336` | `INFO` | `  Vectorizing:  64%|################5         | 137/215 [02:04<01:02,  1.26it/s]`\n",
      "`2022-06-29 10:09:50,430` | `INFO` | `  Vectorizing:  64%|################5         | 137/215 [02:15<01:02,  1.26it/s]`\n",
      "`2022-06-29 10:09:50,618` | `INFO` | `  Vectorizing:  70%|##################1       | 150/215 [02:16<00:52,  1.23it/s]`\n",
      "`2022-06-29 10:10:03,047` | `INFO` | `  Vectorizing:  76%|###################7      | 163/215 [02:28<00:44,  1.17it/s]`\n",
      "`2022-06-29 10:10:15,176` | `INFO` | `  Vectorizing:  82%|#####################2    | 176/215 [02:40<00:34,  1.14it/s]`\n",
      "`2022-06-29 10:10:27,462` | `INFO` | `  Vectorizing:  87%|######################7   | 188/215 [02:53<00:24,  1.09it/s]`\n",
      "`2022-06-29 10:10:40,466` | `INFO` | `  Vectorizing:  87%|######################7   | 188/215 [03:06<00:24,  1.09it/s]`\n",
      "2022-06-29 10:10:46.674650: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.98GiB (rounded to 3199810560)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 10:10:46.675270: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *************************________*****________*****___************************************__________\n",
      "2022-06-29 10:10:46.675309: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[624963,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 10:10:46,676` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[624963,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 10:10:48,153` | `INFO` | `  Vectorizing:  92%|#######################9  | 198/215 [03:13<00:20,  1.20s/it]`\n",
      "2022-06-29 10:10:58.376782: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.93GiB (rounded to 3148800000)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 10:10:58.377379: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ************************________*****________*****___************************************___________\n",
      "2022-06-29 10:10:58.377416: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[615000,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 10:10:58,378` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[615000,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 10:11:00,467` | `INFO` | `  Vectorizing:  92%|#######################9  | 198/215 [03:26<00:20,  1.20s/it]`\n",
      "`2022-06-29 10:11:00,813` | `INFO` | `  Vectorizing:  93%|########################1 | 200/215 [03:26<00:23,  1.54s/it]`\n",
      "`2022-06-29 10:11:12,734` | `INFO` | `  Vectorizing: 100%|#########################8| 214/215 [03:38<00:01,  1.26s/it]`\n",
      "`2022-06-29 10:11:13,581` | `INFO` | `  Vectorizing: 100%|##########################| 215/215 [03:39<00:00,  1.02s/it]`\n",
      "\n",
      "`2022-06-29 10:11:14,851` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925/000000000010-322145_by_515.parquet`\n",
      "`2022-06-29 10:11:41,358` | `INFO` | `Files in batch:  31%|##8      | 11/35 [51:05<1:44:00, 260.02s/it]`\n",
      "`2022-06-29 10:11:42,234` | `INFO` | `  Processing: 000000000011.parquet`\n",
      "`2022-06-29 10:11:42,234` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-06-29 10:11:42,451` | `INFO` | `Getting embeddings in batches of size: 1500`\n",
      "`2022-06-29 10:11:53,667` | `INFO` | `  Vectorizing:   6%|#6                         | 14/224 [00:11<02:48,  1.25it/s]`\n",
      "`2022-06-29 10:12:05,676` | `INFO` | `  Vectorizing:  12%|###3                       | 28/224 [00:23<02:43,  1.20it/s]`\n",
      "`2022-06-29 10:12:17,812` | `INFO` | `  Vectorizing:  18%|####9                      | 41/224 [00:35<02:40,  1.14it/s]`\n",
      "`2022-06-29 10:12:29,376` | `INFO` | `  Vectorizing:  24%|######5                    | 54/224 [00:46<02:30,  1.13it/s]`\n",
      "`2022-06-29 10:12:40,683` | `INFO` | `  Vectorizing:  24%|######5                    | 54/224 [00:58<02:30,  1.13it/s]`\n",
      "`2022-06-29 10:12:41,298` | `INFO` | `  Vectorizing:  30%|########                   | 67/224 [00:58<02:20,  1.12it/s]`\n",
      "`2022-06-29 10:12:52,453` | `INFO` | `  Vectorizing:  35%|#########5                 | 79/224 [01:10<02:11,  1.10it/s]`\n",
      "`2022-06-29 10:13:03,890` | `INFO` | `  Vectorizing:  42%|###########2               | 93/224 [01:21<01:54,  1.14it/s]`\n",
      "`2022-06-29 10:13:16,384` | `INFO` | `  Vectorizing:  48%|############4             | 107/224 [01:33<01:43,  1.14it/s]`\n",
      "`2022-06-29 10:13:30,803` | `INFO` | `  Vectorizing:  48%|############4             | 107/224 [01:48<01:43,  1.14it/s]`\n",
      "2022-06-29 10:13:38.862630: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.70GiB (rounded to 2899184640)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 10:13:38.863216: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****______*****___*********************************__________________\n",
      "2022-06-29 10:13:38.863255: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[566247,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 10:13:38,864` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[566247,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 10:13:40,272` | `INFO` | `  Vectorizing:  54%|#############9            | 120/224 [01:57<02:01,  1.17s/it]`\n",
      "2022-06-29 10:13:50.460758: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.70GiB (rounded to 2902983680)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-06-29 10:13:50.461321: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****______*****___*********************************__________________\n",
      "2022-06-29 10:13:50.461357: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[566989,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-06-29 10:13:50,462` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[566989,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-06-29 10:13:50,804` | `INFO` | `  Vectorizing:  54%|#############9            | 120/224 [02:08<02:01,  1.17s/it]`\n",
      "`2022-06-29 10:13:51,885` | `INFO` | `  Vectorizing:  54%|##############            | 121/224 [02:09<02:35,  1.51s/it]`\n",
      "`2022-06-29 10:14:03,164` | `INFO` | `  Vectorizing:  58%|###############2          | 131/224 [02:20<02:09,  1.39s/it]`\n",
      "`2022-06-29 10:14:14,260` | `INFO` | `  Vectorizing:  64%|################5         | 143/224 [02:31<01:39,  1.23s/it]`\n",
      "`2022-06-29 10:14:25,950` | `INFO` | `  Vectorizing:  69%|#################9        | 155/224 [02:43<01:18,  1.14s/it]`\n",
      "`2022-06-29 10:14:37,240` | `INFO` | `  Vectorizing:  75%|###################3      | 167/224 [02:54<01:01,  1.08s/it]`\n"
     ]
    }
   ],
   "source": [
    "# run on full data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
