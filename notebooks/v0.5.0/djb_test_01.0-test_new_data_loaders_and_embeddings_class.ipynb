{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f3b647",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to test the new data-loader (SQL) and embeddings class that we'll be using in kubeflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb1b5b",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f164278",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83f9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.20.0\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# import mlflow\n",
    "\n",
    "import subclu\n",
    "# from subclu.utils import set_working_directory\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "# new modules to test:\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_sql import (\n",
    "    convert_iter_to_sql_str,\n",
    "    LoadSubredditsSQL\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fbeefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:19:26 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d7081",
   "metadata": {},
   "source": [
    "# Test SQL query speeds\n",
    "\n",
    "Which one is faster? Since we'll be loading lots of text data speed will matter.\n",
    "Turns out the default library wins, so no need to add `pandas-gbq` to the full requirements file.\n",
    "\n",
    "```\n",
    "# 200k subreddits\n",
    "\n",
    "## pandas-gbq\n",
    "15.7 s ± 555 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "\n",
    "## bigquery client\n",
    "8.51 s ± 1.34 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "# 100k subreddits \n",
    "(change order in case one of them is getting the benefit of getting cached)\n",
    "\n",
    "## BQ\n",
    "6.75 s ± 265 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "## pandas-gbq\n",
    "9.43 s ± 369 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98671645",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sql_200 = \"\"\"\n",
    "SELECT\n",
    "            \n",
    "        subreddit_id\n",
    "        , name\n",
    "        , title\n",
    "        , description\n",
    "\n",
    "FROM data-prod-165221.ds_v2_postgres_tables.subreddit_lookup\n",
    "WHERE 1=1\n",
    "    AND dt = (CURRENT_DATE() - 2)  -- subreddit_lookup\n",
    "    -- Exclude user-profiles + spam & sketchy subs\n",
    "    AND COALESCE(verdict, 'f') <> 'admin_removed'\n",
    "    AND COALESCE(is_spam, FALSE) = FALSE\n",
    "    AND COALESCE(is_deleted, FALSE) = FALSE\n",
    "    AND deleted IS NULL\n",
    "    AND type IN ('public', 'private', 'restricted')\n",
    "    AND NOT REGEXP_CONTAINS(LOWER(name), r'^u_.*')\n",
    "LIMIT 200000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7a68f",
   "metadata": {},
   "source": [
    "### Pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f148a5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:55:12 | INFO | \"Total time taken 15.97 s.\n",
      "Finished at 2022-03-30 23:55:12.\"\n",
      "23:55:28 | INFO | \"Total time taken 16.13 s.\n",
      "Finished at 2022-03-30 23:55:28.\"\n",
      "23:55:44 | INFO | \"Total time taken 15.84 s.\n",
      "Finished at 2022-03-30 23:55:44.\"\n",
      "23:55:59 | INFO | \"Total time taken 15.51 s.\n",
      "Finished at 2022-03-30 23:55:59.\"\n",
      "23:56:15 | INFO | \"Total time taken 15.42 s.\n",
      "Finished at 2022-03-30 23:56:15.\"\n",
      "23:56:29 | INFO | \"Total time taken 14.91 s.\n",
      "Finished at 2022-03-30 23:56:29.\"\n",
      "23:56:46 | INFO | \"Total time taken 16.8 s.\n",
      "Finished at 2022-03-30 23:56:46.\"\n",
      "23:57:02 | INFO | \"Total time taken 15.6 s.\n",
      "Finished at 2022-03-30 23:57:02.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.7 s ± 555 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_pd = pd.read_gbq(\n",
    "    query=test_sql_200,\n",
    "    progress_bar_type=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad4b60",
   "metadata": {},
   "source": [
    "### GCP's client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7341cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.51 s ± 1.34 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bigquery_client = bigquery.Client()\n",
    "df_gcp = bigquery_client.query(test_sql_200).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20cccf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sql_100 = \"\"\"\n",
    "SELECT\n",
    "            \n",
    "        subreddit_id\n",
    "        , name\n",
    "        , title\n",
    "        , description\n",
    "\n",
    "FROM data-prod-165221.ds_v2_postgres_tables.subreddit_lookup\n",
    "WHERE 1=1\n",
    "    AND dt = (CURRENT_DATE() - 2)  -- subreddit_lookup\n",
    "    -- Exclude user-profiles + spam & sketchy subs\n",
    "    AND COALESCE(verdict, 'f') <> 'admin_removed'\n",
    "    AND COALESCE(is_spam, FALSE) = FALSE\n",
    "    AND COALESCE(is_deleted, FALSE) = FALSE\n",
    "    AND deleted IS NULL\n",
    "    AND type IN ('public', 'private', 'restricted')\n",
    "    AND NOT REGEXP_CONTAINS(LOWER(name), r'^u_.*')\n",
    "LIMIT 100000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549485f",
   "metadata": {},
   "source": [
    "### GCP's client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab094b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.75 s ± 265 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bigquery_client = bigquery.Client()\n",
    "df_gcp = bigquery_client.query(test_sql_100).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d060e58",
   "metadata": {},
   "source": [
    "### Pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0981070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:01:07 | INFO | \"Total time taken 9.53 s.\n",
      "Finished at 2022-03-31 00:01:07.\"\n",
      "00:01:16 | INFO | \"Total time taken 9.33 s.\n",
      "Finished at 2022-03-31 00:01:16.\"\n",
      "00:01:26 | INFO | \"Total time taken 9.63 s.\n",
      "Finished at 2022-03-31 00:01:26.\"\n",
      "00:01:35 | INFO | \"Total time taken 9.67 s.\n",
      "Finished at 2022-03-31 00:01:35.\"\n",
      "00:01:45 | INFO | \"Total time taken 9.81 s.\n",
      "Finished at 2022-03-31 00:01:45.\"\n",
      "00:01:54 | INFO | \"Total time taken 8.68 s.\n",
      "Finished at 2022-03-31 00:01:54.\"\n",
      "00:02:04 | INFO | \"Total time taken 9.71 s.\n",
      "Finished at 2022-03-31 00:02:04.\"\n",
      "00:02:13 | INFO | \"Total time taken 9.19 s.\n",
      "Finished at 2022-03-31 00:02:13.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.43 s ± 369 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_pd = pd.read_gbq(\n",
    "    query=test_sql_100,\n",
    "    progress_bar_type=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8f40d",
   "metadata": {},
   "source": [
    "# Run query to get subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f5874f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_cls = LoadSubredditsSQL(\n",
    "#     table='all_reddit_subreddits',\n",
    "#     dataset='all_reddit',\n",
    "#     columns='subreddit_name',\n",
    "#     project_name='data-prod-165221',\n",
    "#     col_unique_check='subreddit_name',\n",
    "#     sql_template='all_reddit_subreddits',\n",
    "#     log_query=True,\n",
    "# )\n",
    "# df_subs = subreddit_cls.get_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a274e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:21:50 | INFO | \"# Connecting to BigQuery... #\"\n",
      "03:21:51 | INFO | \"# Running query... #\"\n",
      "03:21:51 | INFO | \"\n",
      "        SELECT\n",
      "            \n",
      "                subreddit_id\n",
      "                # , name\n",
      "                # , title\n",
      "                # , description\n",
      "            \n",
      ", CONCAT(name, '. ', COALESCE(title, ''), '. ', COALESCE(description, '')) AS concat_text\n",
      "        FROM data-prod-165221.ds_v2_postgres_tables.subreddit_lookup\n",
      "        WHERE 1=1\n",
      "            AND dt = (CURRENT_DATE() - 2)  -- subreddit_lookup\n",
      "            -- Exclude user-profiles + spam & sketchy subs\n",
      "            AND COALESCE(verdict, 'f') <> 'admin_removed'\n",
      "            AND COALESCE(is_spam, FALSE) = FALSE\n",
      "            AND COALESCE(is_deleted, FALSE) = FALSE\n",
      "            AND deleted IS NULL\n",
      "            AND type IN ('public', 'private', 'restricted')\n",
      "            AND NOT REGEXP_CONTAINS(LOWER(name), r'^u_.*')\n",
      "        LIMIT 50000\n",
      "        \"\n",
      "03:21:51 | INFO | \"  2022-03-31 03:21:51.257244 | query START time\"\n",
      "03:21:55 | INFO | \"  2022-03-31 03:21:55.441602 | query END time\"\n",
      "03:21:55 | INFO | \"  0:00:04.184358 | query ELAPSED time\"\n",
      "03:21:55 | INFO | \"Col subreddit_id is unique\"\n",
      "03:21:55 | INFO | \"  (50000, 2) <- df.shape\"\n"
     ]
    }
   ],
   "source": [
    "subreddit_cls2 = LoadSubredditsSQL(\n",
    "    table='subreddit_lookup',\n",
    "    dataset='ds_v2_postgres_tables',\n",
    "    project_name='data-prod-165221',\n",
    "    sql_template='subreddit_lookup',\n",
    "    concat_text_cols=\"CONCAT(name, '. ', COALESCE(title, ''), '. ', COALESCE(description, ''))\",\n",
    "    log_query=True,\n",
    "    limit_clause='LIMIT 50000',\n",
    ")\n",
    "df_subs_slo = subreddit_cls2.get_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3513e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>concat_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_zo9in</td>\n",
       "      <td>a:t5_zo9in. Subreddit to use the in app reddit browser.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_gdrsz</td>\n",
       "      <td>agentpanda. a testbed for agentpanda stuff. there's nothing that matters here, go away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_62eaoz</td>\n",
       "      <td>furryeslove. furryeslove.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5_5742f0</td>\n",
       "      <td>AvengeCopperGolem. AvengeCopperGolem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5_5zz2v4</td>\n",
       "      <td>Technologymiles. Technologymiles.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id                                                                             concat_text\n",
       "0     t5_zo9in                                a:t5_zo9in. Subreddit to use the in app reddit browser. \n",
       "1     t5_gdrsz  agentpanda. a testbed for agentpanda stuff. there's nothing that matters here, go away\n",
       "2    t5_62eaoz                                                              furryeslove. furryeslove. \n",
       "3    t5_5742f0                                                  AvengeCopperGolem. AvengeCopperGolem. \n",
       "4    t5_5zz2v4                                                      Technologymiles. Technologymiles. "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs_slo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa842ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:21:55 | INFO | \"  Query already cached\"\n",
      "03:21:55 | INFO | \"  (50000, 2) <- df.shape\"\n"
     ]
    }
   ],
   "source": [
    "df_subs = subreddit_cls2.get_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af517db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit_id    0\n",
       "concat_text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17021b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>concat_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subreddit_id, concat_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subs[df_subs['concat_text'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89990e0d",
   "metadata": {},
   "source": [
    "# Load data from within new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37932099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_subreddits_sql_test_local\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_name = 'vectorize_subreddits_sql_test_local'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90f75ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-31 07:53:36.630460: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "CFG keys: dict_keys(['data_text', 'config_description', 'model_name', 'thing_to_vectorize', 'col_text_for_embeddings', 'batch_inference_rows', 'limit_first_n_chars', 'get_embeddings_verbose', 'cols_index', 'output_bucket', 'output_folder'])\n",
      "`2022-03-31 07:53:38,826` | `INFO` | `Creating vectorizing class for subreddit_meta...`\n",
      "`2022-03-31 07:53:38,827` | `INFO` | `Start vectorize function`\n",
      "`2022-03-31 07:53:38,827` | `INFO` | `Lodaing model: use_multilingual_3`\n",
      "`2022-03-31 07:53:38,857` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "2022-03-31 07:53:39.009061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-31 07:53:39.948886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:39.949609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-03-31 07:53:39.949659: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-31 07:53:39.952895: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-31 07:53:39.955031: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-31 07:53:39.955547: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-31 07:53:39.959291: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-31 07:53:39.960254: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-31 07:53:39.960517: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-31 07:53:39.960635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:39.961230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:39.961809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-03-31 07:53:39.974931: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199995000 Hz\n",
      "2022-03-31 07:53:39.979550: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559f31a82210 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-03-31 07:53:39.979586: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-03-31 07:53:40.070369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:40.071217: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559f33331100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-03-31 07:53:40.071272: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2022-03-31 07:53:40.071637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:40.072274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-03-31 07:53:40.072346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-31 07:53:40.072415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-31 07:53:40.072441: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-31 07:53:40.072461: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-31 07:53:40.072482: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-03-31 07:53:40.072501: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-31 07:53:40.072529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-31 07:53:40.072643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:40.073342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:40.073975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2022-03-31 07:53:40.074090: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-31 07:53:40.454164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-03-31 07:53:40.454224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2022-03-31 07:53:40.454234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2022-03-31 07:53:40.454609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:40.455521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 07:53:40.456240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13996 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "`2022-03-31 07:53:43,003` | `INFO` | `Model loaded`\n",
      "`2022-03-31 07:53:43,003` | `INFO` | `# Connecting to BigQuery... #`\n",
      "`2022-03-31 07:53:44,186` | `INFO` | `# Running query... #`\n",
      "`2022-03-31 07:53:44,187` | `INFO` | `\n",
      "        SELECT\n",
      "            subreddit_id, LOWER(name) AS subreddit_name\n",
      ", CONCAT(name, '. ', COALESCE(title, ''), '. ', COALESCE(description, '')) AS concat_text\n",
      "        FROM data-prod-165221.ds_v2_postgres_tables.subreddit_lookup\n",
      "        WHERE 1=1\n",
      "            AND dt = (CURRENT_DATE() - 2)  -- subreddit_lookup\n",
      "            -- Exclude user-profiles + spam & sketchy subs\n",
      "            AND COALESCE(verdict, 'f') <> 'admin_removed'\n",
      "            AND COALESCE(is_spam, FALSE) = FALSE\n",
      "            AND COALESCE(is_deleted, FALSE) = FALSE\n",
      "            AND deleted IS NULL\n",
      "            AND type IN ('public', 'private', 'restricted')\n",
      "            AND NOT REGEXP_CONTAINS(LOWER(name), r'^u_.*')\n",
      "        LIMIT 50000\n",
      "        `\n",
      "`2022-03-31 07:53:44,187` | `INFO` | `  2022-03-31 07:53:44.187272 | query START time`\n",
      "`2022-03-31 07:53:48,805` | `INFO` | `  2022-03-31 07:53:48.805312 | query END time`\n",
      "`2022-03-31 07:53:48,805` | `INFO` | `  0:00:04.618040 | query ELAPSED time`\n",
      "`2022-03-31 07:53:48,824` | `INFO` | `Col subreddit_id is unique`\n",
      "`2022-03-31 07:53:48,825` | `INFO` | `  (50000, 3) <- df.shape`\n",
      "0    a:t5_zo9in. Subreddit to use the in app reddit...\n",
      "1    agentpanda. a testbed for agentpanda stuff. th...\n",
      "2                           furryeslove. furryeslove. \n",
      "3               AvengeCopperGolem. AvengeCopperGolem. \n",
      "4                   Technologymiles. Technologymiles. \n",
      "Name: concat_text, dtype: object\n",
      "`2022-03-31 07:53:48,826` | `INFO` | `Vectorizing subreddit descriptions...`\n",
      "`2022-03-31 07:53:48,828` | `INFO` | `cols_index: ['subreddit_name', 'subreddit_id']`\n",
      "`2022-03-31 07:53:48,828` | `INFO` | `col_text: concat_text`\n",
      "`2022-03-31 07:53:48,828` | `INFO` | `lowercase_text: False`\n",
      "`2022-03-31 07:53:48,828` | `INFO` | `limit_first_n_chars: 1100`\n",
      "`2022-03-31 07:53:48,828` | `INFO` | `limit_first_n_chars_retry: 600`\n",
      "`2022-03-31 07:53:49,098` | `INFO` | `Getting embeddings in batches of size: 2000`\n",
      "2022-03-31 07:53:50.018322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\n",
      "`2022-03-31 07:54:00,239` | `INFO` | `progress:  85%|###########################9     | 22/26 [00:11<00:02,  1.98it/s]`\n",
      "`2022-03-31 07:54:01,824` | `INFO` | `progress: 100%|#################################| 26/26 [00:12<00:00,  2.04it/s]`\n",
      "\n",
      "`2022-03-31 07:54:02,013` | `INFO` | `  0:00:13.186560 <- df_subs vectorizing time elapsed`\n",
      "                                embeddings_0  ...  embeddings_511\n",
      "subreddit_name    subreddit_id                ...                \n",
      "a:t5_zo9in        t5_zo9in          0.010862  ...       -0.001617\n",
      "agentpanda        t5_gdrsz         -0.047473  ...        0.026653\n",
      "furryeslove       t5_62eaoz         0.046720  ...        0.062534\n",
      "avengecoppergolem t5_5742f0        -0.040259  ...        0.036934\n",
      "technologymiles   t5_5zz2v4         0.074395  ...        0.061821\n",
      "\n",
      "[5 rows x 512 columns]\n",
      "`2022-03-31 07:54:02,087` | `INFO` | `Saving df_embeddings to: gcs://gazette-models-temp/i18n_topic_model/subreddits/test/2022-03-31_075338/df-50000_by_512.parquet`\n",
      "`2022-03-31 07:54:07,625` | `INFO` | `  0:00:28.798458 <- Total vectorize fxn time elapsed`\n"
     ]
    }
   ],
   "source": [
    "# run on sample data, test experiment\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_name \\\n",
    "    data_text.subreddit_meta.data_loader_kwargs.limit_clause=\"LIMIT 50000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4728585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
