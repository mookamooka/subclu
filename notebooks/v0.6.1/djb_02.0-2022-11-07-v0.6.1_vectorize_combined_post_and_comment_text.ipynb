{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeebb975",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Use this notebook to vectorize the text of combined Post + Comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db432d6",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76facc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdec095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "numpy\t\tv: 1.18.5\n",
      "pandas\t\tv: 1.2.5\n",
      "subclu\t\tv: 0.6.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([np, pd, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647ac2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:07:44 | INFO | \"loggging ready\"\n"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()\n",
    "logging.info('loggging ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6fddd",
   "metadata": {},
   "source": [
    "# Auth note\n",
    "This notebook assumes you have authenticated using the gcloud CLI. Example</br>\n",
    "```bash\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e65361",
   "metadata": {},
   "source": [
    "# Load data AND Vectorize \n",
    "\n",
    "When we call the vectorizing function, it calls the data loader under the hood.\n",
    "See the configs in:\n",
    "- `subclu2/config/`\n",
    "    - `data_text_and_metadata/`\n",
    "        -  `vX.X.X_model.yaml`\n",
    "    - `vectorize_post_and_comments_combined_seed_vX.X.X.yaml`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5887ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/david.bermejo/repos/subreddit_clustering_i18n/\n",
      "subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf\n",
      "vectorize_post_and_comments_combined_seed_v0.6.1\n"
     ]
    }
   ],
   "source": [
    "path_djb_repo = '/home/david.bermejo/repos/subreddit_clustering_i18n/' \n",
    "path_djb_models = '/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models' \n",
    "file_vectorize_py = 'subclu.i18n_topic_model_batch.subclu2.get_embeddings.vectorize_text_tf'\n",
    "\n",
    "config_vectorize = 'vectorize_post_and_comments_combined_seed_v0.6.1'\n",
    "\n",
    "print(path_djb_repo)\n",
    "print(file_vectorize_py)\n",
    "print(config_vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90506e77",
   "metadata": {},
   "source": [
    "## Run in bucket owned by i18n\n",
    "This bucket retains data longer than the gazette temp bucket\n",
    "\n",
    "Expected location for data cache:\n",
    "```bash\n",
    "...\n",
    "gcs_path: i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all\n",
    "local_cache_path: /home/jupyter/subreddit_clustering_i18n/data/local_cache/\n",
    "```\n",
    "\n",
    "Check log file created by hydra (because notebook can timeout and we might lose track of progress)\n",
    "```bash\n",
    "Log file created at: \n",
    "/home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-11-07/08-10-17/logs/2022-11-07_08-XX-XX_vectorize_text.log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b88b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG keys:\n",
      "  dict_keys(['data_text_and_metadata', 'config_description', 'local_cache_path', 'local_model_path', 'output_bucket', 'gcs_path_text_key', 'data_loader_name', 'data_loader_kwargs', 'n_sample_files', 'n_files_slice_start', 'n_files_slice_end', 'process_individual_files', 'col_text_for_embeddings', 'model_name', 'batch_inference_rows', 'limit_first_n_chars', 'limit_first_n_chars_retry', 'get_embeddings_verbose', 'cols_index'])\n",
      "Data Loader kwags:\n",
      "  columns: ['subreddit_id', 'subreddit_name', 'post_id', 'post_and_comment_text_clean']\n",
      "  df_format: pandas\n",
      "  unique_check: False\n",
      "  verbose: True\n",
      "  bucket_name: i18n-subreddit-clustering\n",
      "  gcs_path: i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all\n",
      "  local_cache_path: /home/jupyter/subreddit_clustering_i18n/data/local_cache/\n",
      "  n_sample_files: None\n",
      "  n_files_slice_start: None\n",
      "  n_files_slice_end: None\n",
      "`2022-11-07 08:10:17,113` | `INFO` | `Using hydra's path`\n",
      "`2022-11-07 08:10:17,113` | `INFO` | `  Log file created at: /home/jupyter/subreddit_clustering_i18n/hydra_runs/outputs/2022-11-07/08-10-17/logs/2022-11-07_08-10-17_vectorize_text.log`\n",
      "`2022-11-07 08:10:17,113` | `INFO` | `Start vectorize function`\n",
      "`2022-11-07 08:10:17,113` | `INFO` | `Loading model: use_multilingual_3`\n",
      "`2022-11-07 08:10:19,004` | `INFO` | `Using /tmp/tfhub_modules to cache modules.`\n",
      "`2022-11-07 08:10:19,005` | `INFO` | `Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.`\n",
      "`2022-11-07 08:10:21,761` | `INFO` | `Downloaded https://tfhub.dev/google/universal-sentence-encoder-multilingual/3, Total size: 266.88MB`\n",
      "`2022-11-07 08:10:21,762` | `INFO` | `Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'.`\n",
      "`2022-11-07 08:10:29,831` | `INFO` | `Model loaded`\n",
      "`2022-11-07 08:10:29,831` | `INFO` | `  Loading & Processing each file independently`\n",
      "`2022-11-07 08:10:31,185` | `INFO` | `  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all`\n",
      "`2022-11-07 08:10:31,247` | `INFO` | `  197 <- Files matching prefix`\n",
      "`2022-11-07 08:10:31,247` | `INFO` | `  197 <- Files to check`\n",
      "`2022-11-07 08:14:11,253` | `INFO` | `  Files already cached: 0`\n",
      "`2022-11-07 08:14:11,254` | `INFO` | `0:03:41.422230  <- Downloading files elapsed time`\n",
      "`2022-11-07 08:14:11,255` | `INFO` | `  Files already downloaded.`\n",
      "`2022-11-07 08:14:15,190` | `INFO` | `  Processing: 000000000000.parquet`\n",
      "`2022-11-07 08:14:15,190` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:14:15,434` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:14:42,235` | `INFO` | `  Vectorizing:   0%|1                         | 1/236 [00:26<1:44:58, 26.80s/it]`\n",
      "`2022-11-07 08:14:54,271` | `INFO` | `  Vectorizing:   7%|#9                         | 17/236 [00:38<06:43,  1.84s/it]`\n",
      "`2022-11-07 08:15:06,487` | `INFO` | `  Vectorizing:  15%|####1                      | 36/236 [00:51<03:40,  1.10s/it]`\n",
      "`2022-11-07 08:15:18,494` | `INFO` | `  Vectorizing:  24%|######4                    | 56/236 [01:03<02:34,  1.16it/s]`\n",
      "`2022-11-07 08:15:31,258` | `INFO` | `  Vectorizing:  24%|######4                    | 56/236 [01:15<02:34,  1.16it/s]`\n",
      "`2022-11-07 08:15:31,373` | `INFO` | `  Vectorizing:  31%|########3                  | 73/236 [01:15<02:13,  1.22it/s]`\n",
      "`2022-11-07 08:15:43,480` | `INFO` | `  Vectorizing:  39%|##########6                | 93/236 [01:28<01:45,  1.35it/s]`\n",
      "`2022-11-07 08:15:56,222` | `INFO` | `  Vectorizing:  48%|############4             | 113/236 [01:40<01:26,  1.42it/s]`\n",
      "`2022-11-07 08:16:08,411` | `INFO` | `  Vectorizing:  57%|##############7           | 134/236 [01:52<01:07,  1.51it/s]`\n",
      "`2022-11-07 08:16:20,736` | `INFO` | `  Vectorizing:  66%|#################1        | 156/236 [02:05<00:50,  1.60it/s]`\n",
      "`2022-11-07 08:16:31,334` | `INFO` | `  Vectorizing:  66%|#################1        | 156/236 [02:15<00:50,  1.60it/s]`\n",
      "`2022-11-07 08:16:32,898` | `INFO` | `  Vectorizing:  77%|###################9      | 181/236 [02:17<00:31,  1.73it/s]`\n",
      "`2022-11-07 08:16:46,799` | `INFO` | `  Vectorizing:  87%|######################6   | 206/236 [02:31<00:17,  1.76it/s]`\n",
      "`2022-11-07 08:17:01,383` | `INFO` | `  Vectorizing:  87%|######################6   | 206/236 [02:45<00:17,  1.76it/s]`\n",
      "`2022-11-07 08:17:01,747` | `INFO` | `  Vectorizing:  97%|#########################1| 228/236 [02:46<00:04,  1.66it/s]`\n",
      "`2022-11-07 08:17:07,356` | `INFO` | `  Vectorizing: 100%|##########################| 236/236 [02:51<00:00,  1.37it/s]`\n",
      "\n",
      "`2022-11-07 08:17:08,588` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/000000000000-317975_by_515.parquet`\n",
      "`2022-11-07 08:17:33,182` | `INFO` | `Files in batch:   1%|             | 1/197 [03:21<10:59:37, 201.93s/it]`\n",
      "`2022-11-07 08:17:34,279` | `INFO` | `  Processing: 000000000001.parquet`\n",
      "`2022-11-07 08:17:34,279` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:17:34,487` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:17:47,223` | `INFO` | `  Vectorizing:   5%|#4                         | 12/226 [00:12<03:47,  1.06s/it]`\n",
      "`2022-11-07 08:17:59,908` | `INFO` | `  Vectorizing:  14%|###8                       | 32/226 [00:25<02:27,  1.31it/s]`\n",
      "`2022-11-07 08:18:11,406` | `INFO` | `  Vectorizing:  14%|###8                       | 32/226 [00:36<02:27,  1.31it/s]`\n",
      "`2022-11-07 08:18:12,217` | `INFO` | `  Vectorizing:  22%|#####9                     | 50/226 [00:37<02:07,  1.38it/s]`\n",
      "`2022-11-07 08:18:25,894` | `INFO` | `  Vectorizing:  30%|########1                  | 68/226 [00:51<01:56,  1.35it/s]`\n",
      "`2022-11-07 08:18:38,147` | `INFO` | `  Vectorizing:  38%|##########2                | 86/226 [01:03<01:40,  1.39it/s]`\n",
      "`2022-11-07 08:18:50,699` | `INFO` | `  Vectorizing:  46%|###########9              | 104/226 [01:16<01:26,  1.41it/s]`\n",
      "`2022-11-07 08:19:01,408` | `INFO` | `  Vectorizing:  46%|###########9              | 104/226 [01:26<01:26,  1.41it/s]`\n",
      "`2022-11-07 08:19:02,927` | `INFO` | `  Vectorizing:  54%|#############9            | 121/226 [01:28<01:14,  1.40it/s]`\n",
      "`2022-11-07 08:19:15,775` | `INFO` | `  Vectorizing:  61%|###############8          | 138/226 [01:41<01:03,  1.38it/s]`\n",
      "`2022-11-07 08:19:28,189` | `INFO` | `  Vectorizing:  71%|##################5       | 161/226 [01:53<00:42,  1.52it/s]`\n",
      "`2022-11-07 08:19:41,508` | `INFO` | `  Vectorizing:  71%|##################5       | 161/226 [02:07<00:42,  1.52it/s]`\n",
      "`2022-11-07 08:19:41,855` | `INFO` | `  Vectorizing:  81%|####################9     | 182/226 [02:07<00:28,  1.53it/s]`\n",
      "`2022-11-07 08:19:55,043` | `INFO` | `  Vectorizing:  89%|#######################1  | 201/226 [02:20<00:16,  1.50it/s]`\n",
      "`2022-11-07 08:20:09,274` | `INFO` | `  Vectorizing:  97%|#########################1| 219/226 [02:34<00:04,  1.42it/s]`\n",
      "`2022-11-07 08:20:11,667` | `INFO` | `  Vectorizing: 100%|##########################| 226/226 [02:37<00:00,  1.44it/s]`\n",
      "\n",
      "`2022-11-07 08:20:12,823` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/000000000001-304046_by_515.parquet`\n",
      "`2022-11-07 08:20:37,260` | `INFO` | `Files in batch:   1%|1            | 2/197 [06:26<10:22:08, 191.43s/it]`\n",
      "`2022-11-07 08:20:37,933` | `INFO` | `  Processing: 000000000002.parquet`\n",
      "`2022-11-07 08:20:37,934` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:20:38,179` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:20:50,407` | `INFO` | `  Vectorizing:  14%|###7                       | 24/174 [00:12<01:16,  1.96it/s]`\n",
      "`2022-11-07 08:21:01,602` | `INFO` | `  Vectorizing:  14%|###7                       | 24/174 [00:23<01:16,  1.96it/s]`\n",
      "`2022-11-07 08:21:02,461` | `INFO` | `  Vectorizing:  29%|#######7                   | 50/174 [00:24<00:59,  2.08it/s]`\n",
      "`2022-11-07 08:21:21,605` | `INFO` | `  Vectorizing:  29%|#######7                   | 50/174 [00:43<00:59,  2.08it/s]`\n",
      "`2022-11-07 08:21:22,226` | `INFO` | `  Vectorizing:  44%|###########7               | 76/174 [00:44<00:59,  1.64it/s]`\n",
      "`2022-11-07 08:21:34,655` | `INFO` | `  Vectorizing:  55%|##############7            | 95/174 [00:56<00:49,  1.60it/s]`\n",
      "`2022-11-07 08:21:46,817` | `INFO` | `  Vectorizing:  66%|#################1        | 115/174 [01:08<00:36,  1.61it/s]`\n",
      "`2022-11-07 08:22:00,627` | `INFO` | `  Vectorizing:  78%|####################1     | 135/174 [01:22<00:25,  1.56it/s]`\n",
      "`2022-11-07 08:22:11,727` | `INFO` | `  Vectorizing:  78%|####################1     | 135/174 [01:33<00:25,  1.56it/s]`\n",
      "`2022-11-07 08:22:13,459` | `INFO` | `  Vectorizing:  86%|######################2   | 149/174 [01:35<00:17,  1.41it/s]`\n",
      "`2022-11-07 08:22:26,171` | `INFO` | `  Vectorizing:  94%|########################5 | 164/174 [01:47<00:07,  1.34it/s]`\n",
      "`2022-11-07 08:22:32,413` | `INFO` | `  Vectorizing: 100%|##########################| 174/174 [01:54<00:00,  1.52it/s]`\n",
      "\n",
      "`2022-11-07 08:22:33,345` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/000000000002-234112_by_515.parquet`\n",
      "`2022-11-07 08:22:52,156` | `INFO` | `Files in batch:   2%|2             | 3/197 [08:40<8:55:29, 165.61s/it]`\n",
      "`2022-11-07 08:22:52,883` | `INFO` | `  Processing: 000000000003.parquet`\n",
      "`2022-11-07 08:22:52,884` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:22:53,125` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:23:05,325` | `INFO` | `  Vectorizing:   8%|##                         | 21/271 [00:12<02:25,  1.72it/s]`\n",
      "`2022-11-07 08:23:18,314` | `INFO` | `  Vectorizing:  16%|####2                      | 43/271 [00:25<02:13,  1.70it/s]`\n",
      "`2022-11-07 08:23:30,655` | `INFO` | `  Vectorizing:  25%|######6                    | 67/271 [00:37<01:52,  1.81it/s]`\n",
      "`2022-11-07 08:23:41,746` | `INFO` | `  Vectorizing:  25%|######6                    | 67/271 [00:48<01:52,  1.81it/s]`\n",
      "`2022-11-07 08:23:43,030` | `INFO` | `  Vectorizing:  33%|########8                  | 89/271 [00:49<01:41,  1.80it/s]`\n",
      "`2022-11-07 08:23:56,107` | `INFO` | `  Vectorizing:  41%|##########6               | 111/271 [01:02<01:31,  1.76it/s]`\n",
      "`2022-11-07 08:24:09,250` | `INFO` | `  Vectorizing:  49%|############6             | 132/271 [01:16<01:21,  1.70it/s]`\n",
      "`2022-11-07 08:24:21,347` | `INFO` | `  Vectorizing:  56%|##############6           | 153/271 [01:28<01:08,  1.71it/s]`\n",
      "`2022-11-07 08:24:31,868` | `INFO` | `  Vectorizing:  56%|##############6           | 153/271 [01:38<01:08,  1.71it/s]`\n",
      "`2022-11-07 08:24:33,936` | `INFO` | `  Vectorizing:  66%|#################         | 178/271 [01:40<00:51,  1.80it/s]`\n",
      "`2022-11-07 08:24:50,221` | `INFO` | `  Vectorizing:  75%|###################3      | 202/271 [01:57<00:41,  1.68it/s]`\n",
      "`2022-11-07 08:25:01,949` | `INFO` | `  Vectorizing:  75%|###################3      | 202/271 [02:08<00:41,  1.68it/s]`\n",
      "`2022-11-07 08:25:02,316` | `INFO` | `  Vectorizing:  82%|#####################3    | 223/271 [02:09<00:28,  1.69it/s]`\n",
      "`2022-11-07 08:25:14,937` | `INFO` | `  Vectorizing:  90%|#######################5  | 245/271 [02:21<00:15,  1.71it/s]`\n",
      "`2022-11-07 08:25:28,239` | `INFO` | `  Vectorizing:  98%|#########################5| 266/271 [02:35<00:02,  1.67it/s]`\n",
      "`2022-11-07 08:25:31,470` | `INFO` | `  Vectorizing: 100%|##########################| 271/271 [02:38<00:00,  1.71it/s]`\n",
      "\n",
      "`2022-11-07 08:25:32,876` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/000000000003-365746_by_515.parquet`\n",
      "`2022-11-07 08:25:57,786` | `INFO` | `Files in batch:   2%|2             | 4/197 [11:46<9:18:08, 173.52s/it]`\n",
      "`2022-11-07 08:25:58,559` | `INFO` | `  Processing: 000000000004.parquet`\n",
      "`2022-11-07 08:25:58,561` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:25:58,806` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:26:11,063` | `INFO` | `  Vectorizing:  11%|###                        | 24/211 [00:12<01:35,  1.96it/s]`\n",
      "`2022-11-07 08:26:22,017` | `INFO` | `  Vectorizing:  11%|###                        | 24/211 [00:23<01:35,  1.96it/s]`\n",
      "`2022-11-07 08:26:23,376` | `INFO` | `  Vectorizing:  21%|#####6                     | 44/211 [00:24<01:34,  1.76it/s]`\n",
      "`2022-11-07 08:26:35,662` | `INFO` | `  Vectorizing:  32%|########5                  | 67/211 [00:36<01:19,  1.81it/s]`\n",
      "`2022-11-07 08:26:48,162` | `INFO` | `  Vectorizing:  43%|###########5               | 90/211 [00:49<01:06,  1.82it/s]`\n",
      "`2022-11-07 08:27:02,079` | `INFO` | `  Vectorizing:  43%|###########5               | 90/211 [01:03<01:06,  1.82it/s]`\n",
      "`2022-11-07 08:27:02,542` | `INFO` | `  Vectorizing:  52%|#############5            | 110/211 [01:03<01:01,  1.65it/s]`\n",
      "2022-11-07 08:27:21.484865: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.76GiB (rounded to 2962503680)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-11-07 08:27:21.485537: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****_______*****___**********************************________________\n",
      "2022-11-07 08:27:21.485605: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[578614,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-11-07 08:27:21,487` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[578614,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-11-07 08:27:22,080` | `INFO` | `  Vectorizing:  52%|#############5            | 110/211 [01:23<01:01,  1.65it/s]`\n",
      "`2022-11-07 08:27:22,831` | `INFO` | `  Vectorizing:  57%|##############9           | 121/211 [01:24<01:17,  1.16it/s]`\n",
      "2022-11-07 08:27:33.027113: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.74GiB (rounded to 2937533440)requested by op StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2\n",
      "Current allocation summary follows.\n",
      "2022-11-07 08:27:33.027868: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ***********************_______*****_______*****___*********************************_________________\n",
      "2022-11-07 08:27:33.027920: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at concat_op.cc:161 : Resource exhausted: OOM when allocating tensor with shape[573737,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "`2022-11-07 08:27:33,029` | `WARNING` | `\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[573737,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_15375]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "`\n",
      "`2022-11-07 08:27:38,527` | `INFO` | `  Vectorizing:  61%|###############7          | 128/211 [01:39<01:29,  1.08s/it]`\n",
      "`2022-11-07 08:27:50,587` | `INFO` | `  Vectorizing:  70%|##################2       | 148/211 [01:51<00:56,  1.12it/s]`\n",
      "`2022-11-07 08:28:02,135` | `INFO` | `  Vectorizing:  70%|##################2       | 148/211 [02:03<00:56,  1.12it/s]`\n",
      "`2022-11-07 08:28:02,898` | `INFO` | `  Vectorizing:  79%|####################5     | 167/211 [02:04<00:35,  1.24it/s]`\n",
      "`2022-11-07 08:28:16,426` | `INFO` | `  Vectorizing:  88%|######################9   | 186/211 [02:17<00:19,  1.29it/s]`\n",
      "`2022-11-07 08:28:28,689` | `INFO` | `  Vectorizing:  98%|#########################5| 207/211 [02:29<00:02,  1.41it/s]`\n",
      "`2022-11-07 08:28:31,545` | `INFO` | `  Vectorizing: 100%|##########################| 211/211 [02:32<00:00,  1.38it/s]`\n",
      "\n",
      "`2022-11-07 08:28:32,631` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/000000000004-284006_by_515.parquet`\n",
      "`2022-11-07 08:28:55,620` | `INFO` | `Files in batch:   3%|3             | 5/197 [14:44<9:20:14, 175.07s/it]`\n",
      "`2022-11-07 08:28:56,509` | `INFO` | `  Processing: 000000000005.parquet`\n",
      "`2022-11-07 08:28:56,511` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:28:56,758` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:29:08,793` | `INFO` | `  Vectorizing:   9%|##3                        | 22/255 [00:12<02:07,  1.83it/s]`\n",
      "`2022-11-07 08:29:22,222` | `INFO` | `  Vectorizing:   9%|##3                        | 22/255 [00:25<02:07,  1.83it/s]`\n",
      "`2022-11-07 08:29:23,075` | `INFO` | `  Vectorizing:  16%|####4                      | 42/255 [00:26<02:16,  1.56it/s]`\n",
      "`2022-11-07 08:29:35,952` | `INFO` | `  Vectorizing:  25%|######7                    | 64/255 [00:39<01:57,  1.63it/s]`\n",
      "`2022-11-07 08:29:52,268` | `INFO` | `  Vectorizing:  25%|######7                    | 64/255 [00:55<01:57,  1.63it/s]`\n",
      "`2022-11-07 08:29:53,020` | `INFO` | `  Vectorizing:  33%|#########                  | 85/255 [00:56<01:57,  1.44it/s]`\n",
      "`2022-11-07 08:30:06,165` | `INFO` | `  Vectorizing:  42%|###########               | 108/255 [01:09<01:35,  1.55it/s]`\n",
      "`2022-11-07 08:30:22,309` | `INFO` | `  Vectorizing:  51%|#############1            | 129/255 [01:25<01:26,  1.46it/s]`\n",
      "`2022-11-07 08:30:32,416` | `INFO` | `  Vectorizing:  51%|#############1            | 129/255 [01:35<01:26,  1.46it/s]`\n",
      "`2022-11-07 08:30:34,538` | `INFO` | `  Vectorizing:  58%|###############           | 148/255 [01:37<01:12,  1.48it/s]`\n",
      "`2022-11-07 08:30:47,004` | `INFO` | `  Vectorizing:  68%|#################6        | 173/255 [01:50<00:50,  1.64it/s]`\n",
      "`2022-11-07 08:31:02,519` | `INFO` | `  Vectorizing:  68%|#################6        | 173/255 [02:05<00:50,  1.64it/s]`\n",
      "`2022-11-07 08:31:03,438` | `INFO` | `  Vectorizing:  76%|###################8      | 195/255 [02:06<00:39,  1.53it/s]`\n",
      "`2022-11-07 08:31:18,270` | `INFO` | `  Vectorizing:  83%|#####################6    | 212/255 [02:21<00:30,  1.41it/s]`\n",
      "`2022-11-07 08:31:30,484` | `INFO` | `  Vectorizing:  93%|########################1 | 237/255 [02:33<00:11,  1.58it/s]`\n",
      "`2022-11-07 08:31:41,054` | `INFO` | `  Vectorizing: 100%|##########################| 255/255 [02:44<00:00,  1.55it/s]`\n",
      "\n",
      "`2022-11-07 08:31:42,332` | `INFO` | `Saving df_embeddings to: gcs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/000000000005-344157_by_515.parquet`\n",
      "`2022-11-07 08:32:07,717` | `INFO` | `Files in batch:   3%|4             | 6/197 [17:56<9:35:44, 180.86s/it]`\n",
      "`2022-11-07 08:32:08,332` | `INFO` | `  Processing: 000000000006.parquet`\n",
      "`2022-11-07 08:32:08,334` | `INFO` | `Vectorizing column: post_and_comment_text_clean`\n",
      "`2022-11-07 08:32:08,547` | `INFO` | `Getting embeddings in batches of size: 1350`\n",
      "`2022-11-07 08:32:20,981` | `INFO` | `  Vectorizing:   7%|##                         | 16/214 [00:12<02:33,  1.29it/s]`\n",
      "`2022-11-07 08:32:32,521` | `INFO` | `  Vectorizing:   7%|##                         | 16/214 [00:23<02:33,  1.29it/s]`\n",
      "`2022-11-07 08:32:32,996` | `INFO` | `  Vectorizing:  16%|####4                      | 35/214 [00:24<02:02,  1.46it/s]`\n"
     ]
    }
   ],
   "source": [
    "# run on full data\n",
    "\n",
    "!cd $path_djb_repo && python -m $file_vectorize_py \\\n",
    "    --config-name $config_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7458945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
