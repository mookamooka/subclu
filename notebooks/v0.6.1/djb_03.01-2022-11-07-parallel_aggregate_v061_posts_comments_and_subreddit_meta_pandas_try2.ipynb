{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba9487f",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "**2022-11-07: v0.6.1**\n",
    "<br> Use `dask.delayed` to run aggregation slightly faster (~3x). Still not fast enough, but better.\n",
    "Note that I created a 2nd notebook because the first one timed out, so it's not clear whether the other notebook is still running or not.\n",
    "\n",
    "**2022-08-15: v0.6.0**\n",
    "<br>Test `dask.delayed` to run aggregation of multiple subreddits in parallel.\n",
    "With the new project we expect to aggregate posts for over 300k subreddits. For most of the process, each subreddit can be processed independently of other subreddits, so it makes sense to try to split up the work so we can speed things up.\n",
    "\n",
    "**2022-06-29: v0.5.0**\n",
    "<br>Because we embedded post & text as a single embedding and we didn't use MLflow to create those embeddings, it's easier to  run the embeddings in this notebook rather than to re-use or re-write the old `AggregateEmbeddings` class.\n",
    "\n",
    "Provenance:\n",
    "* `v0.4.1 / djb_03.01-2021-12-aggregate_v041_posts_and_comments_pandas.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ec2fd",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f939a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242763ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "dask\t\tv: 2021.06.0\n",
      "hydra\t\tv: 1.1.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "plotly\t\tv: 4.14.3\n",
      "seaborn\t\tv: 0.11.1\n",
      "subclu\t\tv: 0.6.1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "from logging import info\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "import hydra\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.aggregate_embeddings import (\n",
    "    AggregateEmbeddings, AggregateEmbeddingsConfig,\n",
    "    load_config_agg_jupyter, get_dask_df_shape,\n",
    ")\n",
    "from subclu.models import aggregate_embeddings_pd\n",
    "\n",
    "from subclu.utils import set_working_directory, get_project_subfolder\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric,\n",
    "    elapsed_time,\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger, save_pd_df_to_parquet_in_chunks\n",
    "from subclu.eda.aggregates import (\n",
    "    compare_raw_v_weighted_language\n",
    ")\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl\n",
    ")\n",
    "\n",
    "from subclu.i18n_topic_model_batch.subclu2.utils.data_loaders_gcs import LoadSubredditsGCS\n",
    "\n",
    "\n",
    "print_lib_versions([dask, hydra, mlflow, np, pd, plotly, sns, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f643970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246af7c",
   "metadata": {},
   "source": [
    "# Set Local model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaecbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/subreddit_clustering_i18n/data/models/aggregate_embeddings/manual_v061_2022-11-08_035115')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_model_timestamp = datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')\n",
    "path_this_model = get_project_subfolder(\n",
    "    f\"data/models/aggregate_embeddings/manual_v061_{manual_model_timestamp}\"\n",
    ")\n",
    "Path.mkdir(path_this_model, parents=True, exist_ok=True)\n",
    "path_this_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d7906",
   "metadata": {},
   "source": [
    "# CREATE MLFLOW EXPERIMENTS!!\n",
    "Before kicking off these jobs, make sure to create mlflow experiments for embeddings!!!\n",
    "\n",
    "Otherwise we might end up with broken MLflow SQLite databases\n",
    "\n",
    "master experiment list here:\n",
    "- `subclu/utils/mlflow_logger.py`\n",
    "    - `MlflowLogger.initialize_experiment_names` (class.method)\n",
    "    \n",
    "Example:\n",
    "```python\n",
    "l_experiments = [\n",
    "    ...\n",
    "    \n",
    "    'v0.6.1_mUSE_aggregates_test',\n",
    "    'v0.6.1_mUSE_aggregates',\n",
    "    'v0.6.1_mUSE_clustering_test',\n",
    "    'v0.6.1_mUSE_clustering',\n",
    "    'v0.6.1_nearest_neighbors',\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {
    "aggregate_embeddings_v0.6.1.yaml": {
     "yaml": "IyBVc2UgdGhpcyBjb25maWcgdG8gZ2V0IGFnZ3JlZ2F0ZSBlbWJlZGRpbmdzCmRlZmF1bHRzOgogIC0gZGF0YV90ZXh0X2FuZF9tZXRhZGF0YTogIHYwLjYuMV9tb2RlbAogIC0gZGF0YV9lbWJlZGRpbmdzX3RvX2FnZ3JlZ2F0ZTogdjAuNi4xXzIwMjItMTEtMDdfbXVzZV9sb3dlcl9jYXNlX2ZhbHNlCiAgLSBhZ2dyZWdhdGVfcGFyYW1zOiB2MC42LjFfYWdnCgpidWNrZXRfb3V0cHV0OiAnaTE4bi1zdWJyZWRkaXQtY2x1c3RlcmluZycKbWxmbG93X3RyYWNraW5nX3VyaTogJ3NxbGl0ZScKIyBFeGFtcGxlczogJ3YwLjQuMF91c2VfbXVsdGlfYWdncmVnYXRlcycgICd2MC4zLjJfdXNlX211bHRpX2FnZ3JlZ2F0ZXNfdGVzdCcKbWxmbG93X2V4cGVyaW1lbnQ6ICd2MC42LjFfbVVTRV9hZ2dyZWdhdGVzJwpuX3NhbXBsZV9zdWJyZWRkaXRzOiBudWxsCm5fc2FtcGxlX3Bvc3RzX2ZpbGVzOiBudWxsCm5fc2FtcGxlX2NvbW1lbnRzX2ZpbGVzOiBudWxsCgphZ2dfc3R5bGU6ICdkYXNrX2RlbGF5ZWQnCg=="
    }
   },
   "cell_type": "markdown",
   "id": "1f31a05c",
   "metadata": {},
   "source": [
    "# Load config for embeddings aggregation\n",
    "\n",
    "For v0.6.1 embeddings I didn't use mlflow to track the embeddings inference. We'll need to get them from these folders in GCS.\n",
    "<br>For example:\n",
    "- [Subreddit metadata](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/subreddits/text/embedding/2022-06-29_084555)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/2022xxxx/subreddits/text/embedding/2022-xx-xx_084555`\n",
    "- [Post + Comment Text (already combined)](https://console.cloud.google.com/storage/browser/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20220629/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-06-29_091925)\n",
    "    - `i18n-subreddit-clustering/i18n_topic_model_batch/runs/2022xxxx/post_and_comment_text_combined/text_subreddit_seeds/embedding/2022-xx-xx_091925`\n",
    "\n",
    "---\n",
    "\n",
    "### Configs to update:\n",
    "\n",
    "\n",
    "- `subclu/configs/`\n",
    "    - `data_text_and_metadata/` <- This is where the raw metadata & text gets pulled\n",
    "        - `vX.x.x_model.yaml`\n",
    "    - `data_embeddings_to_aggregate/` <- This is where we pull the embeddings for a) subreddit meta & b) post+comments text\n",
    "        - `v0.6.1_2022-11-07_muse_lower_case_false.yaml`\n",
    "    - `aggregate_params/`  <- Parameters for aggregation weights\n",
    "        - `v0.6.1_agg.yaml`\n",
    "    - `aggregate_embeddings_v0.6.1.yaml`  <- File that references all the configs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d19962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data_text_and_metadata', 'data_embeddings_to_aggregate', 'aggregate_params', 'bucket_output', 'mlflow_tracking_uri', 'mlflow_experiment', 'n_sample_subreddits', 'n_sample_posts_files', 'n_sample_comments_files', 'agg_style'])\n"
     ]
    }
   ],
   "source": [
    "cfg_agg_embeddings = LoadHydraConfig(\n",
    "    config_name='aggregate_embeddings_v0.6.1.yaml',\n",
    "    config_path=\"../config\",\n",
    "    overrides=[\n",
    "        f\"agg_style=dask_delayed\",\n",
    "    ],\n",
    ")\n",
    "print(cfg_agg_embeddings.config_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609da0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745d8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_text_and_metadata:\n",
      "    dataset_name: v0.6.1 inputs. ~110k seed subreddits, ~340k with 3+ posts, ~700k total subreddits\n",
      "    bucket_name: i18n-subreddit-clustering\n",
      "    folder_subreddits_text_and_meta: i18n_topic_model_batch/runs/20221107/subreddits/text\n",
      "    folder_posts_text_and_meta: i18n_topic_model_batch/runs/20221107/posts\n",
      "    folder_comments_text_and_meta: i18n_topic_model_batch/runs/20221107/comments\n",
      "    folder_post_and_comment_text_and_meta: i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all\n",
      "data_embeddings_to_aggregate:\n",
      "    bucket_embeddings: i18n-subreddit-clustering\n",
      "    post_and_comments_folder_embeddings: i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017\n",
      "    subreddit_desc_folder_embeddings: i18n_topic_model_batch/runs/20221107/subreddits/text/embedding/2022-11-07_074632\n",
      "    col_subreddit_id: subreddit_id\n",
      "aggregate_params:\n",
      "    min_post_and_comment_text_len: 3\n",
      "    agg_post_post_and_comment_weight: 85\n",
      "    agg_post_subreddit_desc_weight: 15\n",
      "bucket_output: i18n-subreddit-clustering\n",
      "mlflow_tracking_uri: sqlite\n",
      "mlflow_experiment: v0.6.1_mUSE_aggregates\n",
      "n_sample_subreddits: None\n",
      "n_sample_posts_files: None\n",
      "n_sample_comments_files: None\n",
      "agg_style: dask_delayed\n"
     ]
    }
   ],
   "source": [
    "for k_, v_ in cfg_agg_embeddings.config_dict.items():\n",
    "    if isinstance(v_, dict):\n",
    "        print(f\"{k_}:\")\n",
    "        for k2_, v2_ in v_.items():\n",
    "            print(f\"    {k2_}: {v2_}\")\n",
    "    else:\n",
    "        print(f\"{k_}: {v_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4561b2",
   "metadata": {},
   "source": [
    "# Download post embeddings with `gsutil`\n",
    "\n",
    "gsutil can be 5x+ faster than the python library(!)\n",
    "However, it can sometimes lock up the VM and crash jupyter :/\n",
    "\n",
    "\n",
    "- https://cloud.google.com/storage/docs/gsutil/commands/cp#description\n",
    "- https://cloud.google.com/storage/docs/wildcards\n",
    "\n",
    "- If you have a large number of files to transfer, you can perform a parallel multi-threaded/multi-processing copy using the top-level gsutil `-m` option\n",
    "- the `-n` option to prevent overwriting the content of existing files. The following example downloads text files from a bucket without clobbering the data in your directory\n",
    "- Use the `-r` option to copy an entire directory tree.\n",
    "\n",
    "- `-o` Set/override values in the boto configuration value, in the format \\\\`<section>:<name>=<value>`:\n",
    "    - Examples: `-o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20`\n",
    "\n",
    "    \n",
    "```bash\n",
    "gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017 \\\n",
    "    /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "# Try w/o parallel option (works, but pretty slow)\n",
    "gsutil cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017 \\\n",
    "    /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding\n",
    "\n",
    "    \n",
    "# ===============\n",
    "# Try batches using wildcards/regex\n",
    "# ===\n",
    "\n",
    "#  Batches of 100 & 50 still breaks\n",
    "# ===\n",
    "gsutil -m cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/0000000001*.parquet \\\n",
    "    /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding\n",
    "\n",
    "/ [0/201 files][ 21.5 GiB/150.3 GiB]  14% Done 776.4 MiB/s ETA 00:02:50\n",
    "\n",
    "\n",
    "# Batches of ~20 seems to work ok, but better to use the flags and let it figure out the thread itself\n",
    "# ===\n",
    "gsutil -m cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017/0000000000[0-2]*.parquet \\\n",
    "    /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d3ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i18n_topic_model_batch/runs/20221107/subreddits/text/embedding/2022-11-07_074632\n",
      "i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017 \n",
      "\n",
      "gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m cp -r -n gs://i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017 /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding \n",
      "\n",
      "CPU times: user 322 µs, sys: 0 ns, total: 322 µs\n",
      "Wall time: 291 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gcs_sub_embeddings = cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['subreddit_desc_folder_embeddings']\n",
    "print(gcs_sub_embeddings)\n",
    "gcs_post_comment_embeddings = cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['post_and_comments_folder_embeddings']\n",
    "print(gcs_post_comment_embeddings, '\\n')\n",
    "\n",
    "\n",
    "# gsutil is usually faster than the python library.\n",
    "remote_bucket_and_key = f\"{cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings']}/{gcs_post_comment_embeddings}\"\n",
    "remote_gs_path = f'gs://{remote_bucket_and_key}'\n",
    "\n",
    "# Need to remove the last part of the local path otherwise we'll get duplicate subfolders:\n",
    "#. top/2021-12-14/2021-12-14 instead of top/2021-12-14\n",
    "local_f = f\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/{'/'.join(remote_bucket_and_key.split('/')[:-1])}\"\n",
    "Path(local_f).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(f\"Remote path:\\n  {remote_gs_path}\")\n",
    "# print(f\"Local path:\\n  {local_f}\")\n",
    "\n",
    "# could do it manually (a pain & not worth it!)\n",
    "#. Use the limits for parallel threads & processes instead!\n",
    "l_parallel_file_regex = [\n",
    "    \"0000000000[0-1]*.parquet\",\n",
    "    \"0000000000[2-3]*.parquet\",\n",
    "    \"0000000000[4-5]*.parquet\",\n",
    "    \"0000000000[6-7]*.parquet\",\n",
    "    \"0000000000[8-9]*.parquet\",\n",
    "\n",
    "    \"0000000001[0-1]*.parquet\",\n",
    "    \"0000000001[2-3]*.parquet\",\n",
    "    \"0000000001[4-5]*.parquet\",\n",
    "    \"0000000001[6-7]*.parquet\",\n",
    "    \"0000000001[8-9]*.parquet\",\n",
    "\n",
    "#     \"0000000002[0-1]*.parquet\",\n",
    "#     \"0000000002[2-3]*.parquet\",\n",
    "#     \"0000000002[4-5]*.parquet\",\n",
    "#     \"0000000002[6-7]*.parquet\",\n",
    "#     \"0000000002[8-9]*.parquet\",\n",
    "]\n",
    "# could do it manually (a pain)\n",
    "# for rx_ in l_parallel_file_regex:\n",
    "#     print(\n",
    "#         f\"\\n=== {rx_} ===\\ngsutil -m cp -r -n {remote_gs_path}/{rx_}  {local_f} \\n\"\n",
    "#     )\n",
    "#     ## !gsutil -m cp -r -n $remote_gs_path/$rx_ $local_f\n",
    "\n",
    "# NOTE: best to run this command from a separate terminal b/c it can crash a jupyter notebook \n",
    "#  when loading many large files (30+)\n",
    "# Add flags to limit thread & process count to ~20 fixes most problems\n",
    "print(f\"gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m cp -r -n {remote_gs_path} {local_f} \\n\")\n",
    "\n",
    "# !gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m cp -r -n $remote_gs_path $local_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b3a5b",
   "metadata": {},
   "source": [
    "# Start MLflow & Log base params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69edf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MlflowLogger(tracking_uri=cfg_agg_embeddings.config_dict['mlflow_tracking_uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7384026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:51:17 | INFO | \"== Start run_aggregation() method ==\"\n",
      "03:51:17 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-100-2021-04-28-djb-eda-german-subs/mlruns.db\"\n",
      "03:51:17 | INFO | \"host_name: djb-100-2021-04-28-djb-eda-german-subs\"\n",
      "03:51:17 | INFO | \"cpu_count: 96\"\n",
      "03:51:17 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '36.28%', 'memory_total': '1,444,961', 'memory_used': '524,198', 'memory_free': '759,036'}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_total': 1444961,\n",
       " 'memory_used_percent': 0.3627765732085503,\n",
       " 'memory_used': 524198,\n",
       " 'memory_free': 759036}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_experiment = cfg_agg_embeddings.config_dict['mlflow_experiment']\n",
    "# 'v0.6.0_mUSE_aggregates', 'v0.6.0_mUSE_aggregates_test'\n",
    "\n",
    "\n",
    "t_start_agg_embed = datetime.utcnow()\n",
    "info(f\"== Start run_aggregation() method ==\")\n",
    "\n",
    "\n",
    "info(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "mlf.set_experiment(mlflow_experiment)\n",
    "mlflow.start_run()\n",
    "mlf.add_git_hash_to_active_run()\n",
    "mlf.set_tag_hostname(key='host_name')\n",
    "mlf.log_param_hostname(key='host_name')\n",
    "mlf.log_cpu_count()\n",
    "mlf.log_ram_stats(param=True, only_memory_used=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d78e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# set weights\n",
    "# Normalize them by dividing by 100\n",
    "WEIGHT_POST_COMMENT = (\n",
    "    cfg_agg_embeddings.config_dict['aggregate_params']['agg_post_post_and_comment_weight'] / 100\n",
    ")\n",
    "WEIGHT_SUB_META = (\n",
    "    cfg_agg_embeddings.config_dict['aggregate_params']['agg_post_subreddit_desc_weight'] / 100\n",
    ")\n",
    "print(WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "assert(1.0 == WEIGHT_POST_COMMENT + WEIGHT_SUB_META)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlflow.log_params(\n",
    "    {\n",
    "        'embeddings_bucket': cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "        'embeddings_subreddit_path': gcs_sub_embeddings,\n",
    "        'embeddings_post_and_comments_path': gcs_post_comment_embeddings,\n",
    "        'weight_post_and_comments': WEIGHT_POST_COMMENT,\n",
    "        'weight_subreddit_meta': WEIGHT_SUB_META,\n",
    "    }\n",
    ")\n",
    "for k_, v_ in cfg_agg_embeddings.config_dict.items():\n",
    "    if isinstance(v_, str):\n",
    "        try:\n",
    "            mlflow.log_param(k_, v_)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91742b3",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "NOTE: Manually end run if it failed\n",
    "\n",
    "mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7edd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:51:19 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/subreddits/text/embedding/2022-11-07_074632\"\n",
      "03:51:19 | INFO | \"  7 <- Files matching prefix\"\n",
      "03:51:19 | INFO | \"  7 <- Files to check\"\n",
      "03:51:19 | INFO | \"    000000000000-100179_by_514.parquet <- File already exists, not downloading\"\n",
      "03:51:19 | INFO | \"    000000000001-233442_by_514.parquet <- File already exists, not downloading\"\n",
      "03:51:19 | INFO | \"    000000000002-448032_by_514.parquet <- File already exists, not downloading\"\n",
      "03:51:19 | INFO | \"    2022-11-07_07-46-32_vectorize_text.log <- File already exists, not downloading\"\n",
      "03:51:19 | INFO | \"  Files already cached: 4\"\n",
      "03:51:19 | INFO | \"  Files already downloaded.\"\n",
      "03:51:19 | INFO | \"  df format: pandas\"\n",
      "03:51:24 | INFO | \"  Checking ID uniqueness...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781,653 rows, 514 cols\n",
      "CPU times: user 4.35 s, sys: 4.2 s, total: 8.55 s\n",
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_start_data_load_ = datetime.utcnow()\n",
    "\n",
    "subs_v = LoadSubredditsGCS(\n",
    "    bucket_name=cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "    gcs_path=gcs_sub_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='subreddit_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=True,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=None,\n",
    "    n_files_slice_start=None,\n",
    "    n_files_slice_end=None,\n",
    ")\n",
    "subs_v.local_cache()\n",
    "\n",
    "df_v_subs = subs_v.read_as_one_df()\n",
    "r_subs, c_subs = df_v_subs.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_subs-rows\": r_subs,\n",
    "        f\"df_v_subs-cols\": c_subs,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_subs:,.0f} rows, {c_subs:,.0f} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fe9ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:51:25 | INFO | \"  Local folder to download artifact(s):\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/i18n_topic_model_batch/runs/20221107/post_and_comment_text_combined/text_all/embedding/2022-11-07_081017\"\n",
      "03:51:25 | INFO | \"  201 <- Files matching prefix\"\n",
      "03:51:25 | INFO | \"  201 <- Files to check\"\n",
      "03:51:25 | INFO | \"    000000000000-317975_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000001-304046_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000002-234112_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000003-365746_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000004-284006_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000005-344157_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000006-288452_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000007-328399_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000008-286643_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000009-285034_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000010-362352_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000011-329918_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000012-361456_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000013-160027_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000014-155936_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000015-207033_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000016-145423_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000017-142225_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000018-145328_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000019-129978_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000020-160575_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000021-190841_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000022-161259_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000023-185042_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000024-221256_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000025-175580_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000026-204490_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000027-173246_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000028-149440_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000029-198851_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000030-195427_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000031-149365_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000032-206159_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000033-162085_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000034-202458_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000035-158788_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000036-161242_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000037-170940_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000038-143692_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000039-153882_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000040-208440_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000041-188220_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000042-161006_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000043-171565_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000044-186888_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000045-157992_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000046-187810_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000047-169131_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000048-197850_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000049-219704_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000050-207211_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000051-156679_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000052-189484_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000053-217213_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000054-254263_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000055-144382_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000056-194025_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000057-165076_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000058-203558_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000059-145972_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000060-197097_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000061-210309_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000062-199302_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000063-207950_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000064-165508_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000065-206735_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000066-186393_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000067-217251_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000068-185667_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000069-214155_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000070-203997_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000071-208625_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000072-203783_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000073-180830_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000074-272170_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000075-243572_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000076-211377_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000077-210118_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000078-148317_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000079-196201_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000080-198679_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000081-213339_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000082-171514_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000083-229399_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000084-229895_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000085-258362_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000086-261583_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000087-216437_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000088-231726_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000089-242482_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000090-203483_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000091-285791_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000092-264828_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000093-210404_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000094-179188_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000095-202139_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000096-286079_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000097-275555_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000098-141527_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000099-175522_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000100-215154_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000101-240761_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000102-242195_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000103-260886_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000104-229344_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000105-215206_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000106-196658_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000107-228972_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000108-211435_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000109-265602_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000110-247074_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000111-223537_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000112-215301_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000113-236618_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000114-219441_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000115-281575_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000116-177301_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000117-210333_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000118-265935_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000119-282584_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000120-198459_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000121-236523_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000122-212209_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000123-254515_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000124-284412_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000125-249263_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000126-222672_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000127-244248_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000128-256189_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000129-263192_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000130-249298_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000131-288456_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000132-264451_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000133-272104_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000134-307344_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000135-300822_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000136-210075_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000137-211541_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000138-274607_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000139-305330_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000140-396154_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000141-354247_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000142-296142_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000143-323654_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000144-284877_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000145-263252_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000146-274500_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000147-296964_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000148-232566_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000149-258940_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000150-280701_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000151-392875_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000152-378452_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000153-352032_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000154-326825_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000155-254810_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000156-444671_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000157-369857_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000158-343842_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000159-270505_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000160-510901_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000161-455564_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000162-504390_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000163-410218_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000164-429882_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000165-573689_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000166-421047_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000167-435050_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000168-403894_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000169-428007_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000170-423054_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000171-463704_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000172-440470_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000173-557078_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000174-543647_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000175-535768_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000176-687466_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000177-573377_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000178-588104_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000179-562089_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000180-585265_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000181-644225_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000182-745625_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000183-617542_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000184-622365_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000185-265229_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000186-301089_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000187-320220_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000188-165195_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000189-330936_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000190-263964_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000191-267037_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000192-366197_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000193-283868_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000194-248630_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000195-187859_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    000000000196-291062_by_515.parquet <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"    2022-11-07_08-10-17_vectorize_text.log <- File already exists, not downloading\"\n",
      "03:51:25 | INFO | \"  Files already cached: 198\"\n",
      "03:51:25 | INFO | \"  Files already downloaded.\"\n",
      "03:51:25 | INFO | \"  df format: pandas\"\n",
      "03:55:50 | INFO | \"  0:04:31.870024 <- Data Loading Time time elapsed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53,597,817 rows, 515 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:55:53 | INFO | \"RAM stats:\n",
      "{'memory_used_percent': '49.77%', 'memory_used': '719,153'}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 26s, sys: 16min 43s, total: 24min 10s\n",
      "Wall time: 4min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory_used_percent': 0.49769716968139627, 'memory_used': 719153}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pc_v = LoadSubredditsGCS(\n",
    "    bucket_name=cfg_agg_embeddings.config_dict['data_embeddings_to_aggregate']['bucket_embeddings'],\n",
    "    gcs_path=gcs_post_comment_embeddings,\n",
    "    local_cache_path=\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/\",\n",
    "    columns=None,\n",
    "    col_unique_check='post_id',\n",
    "    df_format='pandas',\n",
    "    unique_check=False,\n",
    "    verbose= True,\n",
    "    \n",
    "    n_sample_files=cfg_agg_embeddings.config_dict['n_sample_posts_files'],  # None,\n",
    "    n_files_slice_start=None,  # None,\n",
    "    n_files_slice_end=None,  # None, \n",
    ")\n",
    "pc_v.local_cache()\n",
    "\n",
    "df_v_pc = pc_v.read_as_one_df()\n",
    "r_pc, c_pc = df_v_pc.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_v_post_comments-rows\": r_pc,\n",
    "        f\"df_v_post_comments-cols\": c_pc,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_pc:,.0f} rows, {c_pc:,.0f} cols\")\n",
    "\n",
    "t_data_load = elapsed_time(start_time=t_start_data_load_, log_label='Data Loading Time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-data_loading_time',\n",
    "                  t_data_load / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccc20b",
   "metadata": {},
   "source": [
    "# Set weights & create copy dfs for new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad7fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "l_ix_sub_level = ['subreddit_id', 'subreddit_name']\n",
    "l_ix_post_level = l_ix_sub_level + ['post_id']\n",
    "\n",
    "l_embedding_cols = [c for c in df_v_pc if c.startswith('embeddings_')]\n",
    "print(len(l_embedding_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbc4ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:56:39 | INFO | \"Initializing weighted SUBS meta\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:56:41 | INFO | \"Initializing weighted POSTS embeddings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "CPU times: user 1min 46s, sys: 1min 44s, total: 3min 31s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_v_pc_weighted = df_v_pc.copy()\n",
    "\n",
    "df_v_subs_weighted = df_v_subs.copy()\n",
    "\n",
    "# should be True b/c they're copies\n",
    "print(np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515]))\n",
    "print(np.allclose(df_v_subs_weighted.iloc[:1000,2:515], df_v_subs.iloc[:1000,2:515]))\n",
    "\n",
    "# apply weight to all posts & subreddit meta at once (vectorized)\n",
    "info(f\"Initializing weighted SUBS meta\")\n",
    "df_v_subs_weighted[l_embedding_cols] = df_v_subs_weighted[l_embedding_cols] * WEIGHT_SUB_META\n",
    "\n",
    "info(f\"Initializing weighted POSTS embeddings\")\n",
    "df_v_pc_weighted[l_embedding_cols] = df_v_pc_weighted[l_embedding_cols] * WEIGHT_POST_COMMENT\n",
    "\n",
    "# NOW they shouldn't be equal (Should be False)\n",
    "print(np.allclose(df_v_pc_weighted.iloc[:1000,3:515], df_v_pc.iloc[:1000,3:515]))\n",
    "print(np.allclose(df_v_subs_weighted.iloc[:1000,2:515], df_v_subs.iloc[:1000,2:515]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "147e82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_describe(df_v_pc[l_ix_post_level])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658c13b",
   "metadata": {},
   "source": [
    "# Aggregate to Post-Level: Post&Comments + Subreddit Meta\n",
    "\n",
    "It's better to let pandas handle the interations with `.groupby('subreddit_id')`. Otherwise we have to create masks for each subreddit that can take much longer (17+ hours).\n",
    "\n",
    "\n",
    "For creating the DAG with 81k subreddits:\n",
    "- ETA with masks: +17.6 hours\n",
    "- ETA with groupby: ~2.5 hours\n",
    "\n",
    "For creating DAG with 700k subreddits:\n",
    "- 8.5 hours: with dask + groupby\n",
    "\n",
    "```\n",
    "# mask:\n",
    "0%  329/81973 [04:18<17:42:36, 1.28it/s]\n",
    "\n",
    "# .groupby()\n",
    "6% 4751/81973 [09:56<2:35:06, 8.30it/s]\n",
    "\n",
    "\n",
    "# .groupby() + dask.delayed(....to_numpy()) | 700k+ subreddits:\n",
    "5%  34121/705963 [26:36<8:15:14, 22.61it/s]\n",
    "\n",
    "5% 38903/711664 [30:03<8:09:53, 22.89it/s\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Updates using `dask.delayed`:\n",
    "By combining .groupby() + `dask.delayed` we can process things ~3x faster:\n",
    "\n",
    "```\n",
    "# .groupby() + dask.delayed(....to_numpy()) | FASTEST\n",
    "100% 3467/3467 [02:31<00:00, 23.08it/s]\n",
    "Wall time: 2min 38s\n",
    "\n",
    "\n",
    "# masks with dask.delayed():\n",
    "#  This is 2x faster than serial processing, but .groupby() + dask.delayed() is much faster\n",
    "100% 3467/3467 [00:11<00:00, 299.85it/s]\n",
    "05:44:20 | INFO | \"Define new C1 df DAG in dask\"\n",
    "05:44:20 | INFO | \"COMPUTE new C1 df START\"\n",
    "05:48:20 | INFO | \"COMPUTE new C1 df DONE\"\n",
    "05:48:20 | INFO | \"  0:04:11.393036 <- Total Agg fxn time time elapsed\"\n",
    "CPU times: user 4min 33s, sys: 24.3 s, total: 4min 57s\n",
    "Wall time: 4min 12s\n",
    "\n",
    "\n",
    "# .groupby(), no dask delayed | SLOWEST\n",
    "100% 3467/3467 [08:20<00:00, 6.97it/s]\n",
    "  0:08:21.661816 <- Total Agg fxn time time elapsed\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ef66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:59:24 | INFO | \"Start C1 - posts + comments + sub descriptions with format: `dask_delayed`\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8570b39945bf471284908fc5190894d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/711664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# set style so that we can try output & time in either format\n",
    "AGG_STYLE = cfg_agg_embeddings.config_dict['agg_style']  # serial v. dask.delayed\n",
    "\n",
    "info(f\"Start C1 - posts + comments + sub descriptions with format: `{AGG_STYLE}`\")\n",
    "t_start_agg_post_c1 = datetime.utcnow()\n",
    "\n",
    "l_df_c1_weights = list()\n",
    "\n",
    "if AGG_STYLE == 'serial':\n",
    "    for s_id, df_ in tqdm(\n",
    "        df_v_pc_weighted.groupby('subreddit_id'),\n",
    "        ascii=True, mininterval=5,\n",
    "    ):\n",
    "        # For each post in a subreddit, get new embedding: combine subreddit_meta + post(and_comment)\n",
    "        df_.loc[:, l_embedding_cols] = np.add(\n",
    "            df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy(),\n",
    "            df_[l_embedding_cols]\n",
    "        )\n",
    "        l_df_c1_weights.append(df_)\n",
    "        del df_\n",
    "\n",
    "    info(f\"Create new C1 df\")\n",
    "    df_posts_agg_c1 = pd.concat(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "elif AGG_STYLE == 'dask_delayed':\n",
    "    for s_id, df_ in tqdm(\n",
    "        df_v_pc_weighted.groupby('subreddit_id'),\n",
    "        ascii=True, mininterval=5,\n",
    "    ):\n",
    "        # For each post in a subreddit, get new embedding: combine subreddit_meta + post(and_comment)\n",
    "        df_pc_embeddings_ = dask.delayed(np.add)(\n",
    "            dask.delayed(df_v_subs_weighted[df_v_subs_weighted['subreddit_id'] == s_id][l_embedding_cols].to_numpy()),\n",
    "            dask.delayed(df_[l_embedding_cols])\n",
    "        )\n",
    "        l_df_c1_weights.append(\n",
    "            dask.delayed(pd.concat)([dask.delayed(df_[l_ix_post_level]), df_pc_embeddings_], ignore_index=False, axis=1)\n",
    "        )\n",
    "\n",
    "    info(f\"Define new posts C1 df DAG in dask\")\n",
    "    df_posts_agg_c1_delayed = dask.delayed(pd.concat)(l_df_c1_weights, ignore_index=True)\n",
    "\n",
    "    info(f\"COMPUTE new C1 df START\")\n",
    "    df_posts_agg_c1 = df_posts_agg_c1_delayed.compute()\n",
    "    info(f\"COMPUTE new C1 df DONE\")\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError(f'Other agg style not implemented: {AGG_STYLE}')\n",
    "\n",
    "\n",
    "r_, c_ = df_posts_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_posts_agg_c1-rows\": r_,\n",
    "        f\"df_posts_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "print(f\"{r_:,.0f} rows, {c_:,.0f} cols\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_pc_c1 = elapsed_time(start_time=t_start_agg_post_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_posts_agg_c1_no_delay',\n",
    "                  t_agg_pc_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "info(f\"C1 - post level complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2601b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_agg_c1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts_agg_c1.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca69346",
   "metadata": {},
   "source": [
    "### Save post-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dfs_to_save = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_posts_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_posts_agg_c1_{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_posts_agg_c1,\n",
    "    d_dfs_to_save['df_posts_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_posts_agg_c1']['local'], artifact_path='df_posts_agg_c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba5c73",
   "metadata": {},
   "source": [
    "# Aggregate to Subreddit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# first, figure out how many posts each subreddit has\n",
    "info(f\"Count posts per subreddit...\")\n",
    "c_post_embedding_count = 'posts_for_embeddings_count'\n",
    "\n",
    "\n",
    "df_posts_for_embedding_count = (\n",
    "    df_posts_agg_c1\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .agg(**{c_post_embedding_count: ('post_id', 'nunique')})\n",
    ")\n",
    "# fill subs that have no posts\n",
    "df_posts_for_embedding_count = pd.concat(\n",
    "    [\n",
    "        df_posts_for_embedding_count, \n",
    "        df_v_subs[\n",
    "            ~df_v_subs['subreddit_id'].isin(df_posts_agg_c1['subreddit_id'])\n",
    "        ][l_ix_sub_level].assign(**{c_post_embedding_count: 0})\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "mlf.log_ram_stats(only_memory_used=True)\n",
    "\n",
    "# min_posts >= -> regular mean. If it's less than this, then mix in subreddit_description into average\n",
    "n_min_posts_for_regular_mean = 3\n",
    "subreddits_above_n_ = (\n",
    "    df_posts_for_embedding_count\n",
    "    [df_posts_for_embedding_count[c_post_embedding_count] >= n_min_posts_for_regular_mean]\n",
    "    ['subreddit_id']\n",
    ")\n",
    "subreddits_below_n_ = set(df_v_subs['subreddit_id']) - set(subreddits_above_n_)\n",
    "mask_min_posts_for_reg_mean = df_posts_agg_c1['subreddit_id'].isin(subreddits_above_n_)\n",
    "\n",
    "\n",
    "info(f\"SUBREDDIT-LEVEL C1 - posts + comments + sub descriptions\")\n",
    "t_start_agg_subs_c1 = datetime.utcnow()\n",
    "\n",
    "# 3+ posts: simple mean()\n",
    "info(f\"Mean for subs above threshold: {n_min_posts_for_regular_mean}\")\n",
    "df_subs_agg_c1_Nplus = (\n",
    "    df_posts_agg_c1[mask_min_posts_for_reg_mean]\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# calculate mean for all other subs: add UNWEIGHTED subreddit_description into averages\n",
    "info(f\"Calculating mean for subs BELOW post threshold...\")\n",
    "df_subs_agg_c1_Nbelow = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df_posts_agg_c1[~mask_min_posts_for_reg_mean],\n",
    "            df_v_subs[df_v_subs['subreddit_id'].isin(subreddits_below_n_)]\n",
    "        ]\n",
    "    )\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "mlf.log_ram_stats(only_memory_used=True)\n",
    "info(f\"Combining all subreddits...\")\n",
    "df_subs_agg_c1 = (\n",
    "    df_posts_for_embedding_count\n",
    "    .merge(\n",
    "        pd.concat([df_subs_agg_c1_Nplus, df_subs_agg_c1_Nbelow]),\n",
    "        how='outer',\n",
    "        on=l_ix_sub_level\n",
    "    )\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "\n",
    "# Check for dupes\n",
    "assert(len(df_subs_agg_c1) == df_subs_agg_c1['subreddit_id'].nunique()), f\"Found duplicate subreddit_ids\"\n",
    "\n",
    "r_, c_ = df_subs_agg_c1.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_subs_agg_c1-rows\": r_,\n",
    "        f\"df_subs_agg_c1-cols\": c_,\n",
    "    }\n",
    ")\n",
    "info(f\"{r_:,.0f} rows, {c_:,.0f} cols  <- df_subs_agg_c1.shape (posts + comments + sub description)\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_subs_c1 = elapsed_time(start_time=t_start_agg_subs_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_subs_agg_c1',\n",
    "                  t_agg_subs_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_agg_c1.iloc[-8:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs_agg_c1.iloc[10:18, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a2db0",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "Save to dask anyway b/c it could require multiple files as we cover 700k+ subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c420a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d_dfs_to_save['df_subs_agg_c1']['local'] = (\n",
    "    path_this_model / f\"df_subs_agg_c1-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_subs_agg_c1,\n",
    "    d_dfs_to_save['df_subs_agg_c1']['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save['df_subs_agg_c1']['local'], artifact_path='df_subs_agg_c1')\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378926ab",
   "metadata": {},
   "source": [
    "## 2nd flow for subreddit level -- do not include additional weight from subreddit description\n",
    "Potentially, we might be skewing the embeddings too much by adding extra weight to subreddit description.\n",
    "\n",
    "So save embeddings WITHOUT additional weights so that we can compare the two approaches.\n",
    "\n",
    "We'll still fill subreddits w/o posts with subreddit description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05920823",
   "metadata": {},
   "outputs": [],
   "source": [
    "info(f\"SUBREDDIT-LEVEL C1 no extra sub description weight - posts + comments + sub descriptions\")\n",
    "t_start_agg_subs_c1_uw = datetime.utcnow()\n",
    "\n",
    "# 3+ posts: simple mean()\n",
    "info(f\"Mean for subs above threshold: {n_min_posts_for_regular_mean} (already calculated)\")\n",
    "\n",
    "# calculate mean for all other subs: add UNWEIGHTED subreddit_description into averages\n",
    "info(f\"Calculating mean for subs BELOW post threshold...\")\n",
    "df_subs_agg_c1_Nbelow_uw = (\n",
    "    df_posts_agg_c1[~mask_min_posts_for_reg_mean]\n",
    "    .groupby(l_ix_sub_level, as_index=False)\n",
    "    .mean()\n",
    ")\n",
    "# get embeddings for subs w/ zero posts\n",
    "subs_wo_posts = df_posts_for_embedding_count[df_posts_for_embedding_count[c_post_embedding_count] == 0]['subreddit_id']\n",
    "info(f\"{len(subs_wo_posts):,.0f}\")\n",
    "\n",
    "mlf.log_ram_stats(only_memory_used=True)\n",
    "info(f\"Combining all subreddits...\")\n",
    "df_subs_agg_c1_uw = (\n",
    "    df_posts_for_embedding_count\n",
    "    .merge(\n",
    "        pd.concat(\n",
    "            [\n",
    "                df_subs_agg_c1_Nplus, df_subs_agg_c1_Nbelow_uw, \n",
    "                df_v_subs[df_v_subs['subreddit_id'].isin(subs_wo_posts)]\n",
    "            ]\n",
    "        ),\n",
    "        how='outer',\n",
    "        on=l_ix_sub_level\n",
    "    )\n",
    "    .sort_values(by=l_ix_sub_level)\n",
    ")\n",
    "\n",
    "# Check for dupes\n",
    "assert(len(df_subs_agg_c1_uw) == df_subs_agg_c1_uw['subreddit_id'].nunique()), f\"Found duplicate subreddit_ids\"\n",
    "\n",
    "r_, c_ = df_subs_agg_c1_uw.shape\n",
    "mlflow.log_metrics(\n",
    "    {\n",
    "        f\"df_subs_agg_c1_uw-rows\": r_,\n",
    "        f\"df_subs_agg_c1_uw-cols\": c_,\n",
    "    }\n",
    ")\n",
    "info(f\"{r_:,.0f} rows, {c_:,.0f} cols  <- df_subs_agg_c1_uw.shape (posts + comments + sub description)\")\n",
    "del r_, c_\n",
    "\n",
    "t_agg_subs_c1_uw = elapsed_time(start_time=t_start_agg_subs_c1, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-df_subs_agg_c1_uw',\n",
    "                  t_agg_subs_c1 / timedelta(minutes=1)\n",
    "                  )\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5afcbe",
   "metadata": {},
   "source": [
    "### Check equality of unweighted v. weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be True\n",
    "assert True == np.allclose(\n",
    "    (\n",
    "        df_subs_agg_c1_uw\n",
    "        [df_subs_agg_c1_uw['subreddit_id'].isin(subreddits_above_n_.head(20))]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    ),\n",
    "    (\n",
    "        df_subs_agg_c1\n",
    "        [df_subs_agg_c1['subreddit_id'].isin(subreddits_above_n_.head(20))]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be False\n",
    "l_sample_subs_below_n_ = list(subreddits_below_n_)[:20]\n",
    "assert False == np.allclose(\n",
    "    (\n",
    "        df_subs_agg_c1_uw\n",
    "        [df_subs_agg_c1_uw['subreddit_id'].isin(l_sample_subs_below_n_)]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    ),\n",
    "    (\n",
    "        df_subs_agg_c1\n",
    "        [df_subs_agg_c1['subreddit_id'].isin(l_sample_subs_below_n_)]\n",
    "        .sort_values(by=['subreddit_id'])\n",
    "        [l_embedding_cols]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486822ff",
   "metadata": {},
   "source": [
    "### Save Subreddit level\n",
    "\n",
    "Use dask b/c as we model over 200k subreddits a single file gets too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "name_sub_agg_unweighted = 'df_subs_agg_c1_unweighted'\n",
    "d_dfs_to_save[name_sub_agg_unweighted]['local'] = (\n",
    "    path_this_model / f\"{name_sub_agg_unweighted}-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    ")\n",
    "\n",
    "save_pd_df_to_parquet_in_chunks(\n",
    "    df_subs_agg_c1_uw,\n",
    "    d_dfs_to_save[name_sub_agg_unweighted]['local'],\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "\n",
    "info(f\"  Logging df to mlflow...\")\n",
    "mlflow.log_artifacts(d_dfs_to_save[name_sub_agg_unweighted]['local'], artifact_path=name_sub_agg_unweighted)\n",
    "mlf.log_ram_stats(only_memory_used=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faac04",
   "metadata": {},
   "source": [
    "# End run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecf764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish logging total time + end mlflow run\n",
    "total_fxn_time = elapsed_time(start_time=t_start_agg_embed, log_label='Total Agg fxn time', verbose=True)\n",
    "mlflow.log_metric('time_fxn-full_aggregation_fxn_minutes',\n",
    "                  total_fxn_time / timedelta(minutes=1)\n",
    "                  )\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run(\"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a60fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
