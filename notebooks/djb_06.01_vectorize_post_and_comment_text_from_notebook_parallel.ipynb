{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074dda82",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading fastText embeddings & create a uSIF model\n",
    "- load post & comment text\n",
    "- train a uSIF model\n",
    "- convert the text into embeddings (at post or comment level)\n",
    "\n",
    "Currently only one job call runs at a time, so I may try running two notebooks at the same time to run some jobs in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21aa4b3",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf25b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0aaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.19.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.4\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "from functools import partial\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    "    D_MODELS_CPU,\n",
    "    process_text_for_fse,\n",
    "    vectorize_text_with_fse,\n",
    ")\n",
    "from subclu.models.preprocess_text import TextPreprocessor, transform_and_tokenize_text\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b181f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b04a71",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845b2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1288660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/mlflow/mlruns.db'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2bd51",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea53ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ced7881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_id': '0',\n",
       "  'name': 'Default',\n",
       "  'artifact_location': './mlruns/0',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '1',\n",
       "  'name': 'fse_v1',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/1',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '2',\n",
       "  'name': 'fse_vectorize_v1',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/2',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '3',\n",
       "  'name': 'subreddit_description_v1',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/3',\n",
       "  'lifecycle_stage': 'active'},\n",
       " {'experiment_id': '4',\n",
       "  'name': 'fse_vectorize_v1.1',\n",
       "  'artifact_location': 'gs://i18n-subreddit-clustering/mlflow/mlruns/4',\n",
       "  'lifecycle_stage': 'active'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd01d1",
   "metadata": {},
   "source": [
    "# Call function to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f26714ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment = 'fse_vectorize_v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0c1d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:22:10 | INFO | \"Start vectorize function\"\n",
      "07:22:10 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0722\"\n",
      "07:22:10 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:22:17 | INFO | \"  0:00:06.316106 <- df_post time elapsed\"\n",
      "07:22:17 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:22:17 | INFO | \"Load comments df...\"\n",
      "07:22:25 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:22:25 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:22:25 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:22:25 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:22:26 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:22:26 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:22:26 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:22:26 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:22:26 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:22:26 | INFO | \"Converting df_train to fse format...\"\n",
      "07:22:26 | INFO | \"  0:00:00.061098 <- Converting to fse time elapsed\"\n",
      "07:22:26 | INFO | \"Logging training df to mlflow...\"\n",
      "07:22:26 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:22:26 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:22:26 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:23:56 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:23:56 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:23:59 | INFO | \"  0:01:33.336307 <- Load FSE model time elapsed\"\n",
      "07:23:59 | INFO | \"Start training fse model...\"\n",
      "07:24:08 | INFO | \"Running inference on all POSTS...\"\n",
      "07:24:08 | INFO | \"Convert vectors to df...\"\n",
      "07:24:13 | INFO | \"(111669, 300) <- Raw vectorized text shape\"\n",
      "07:24:13 | INFO | \"  0:00:05.492418 <- Raw vectorize to df only time elapsed\"\n",
      "07:24:13 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:24:13 | INFO | \"  Setting post_id as index...\"\n",
      "07:24:14 | INFO | \"Merge vectors with df...\"\n",
      "07:24:14 | INFO | \"  0:00:00.344890 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:24:14 | INFO | \"  0:00:06.262068 <- Converting vectors to df full time elapsed\"\n",
      "07:24:14 | INFO | \"Saving inference for comments df\"\n",
      "07:24:17 | INFO | \"  Saving inference complete\"\n",
      "07:24:17 | INFO | \"Get vectors for comments\"\n",
      "07:24:18 | INFO | \"Convert vectors to df...\"\n",
      "07:24:53 | INFO | \"(638052, 300) <- Raw vectorized text shape\"\n",
      "07:24:53 | INFO | \"  0:00:34.664400 <- Raw vectorize to df only time elapsed\"\n",
      "07:24:53 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:24:53 | INFO | \"  Setting comment_id as index...\"\n",
      "07:24:57 | INFO | \"Merge vectors with df...\"\n",
      "07:24:59 | INFO | \"  0:00:02.346834 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:24:59 | INFO | \"  0:00:41.075755 <- Converting vectors to df full time elapsed\"\n",
      "07:24:59 | INFO | \"  0:00:41.586788 <- Inference time for COMMENTS time elapsed\"\n",
      "07:24:59 | INFO | \"Save vectors for comments\"\n",
      "07:25:15 | INFO | \"  0:03:05.189119 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd42852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:25:16 | INFO | \"Start vectorize function\"\n",
      "07:25:16 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0725\"\n",
      "07:25:16 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:25:22 | INFO | \"  0:00:05.708467 <- df_post time elapsed\"\n",
      "07:25:22 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:25:22 | INFO | \"Load comments df...\"\n",
      "07:25:29 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:25:29 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:25:30 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:25:30 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:25:30 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:25:30 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:25:30 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:25:30 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:25:30 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:25:30 | INFO | \"Converting df_train to fse format...\"\n",
      "07:25:30 | INFO | \"  0:00:00.067377 <- Converting to fse time elapsed\"\n",
      "07:25:30 | INFO | \"Logging training df to mlflow...\"\n",
      "07:25:31 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:25:31 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:25:31 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:27:02 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:27:02 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:27:04 | INFO | \"  0:01:33.903202 <- Load FSE model time elapsed\"\n",
      "07:27:04 | INFO | \"Start training fse model...\"\n",
      "07:27:13 | INFO | \"Running inference on all POSTS...\"\n",
      "07:27:13 | INFO | \"Convert vectors to df...\"\n",
      "07:27:19 | INFO | \"(111669, 300) <- Raw vectorized text shape\"\n",
      "07:27:19 | INFO | \"  0:00:05.800530 <- Raw vectorize to df only time elapsed\"\n",
      "07:27:19 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:27:19 | INFO | \"  Setting post_id as index...\"\n",
      "07:27:19 | INFO | \"Merge vectors with df...\"\n",
      "07:27:19 | INFO | \"  0:00:00.344488 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:27:19 | INFO | \"  0:00:06.567358 <- Converting vectors to df full time elapsed\"\n",
      "07:27:19 | INFO | \"Saving inference for comments df\"\n",
      "07:27:23 | INFO | \"  Saving inference complete\"\n",
      "07:27:23 | INFO | \"Get vectors for comments\"\n",
      "07:27:23 | INFO | \"Convert vectors to df...\"\n",
      "07:28:00 | INFO | \"(638052, 300) <- Raw vectorized text shape\"\n",
      "07:28:00 | INFO | \"  0:00:36.960138 <- Raw vectorize to df only time elapsed\"\n",
      "07:28:00 | INFO | \"Create new df from dict_index_to_id to make merging easier...\"\n",
      "07:28:00 | INFO | \"  Setting comment_id as index...\"\n",
      "07:28:03 | INFO | \"Merge vectors with df...\"\n",
      "07:28:05 | INFO | \"  0:00:02.301038 <- Merging df_vect with ID columns time elapsed\"\n",
      "07:28:05 | INFO | \"  0:00:42.310134 <- Converting vectors to df full time elapsed\"\n",
      "07:28:05 | INFO | \"  0:00:42.806270 <- Inference time for COMMENTS time elapsed\"\n",
      "07:28:05 | INFO | \"Save vectors for comments\"\n",
      "07:28:22 | INFO | \"  0:03:05.402697 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "del model, df_posts, d_ix_to_id\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='sklearn_acronyms_emoji',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "280d5eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:45:47 | INFO | \"Start vectorize function\"\n",
      "07:45:47 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0745\"\n",
      "07:45:47 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:45:52 | INFO | \"  0:00:04.924321 <- df_post time elapsed\"\n",
      "07:45:52 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:45:52 | INFO | \"Load comments df...\"\n",
      "07:45:59 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:45:59 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:45:59 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:45:59 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:46:00 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:46:00 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:46:00 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:46:00 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:46:00 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:46:00 | INFO | \"Converting df_train to fse format...\"\n",
      "07:46:00 | INFO | \"  0:00:00.080495 <- Converting to fse time elapsed\"\n",
      "07:46:00 | INFO | \"Logging training df to mlflow...\"\n",
      "07:46:00 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:46:00 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:46:00 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:47:37 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:47:37 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:47:40 | INFO | \"  0:01:40.317414 <- Load FSE model time elapsed\"\n",
      "07:47:40 | INFO | \"Start training fse model...\"\n",
      "07:47:52 | INFO | \"Running inference on all POSTS...\"\n",
      "07:47:52 | INFO | \"  Inference + convert to df...\"\n",
      "07:48:02 | INFO | \"  0:00:09.456951 <- Raw inference+df only time elapsed\"\n",
      "07:48:02 | INFO | \"    (111669, 300) <- Raw vectorized text shape\"\n",
      "07:48:02 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:48:02 | INFO | \"  Setting post_id as index...\"\n",
      "07:48:02 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:48:03 | INFO | \"  0:00:00.478258 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:48:03 | INFO | \"  0:00:10.478047 <- Converting vectors to df FULL time elapsed\"\n",
      "07:48:03 | INFO | \"Saving inference for comments df\"\n",
      "07:48:07 | INFO | \"  Saving inference complete\"\n",
      "07:48:07 | INFO | \"Get vectors for comments\"\n",
      "07:48:07 | INFO | \"  Inference + convert to df...\"\n",
      "07:49:11 | INFO | \"  0:01:03.325285 <- Raw inference+df only time elapsed\"\n",
      "07:49:11 | INFO | \"    (638052, 300) <- Raw vectorized text shape\"\n",
      "07:49:11 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:49:11 | INFO | \"  Setting comment_id as index...\"\n",
      "07:49:16 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:49:19 | INFO | \"  0:00:02.969392 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:49:19 | INFO | \"  0:01:11.900149 <- Converting vectors to df FULL time elapsed\"\n",
      "07:49:19 | INFO | \"  0:01:12.535015 <- Inference time for COMMENTS time elapsed\"\n",
      "07:49:19 | INFO | \"Save vectors for comments\"\n",
      "07:49:42 | INFO | \"  0:03:54.915527 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model, df_posts, d_ix_to_id\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='gensim',\n",
    "    tokenize_lowercase=True,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c2dd085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:49:43 | INFO | \"Start vectorize function\"\n",
      "07:49:43 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/fse/2021-06-02_0749\"\n",
      "07:49:43 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/2021-05-19\"\n",
      "07:49:50 | INFO | \"  0:00:06.091006 <- df_post time elapsed\"\n",
      "07:49:50 | INFO | \"  (111669, 6) <- df_posts.shape\"\n",
      "07:49:50 | INFO | \"Load comments df...\"\n",
      "07:49:58 | INFO | \"  (757388, 6) <- df_comments shape\"\n",
      "07:49:58 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "07:49:58 | INFO | \"  (638052, 6) <- updated df_comments shape\"\n",
      "07:49:58 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "07:50:00 | INFO | \"Filtering posts for SIF training...\"\n",
      "07:50:00 | INFO | \"59,366 <- Exclude posts because of: subreddits filter\"\n",
      "07:50:00 | INFO | \"30,537 <- Exclude posts because of: duplicated posts\"\n",
      "07:50:00 | INFO | \"25,328 <- Exclude posts because of: minimum word count\"\n",
      "07:50:00 | INFO | \"31,790 <- df_posts for training\"\n",
      "07:50:00 | INFO | \"Converting df_train to fse format...\"\n",
      "07:50:01 | INFO | \"  0:00:00.116683 <- Converting to fse time elapsed\"\n",
      "07:50:01 | INFO | \"Logging training df to mlflow...\"\n",
      "07:50:01 | INFO | \"Loading model fasttext_usif_de...\n",
      "  with kwargs: {'lang_id': 'de', 'workers': 10, 'length': 11, 'lang_freq': 'de', 'verbose': True}\"\n",
      "07:50:01 | INFO | \"  Getting pretrained model for language: de...\"\n",
      "07:50:01 | INFO | \"  fastText embeddings location:\n",
      "    /home/jupyter/subreddit_clustering_i18n/data/embeddings/fasttext\"\n",
      "07:51:35 | INFO | \"  2,000,000 <- Model vocabulary\"\n",
      "07:51:35 | INFO | \"  True <- True if `fse` is running in parallel..\"\n",
      "07:51:38 | INFO | \"  0:01:37.257050 <- Load FSE model time elapsed\"\n",
      "07:51:38 | INFO | \"Start training fse model...\"\n",
      "07:51:47 | INFO | \"Running inference on all POSTS...\"\n",
      "07:51:47 | INFO | \"  Inference + convert to df...\"\n",
      "07:51:55 | INFO | \"  0:00:07.726996 <- Raw inference+df only time elapsed\"\n",
      "07:51:55 | INFO | \"    (111669, 300) <- Raw vectorized text shape\"\n",
      "07:51:55 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:51:55 | INFO | \"  Setting post_id as index...\"\n",
      "07:51:56 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:51:56 | INFO | \"  0:00:00.322830 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:51:56 | INFO | \"  0:00:08.454281 <- Converting vectors to df FULL time elapsed\"\n",
      "07:51:56 | INFO | \"Saving inference for comments df\"\n",
      "07:51:59 | INFO | \"  Saving inference complete\"\n",
      "07:51:59 | INFO | \"Get vectors for comments\"\n",
      "07:52:00 | INFO | \"  Inference + convert to df...\"\n",
      "07:52:49 | INFO | \"  0:00:49.124738 <- Raw inference+df only time elapsed\"\n",
      "07:52:49 | INFO | \"    (638052, 300) <- Raw vectorized text shape\"\n",
      "07:52:49 | INFO | \"  Creating df from dict_index_to_id...\"\n",
      "07:52:49 | INFO | \"  Setting comment_id as index...\"\n",
      "07:52:53 | INFO | \"  Merging df_vectors with df to get new index columns...\"\n",
      "07:52:55 | INFO | \"  0:00:02.418618 <-  Merging df_vect with ID columns time elapsed\"\n",
      "07:52:55 | INFO | \"  0:00:55.369872 <- Converting vectors to df FULL time elapsed\"\n",
      "07:52:55 | INFO | \"  0:00:55.833132 <- Inference time for COMMENTS time elapsed\"\n",
      "07:52:55 | INFO | \"Save vectors for comments\"\n",
      "07:53:14 | INFO | \"  0:03:31.067865 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model, df_posts, d_ix_to_id\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "\n",
    "mlflow.end_run(status='KILLED')\n",
    "model, df_posts, d_ix_to_id = vectorize_text_to_embeddings(\n",
    "    mlflow_experiment=mlflow_experiment,\n",
    "    \n",
    "    tokenize_function='gensim',\n",
    "    tokenize_lowercase=False,\n",
    "    train_min_word_count=4,\n",
    "    train_exclude_duplicated_docs=True,\n",
    "    train_subreddits_to_exclude=['wixbros', 'katjakrasavicenudes',\n",
    "                                 'deutschetributes', 'germannudes',\n",
    "                                 'annitheduck', 'germanonlyfans',\n",
    "                                 'loredana', 'nicoledobrikovof',\n",
    "                                 'germansgonewild', 'elisaalinenudes',\n",
    "                                 'marialoeffler', 'germanwomenandcouples',\n",
    "                                ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041b0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c493cb25",
   "metadata": {},
   "source": [
    "# Recover artifact from mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "532469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 'aac3e007dfc2446790e25887adf287f6'\n",
    "run = mlflow.get_run(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02daed2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://i18n-subreddit-clustering/mlflow/mlruns/4/aac3e007dfc2446790e25887adf287f6/artifacts/d_ix_to_id/d_ix_to_id.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62fe902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111669, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_index</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>t3_mkyj2k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>t3_mkynzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>t3_mkyolv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>t3_mkyp17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>t3_mkyqrz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   training_index    post_id\n",
       "0               0  t3_mkyj2k\n",
       "1               1  t3_mkynzi\n",
       "2               2  t3_mkyolv\n",
       "3               3  t3_mkyp17\n",
       "4               4  t3_mkyqrz"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idx = pd.read_csv(f\"{run.info.artifact_uri}/d_ix_to_id/d_ix_to_id.csv\")\n",
    "print(df_idx.shape)\n",
    "df_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd0244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
