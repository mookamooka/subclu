{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaeded5",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "\n",
    "**2023-04-11**:<br>\n",
    "<br> In this notebook we'll try an end-to-end process to get ANN for 2 different subreddits.\n",
    "\n",
    "- Pull data:\n",
    "    - User Time on Subreddit (save to local now, GCS in prod)\n",
    "    - Get subreddit-level embeddings (save to local, GCS in prod)\n",
    "- Reshape to get user-embeddings\n",
    "    - _(Load/cache files to local drive to speed up processing in prod)_\n",
    "    - Expand subreddit-level embeddings & save as a new file\n",
    "    - Expand ToSub to long format & save to file\n",
    "    - For each ToSub file, get weighted embeddings for a user\n",
    "        - Save user-level (weighted) embeddings to file\n",
    "- Calculate ANN distance to input subreddit(s)\n",
    "    - Load all user-embeddings & create an index with ANNOY\n",
    "    - Load subreddit-embedding file\n",
    "    - For each input target subreddit, get the ANN users\n",
    "        - Try nearest 3 million users\n",
    "        - **`TODO(djb)`**: how do I ensure that there are at least 100k users from each target country?\n",
    "    - Save raw ANN file\n",
    "- Reshape ANN table & upload to BQ table:\n",
    "    - Reshape ANN file for BigQuery format\n",
    "    - Upload ANN file to BQ for table creation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e7dbd",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c44a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f3db76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.13.1\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "polars\t\tv: 0.17.1\n",
      "plotly\t\tv: 5.11.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "subclu\t\tv: 0.6.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "from logging import info\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import dask\n",
    "import mlflow\n",
    "\n",
    "import subclu\n",
    "from subclu.eda.aggregates import compare_raw_v_weighted_language\n",
    "from subclu.utils import set_working_directory, get_project_subfolder\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric, reorder_array,\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl, \n",
    "    get_color_dict, base_colors_for_manual_labels,\n",
    "    check_colors_used,\n",
    ")\n",
    "from subclu.data.data_loaders import LoadPosts, LoadSubreddits, create_sub_level_aggregates\n",
    "\n",
    "\n",
    "# ===\n",
    "# imports specific to this notebook\n",
    "from google.cloud import bigquery\n",
    "from subclu.pn_models import get_data\n",
    "\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, pl, plotly, mlflow, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91525065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c7000",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc243c47",
   "metadata": {},
   "source": [
    "## Get subreddit-level embeddings\n",
    "\n",
    "For this part BigQuery + pandas is fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7702ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 1489.45query/s]\n",
      "Downloading: 100%|██████████| 242345/242345 [00:02<00:00, 83159.14rows/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 673 ms, sys: 494 ms, total: 1.17 s\n",
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery df_sub_emb_raw --project data-science-prod-218515 \n",
    "\n",
    "-- This aggregation is a pain to unnest, but it's the best we have (can't trust UNNESTING in SQL becuse order is not guaranteed)\n",
    "SELECT\n",
    "  subreddit_id\n",
    "  , subreddit_name\n",
    "  , ARRAY_CONCAT(embedding) AS embedding\n",
    "FROM `data-prod-165221.ml_content.subreddit_embeddings_ft2`\n",
    "WHERE DATE(pt) = '2023-04-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c887aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 242345 entries, 0 to 242344\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   subreddit_id    242345 non-null  object\n",
      " 1   subreddit_name  242345 non-null  object\n",
      " 2   embedding       242345 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_sub_emb_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be3f8a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_4f57cs</td>\n",
       "      <td>fuckangelhernandez</td>\n",
       "      <td>[-0.3996698260307312, 0.20269957184791565, -0.3055022060871124, -0.005287058185786009, 0.121136873960495, 0.06231916323304176, -0.37654876708984375, 0.48437756299972534, -0.2151433378458023, 0.31947633624076843, 0.4782879948616028, 0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_2s2gt</td>\n",
       "      <td>fuckapple</td>\n",
       "      <td>[-0.0029481418896466494, 0.053129445761442184, -0.17849114537239075, -0.1555848866701126, -0.20715922117233276, 0.2648947238922119, 0.24177874624729156, 0.23757821321487427, 0.011948454193770885, 0.3712220788002014, 0.3585907518863678, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_7s1lcv</td>\n",
       "      <td>fuckaround_n_findout</td>\n",
       "      <td>[-0.25010135769844055, -0.05121876671910286, -0.4104999303817749, 0.17199638485908508, -0.2111128717660904, -0.01367927622050047, -0.41924068331718445, 0.15966467559337616, 0.1576160341501236, 0.398811936378479, 0.3563167154788971, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5_2mpk27</td>\n",
       "      <td>fuckautismspeaks</td>\n",
       "      <td>[-0.643264889717102, -0.3077889084815979, -0.8766250610351562, 0.34814009070396423, -0.3688446581363678, -0.09676993638277054, -0.01420139241963625, 0.11278355121612549, 0.2917800843715668, 0.48437413573265076, -0.09958196431398392, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5_5rkktn</td>\n",
       "      <td>fuckbagin</td>\n",
       "      <td>[0.09801478683948517, -0.26190200448036194, 0.09046468883752823, -0.18976961076259613, -0.06223154440522194, 0.34227171540260315, -0.07571414113044739, 0.3605010509490967, 0.013768891803920269, 0.4096928536891937, 0.27020058035850525, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id        subreddit_name                                                                                                                                                                                                                                        embedding\n",
       "0    t5_4f57cs    fuckangelhernandez  [-0.3996698260307312, 0.20269957184791565, -0.3055022060871124, -0.005287058185786009, 0.121136873960495, 0.06231916323304176, -0.37654876708984375, 0.48437756299972534, -0.2151433378458023, 0.31947633624076843, 0.4782879948616028, 0.23...\n",
       "1     t5_2s2gt             fuckapple  [-0.0029481418896466494, 0.053129445761442184, -0.17849114537239075, -0.1555848866701126, -0.20715922117233276, 0.2648947238922119, 0.24177874624729156, 0.23757821321487427, 0.011948454193770885, 0.3712220788002014, 0.3585907518863678, ...\n",
       "2    t5_7s1lcv  fuckaround_n_findout  [-0.25010135769844055, -0.05121876671910286, -0.4104999303817749, 0.17199638485908508, -0.2111128717660904, -0.01367927622050047, -0.41924068331718445, 0.15966467559337616, 0.1576160341501236, 0.398811936378479, 0.3563167154788971, 0.07...\n",
       "3    t5_2mpk27      fuckautismspeaks  [-0.643264889717102, -0.3077889084815979, -0.8766250610351562, 0.34814009070396423, -0.3688446581363678, -0.09676993638277054, -0.01420139241963625, 0.11278355121612549, 0.2917800843715668, 0.48437413573265076, -0.09958196431398392, -0....\n",
       "4    t5_5rkktn             fuckbagin  [0.09801478683948517, -0.26190200448036194, 0.09046468883752823, -0.18976961076259613, -0.06223154440522194, 0.34227171540260315, -0.07571414113044739, 0.3605010509490967, 0.013768891803920269, 0.4096928536891937, 0.27020058035850525, -..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_emb_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cf143",
   "metadata": {},
   "source": [
    "## Get Time-on-Subreddit (ToS) for target user IDs\n",
    "\n",
    "We'll use these to create weighted embeddings.\n",
    "\n",
    "I'm caching the `selected users` in a table so that it's easier/faster to re-use while testing.\n",
    "\n",
    "The time-on-subreddit feature is already in gazette features so we don't need to waste time computing them.\n",
    "\n",
    "Key tables:\n",
    "- Select users:\n",
    "    - `data-prod-165221.fact_tables.post_consume_post_detail_view_events` \n",
    "- Get ToS:\n",
    "    - `data-prod-165221.user_feature_platform.time_on_subreddit_pct_time_over_30_day_v1`\n",
    "    \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53ed2f",
   "metadata": {},
   "source": [
    "Pulling the raw data (1-row per user )\n",
    "Time ETAs:\n",
    "\n",
    "| Description | 0.85 Million Users  | 9.8 Million Users |\n",
    "|---|---|---|\n",
    "| Query & load 1-row per user in python  | 37 seconds  | n/a |\n",
    "| Explode to exploded (1-row per user<>subreddit)  | 5 minutes (in Python)  | 1 minute (in BigQuery) |\n",
    "| Export exploded to GCS | n/a  | 1 minute (in BigQuery. 1,500 files) |\n",
    "| Download from GCS  | n/a  | 1 minute (with `gsutil`) |\n",
    "| Load from Local  | n/a  | 15 **seconds** (with `pl.read_parquet()`) |\n",
    "| Total Time  | ~6 minutes | ~3 minutes  |\n",
    "| Total Rows  | 130,601,209 | 851,107,537  |\n",
    "\n",
    "**Note** that for the new format I'm excluding ToS that is less than 0.0001 (0.01 %).\n",
    "- For 9.8 million users: reduces from 1.1 **B**illion rows to 851 **M**illion\n",
    "\n",
    "--- \n",
    "Load data from GCS because loading straight from BQ takes too long (stopped after waiting for 30 minutes). \n",
    "\n",
    "```bash\n",
    "# Copy: the -n flag only copies \"NEW\" files\n",
    "# !gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m cp -r -n $remote_gs_path $local_f\n",
    "\n",
    "# rsync can be nicer because with `-n` you get a preview (dry run) before running a command\n",
    "# !gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m rsync -r -n $gcs_tos $local_path_tos\n",
    "```\n",
    "\n",
    "Note that it's better to run this command from the CLI because it can crash jupyter (fast log info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e8d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS URI:\n",
      "  gs://i18n-subreddit-clustering/pn_model/runs/20230413/user_tos_30_pct/\n",
      "Local path:\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/pn_model/runs/20230413/user_tos_30_pct\n",
      "\n",
      "CLI command:\n",
      "gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m rsync -r -n gs://i18n-subreddit-clustering/pn_model/runs/20230413/user_tos_30_pct/ /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/pn_model/runs/20230413/user_tos_30_pct\n"
     ]
    }
   ],
   "source": [
    "gcs_tos = \"gs://i18n-subreddit-clustering/pn_model/runs/20230413/user_tos_30_pct/\"\n",
    "\n",
    "path_local_cache = Path(\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/\")\n",
    "local_path_tos = path_local_cache / \"pn_model/runs/20230413/user_tos_30_pct/\"\n",
    "Path.mkdir(local_path_tos, exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "print(f\"GCS URI:\\n  {gcs_tos}\\nLocal path:\\n  {local_path_tos}\")\n",
    "\n",
    "print(f\"\\nCLI command:\")\n",
    "!echo gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m rsync -r -n $gcs_tos $local_path_tos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda7493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# bq_user_tos = \"\"\"\n",
    "# SELECT\n",
    "#     *\n",
    "# FROM `reddit-employee-datasets.david_bermejo.pn_test_user_tos_30_pct_20230413`\n",
    "# LIMIT 10000\n",
    "# ;\n",
    "# \"\"\"\n",
    "\n",
    "# info(f\"Start query...\")\n",
    "# df_user_tos_sample = pl.from_arrow(client.query(bq_user_tos).to_arrow())\n",
    "# info(f\"Complete query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_user_tos = pl.read_parquet(f\"{local_path_tos}/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f887d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_tos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af34ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_user_tos.sample(n=int(1e6)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_tos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc92bf",
   "metadata": {},
   "source": [
    "# Transform - sub embeddings to wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb_cols = df_sub_emb_raw['embedding'].iloc[0].shape[0]\n",
    "assert np.alltrue(n_emb_cols == df_sub_emb_raw['embedding'].str.len())\n",
    "\n",
    "\n",
    "embedding_col_prefix = 'embedding_'\n",
    "l_emb_cols = [f\"{embedding_col_prefix}{i:03,.0f}\" for i in range(n_emb_cols)]\n",
    "print(len(l_emb_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_sub_emb_pd = pd.DataFrame(\n",
    "    list(df_sub_emb_raw['embedding']),\n",
    "    columns=l_emb_cols,\n",
    "    index=pd.MultiIndex.from_frame(df_sub_emb_raw[['subreddit_id', 'subreddit_name']]),\n",
    ").reset_index()\n",
    "\n",
    "print(df_sub_emb_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e85492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_sub_emb_pd = pd.DataFrame(\n",
    "    list(df_sub_emb_raw['embedding']),\n",
    "    columns=l_emb_cols,\n",
    "    index=pd.MultiIndex.from_frame(df_sub_emb_raw[['subreddit_id', 'subreddit_name']]),\n",
    ").reset_index()\n",
    "\n",
    "print(df_sub_emb_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_emb_pd.iloc[:5, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# convert it to polars to speed up processing\n",
    "df_sub_emb = pl.from_pandas(df_sub_emb_pd)\n",
    "\n",
    "print(df_sub_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_emb[:5, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aeb477",
   "metadata": {},
   "source": [
    "# Other Transforms \n",
    "\n",
    "We don't need these transforms for ANN, but will use it for modeling step\n",
    "- Count of subreddits per user for ToS\n",
    "\n",
    "Other data:\n",
    "- Subscribed to target subreddit\n",
    "- Age of account\n",
    "- Activity in L14 days\n",
    "- etc...\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c0d9c",
   "metadata": {},
   "source": [
    "# Compute user-level weighted embedding\n",
    "\n",
    "now that we have the long df, let's compute the user-level weighted embedding (multiply weights x embedding & groupby user-id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358fe46",
   "metadata": {},
   "source": [
    "## Test on a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ef5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_users = df_user_tos['user_id'].sample(n=int(1e4)).unique().sort()[:500]\n",
    "print(sample_users.shape)\n",
    "sample_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c14e13",
   "metadata": {},
   "source": [
    "TODO(djb): There are subreddits w/o embeddings.. what do we do about those?\n",
    "\n",
    "- For now, I'm dropping them\n",
    "- Potentially could fillnulls with the `median` subreddit\n",
    "    - Take the median of all subreddits and `.fillna` with the median vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e000b10",
   "metadata": {},
   "source": [
    "### Eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "# Use .join() instead of .filter()!! .filter() is like 8x slower!! (2 secs instead of 16 secs)\n",
    "df_user_emb_tmp_test = (\n",
    "    df_user_tos\n",
    "    .join(\n",
    "        pl.DataFrame(sample_users),\n",
    "        how='inner',\n",
    "        on=['user_id'],\n",
    "    )\n",
    "    # .filter(\n",
    "    #     # Use .join() instead of .filter()!! .filter() is like 8x slower!!\n",
    "    #     df_user_tos['user_id'].is_in(sample_users)\n",
    "    # )\n",
    "    .join(\n",
    "        df_sub_emb.drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_test.shape} <- df.shape ToS with raw embeddings (no nulls)\")\n",
    "\n",
    "info(f\"Compute weighted avg embedding for user-level\")\n",
    "df_user_emb_tmp_wt_test = (\n",
    "    df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "    .with_columns(\n",
    "        pl.DataFrame(\n",
    "            data=((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "            schema=l_emb_cols,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_wt_test.shape} <- df.shape ToS with weighted embedddings\")\n",
    "\n",
    "\n",
    "info(f\"Compute AGGREGATED user-level embedding...\")\n",
    "df_user_emb_test = (\n",
    "    df_user_emb_tmp_wt_test\n",
    "    .groupby(\n",
    "        ['user_id']\n",
    "    )\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(l_emb_cols).mean()\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "info(f\"{df_user_emb_test.shape} <- df.shape User weighted embedddings\")\n",
    "print(df_user_emb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO(djb) Could I re-write a single, long chain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae091c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_user_emb_test[:5, :15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaaac32",
   "metadata": {},
   "source": [
    "## Run on all users\n",
    "\n",
    "Batches of 150-million user<>subreddit weighted embeddings was too much memory (over 1.2 TB). So the kernel gets killed.\n",
    "\n",
    "```bash\n",
    "# 800k users | 130 million ToS rows\n",
    "07:08:08 | INFO | \"Append subreddit embeddings to user ToS\"\n",
    "07:08:13 | INFO | \"Compute weighted avg embedding for user-level\"\n",
    "07:09:49 | INFO | \"Compute AGGREGATED user-level embedding...\"\n",
    "(130601209, 100)\n",
    "(856211, 101)\n",
    "CPU times: user 18min 9s, sys: 8min 21s, total: 26min 31s\n",
    "Wall time: 4min 52s\n",
    "\n",
    "\n",
    "# 9.8 million users |  million ToS rows\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ad494",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# When computing on full data set, we need to batch b/c the computer will run out of RAM if/when converting parquet to numpy\n",
    "n_sample = int(10e6)\n",
    "batch_size = int(1e6)  # 9e6 = 9Million\n",
    "# iter_chunks = range(1 + len(df_user_tos) // batch_size)\n",
    "iter_chunks = range(1 + n_sample // batch_size)\n",
    "info(f\"{n_sample:09,.0f} <- Sample\")\n",
    "info(f\"{batch_size:09,.0f} <- Batch Size\")\n",
    "\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp = (\n",
    "    df_user_tos\n",
    "    .sample(n=n_sample, seed=42)\n",
    "    .join(\n",
    "        df_sub_emb.drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp.shape} <- df.shape ToS with raw embeddings (no nulls)\")\n",
    "gc.collect()\n",
    "\n",
    "info(f\"Compute weighted avg embedding for user-level\")\n",
    "df_user_emb_tmp_wt = (\n",
    "    pl.select(\n",
    "        df_user_emb_tmp[['user_id', 'subreddit_id']]\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.DataFrame(\n",
    "            data=((df_user_emb_tmp['tos_30_pct'].to_numpy()) * df_user_emb_tmp[l_emb_cols].to_numpy().T).T,\n",
    "            schema=l_emb_cols,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_wt.shape} <- df.shape ToS with weighted embedddings\")\n",
    "gc.collect()\n",
    "\n",
    "info(f\"Compute AGGREGATED user-level embedding...\")\n",
    "df_user_emb = (\n",
    "    df_user_emb_tmp_wt\n",
    "    .groupby(\n",
    "        ['user_id']\n",
    "    )\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(l_emb_cols).mean()\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "info(f\"{df_user_emb.shape} <- df.shape User weighted embedddings\")\n",
    "gc.collect()\n",
    "print(df_user_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# when computing on full data set, we need to batch b/c the computer will run out of RAM if/when converting parquet to numpy\n",
    "n_sample = None  # None  int(10e6)\n",
    "\n",
    "if n_sample is None:\n",
    "    df_user_tos_proc_ = df_user_tos\n",
    "else:\n",
    "    df_user_tos_proc_ = df_user_tos.sample(n=n_sample, seed=42)\n",
    "n_size = len(df_user_tos_proc_)\n",
    "\n",
    "batch_size = int(110e6)  # int(150e6), 9e6 = 9Million\n",
    "n_chunks = 1 + n_size // batch_size\n",
    "iter_chunks = range(n_chunks)\n",
    "\n",
    "info(f\"{n_size:09,.0f} <- Data Size\")\n",
    "info(f\"{batch_size:09,.0f} <- Batch Size\")\n",
    "info(f\"{n_chunks:09,.0f} <- Chunks\")\n",
    "\n",
    "\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp = (\n",
    "    df_user_tos_proc_\n",
    "    .join(\n",
    "        df_sub_emb.drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp.shape} <- df.shape ToS with raw embeddings (no nulls)\")\n",
    "gc.collect()\n",
    "\n",
    "info(f\"Create DAG for weighted avg embedding per user+subreddit\")\n",
    "# Use dask.delayed() to compute these in parallel & prevent some OOM errors\n",
    "l_dfs_tos_wt = list()\n",
    "for i in tqdm(iter_chunks, mininterval=0.1, ascii=True):\n",
    "    l_dfs_tos_wt.append(\n",
    "        dask.delayed(pl.select)(\n",
    "            df_user_emb_tmp[['user_id', 'subreddit_id']]\n",
    "            [i * batch_size:(i + 1) * batch_size]\n",
    "        )\n",
    "        .with_columns(\n",
    "            dask.delayed(pl.DataFrame)(\n",
    "                data=(\n",
    "                    dask.delayed(np.multiply)(\n",
    "                        dask.delayed(df_user_emb_tmp['tos_30_pct'][i * batch_size:(i + 1) * batch_size].to_numpy()),\n",
    "                        dask.delayed(df_user_emb_tmp[l_emb_cols][i * batch_size:(i + 1) * batch_size].to_numpy().T)\n",
    "                    ).T\n",
    "                ),\n",
    "                schema=l_emb_cols,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "df_user_emb_tmp_wt_del = dask.delayed(pl.concat)(\n",
    "    l_dfs_tos_wt, how='vertical', parallel=True\n",
    ")\n",
    "info(f\"Dask visualize DAG\")\n",
    "display(df_user_emb_tmp_wt_del.visualize(\n",
    "    f\"dask_DAG-{n_size}_data-{batch_size}_batch_size-{n_chunks}_chunks.png\"\n",
    "))\n",
    "gc.collect()\n",
    "info(f\"Compute weighted ToS embeddings...\")\n",
    "df_user_emb_tmp_wt = df_user_emb_tmp_wt_del.compute()\n",
    "info(f\"{df_user_emb_tmp_wt.shape} <- df.shape ToS with weighted embedddings\")\n",
    "\n",
    "info(f\"Delete temp df to free up memory\")\n",
    "del df_user_emb_tmp\n",
    "gc.collect()\n",
    "\n",
    "info(f\"Compute AGGREGATED user-level embedding...\")\n",
    "df_user_emb = (\n",
    "    df_user_emb_tmp_wt\n",
    "    .groupby(\n",
    "        ['user_id']\n",
    "    )\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(l_emb_cols).mean()\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "info(f\"{df_user_emb.shape} <- df.shape User weighted embedddings\")\n",
    "gc.collect()\n",
    "print(df_user_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fe785",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "r_, c_ = df_user_emb.shape\n",
    "\n",
    "df_user_emb.write_parquet(\n",
    "    f\"djb_test_df_user_embeddings-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    "    f\"-{r_}_by_{c_}-IGNORE.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_users_sample = df_user_tos['user_id'].n_unique()\n",
    "info(f\"{n_users_sample:,.0f} Users in run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3940a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:00:43 | INFO | \"1.04  Hours to get user-embeddings SEQUENTIALLY\"\n"
     ]
    }
   ],
   "source": [
    "n_mins_sample = 15.5\n",
    "n_users_target = 40e6\n",
    "\n",
    "seq_proc_time = n_mins_sample * (n_users_target / n_users_sample) / 60\n",
    "info(f\"{seq_proc_time:,.2f}  Hours to get user-embeddings SEQUENTIALLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c576624",
   "metadata": {},
   "source": [
    "# Other tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678bb37d",
   "metadata": {},
   "source": [
    "## compare creating weighted matrix in pandas v. polars\n",
    "polars is faster. but part of it might be that I'm creating an index and then resetting it...\n",
    "\n",
    "in any case, sticking to polars as much as possible is probably better for consistency and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "862d7c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:32:34 | INFO | \"Append subreddit embeddings to user ToS\"\n",
      "06:32:36 | INFO | \"(67217, 103) <- df.shape ToS with raw embeddings (no nulls)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 18s, sys: 12.1 s, total: 2min 30s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp_test = (\n",
    "    df_user_tos\n",
    "    .join(\n",
    "        pl.DataFrame(sample_users),\n",
    "        how='inner',\n",
    "        on=['user_id'],\n",
    "    )\n",
    "    .join(\n",
    "        df_sub_emb.drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_test.shape} <- df.shape ToS with raw embeddings (no nulls)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a98c1ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67217, 102)\n",
      "CPU times: user 337 ms, sys: 248 ms, total: 585 ms\n",
      "Wall time: 69.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_pl = (\n",
    "    df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "    .with_columns(\n",
    "        pl.DataFrame(\n",
    "            data=((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "            schema=l_emb_cols,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f657ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67217, 102)\n",
      "CPU times: user 423 ms, sys: 42.5 ms, total: 466 ms\n",
      "Wall time: 88.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_pd = pd.DataFrame(\n",
    "    ((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "    index=pd.MultiIndex.from_frame(df_user_emb_tmp_test[['user_id', 'subreddit_id']].to_pandas()),\n",
    "    columns=l_emb_cols,\n",
    ").reset_index()\n",
    "print(_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8098ea7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pd.equals(_pl.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eee389ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.5 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "_pl = (\n",
    "    df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "    .with_columns(\n",
    "        pl.DataFrame(\n",
    "            data=((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "            schema=l_emb_cols,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef341809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.3 ms ± 820 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_pd = pd.DataFrame(\n",
    "    ((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "    index=pd.MultiIndex.from_frame(df_user_emb_tmp_test[['user_id', 'subreddit_id']].to_pandas()),\n",
    "    columns=l_emb_cols,\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e525cd",
   "metadata": {},
   "source": [
    "## Lazy calculation for weighted embeddings \n",
    "Lazy give us less info in between stages, but it *could* be faster (in theory) because the query optimizer would have more/better info for the whole DAG.\n",
    "\n",
    "However, NVM for now. I'm using numpy in the middle to get the weighted values, so we need eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4ff69aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# info(f\"Append subreddit embeddings to user ToS\")\n",
    "# df_user_emb_tmp_test = (\n",
    "#     df_user_tos\n",
    "#     .lazy()\n",
    "#     .filter(\n",
    "#         df_user_tos['user_id'].is_in(sample_users)\n",
    "#     )\n",
    "#     .join(\n",
    "#         df_sub_emb.drop('subreddit_name').lazy(),\n",
    "#         how='left',\n",
    "#         on=['subreddit_id']\n",
    "#     )\n",
    "#     # Drop or replace nulls as early as possible\n",
    "#     .drop_nulls()\n",
    "# )\n",
    "# # Can't get shape of a lazy df\n",
    "# # info(f\"{df_user_emb_tmp_test.shape} <- df.shape ToS with raw embeddings (no nulls)\")\n",
    "\n",
    "# info(f\"Compute weighted avg embedding for user-level\")\n",
    "# df_user_emb_tmp_wt_test = (\n",
    "#     df_user_emb_tmp_test\n",
    "#     .select(['user_id', 'subreddit_id'])\n",
    "#     .with_columns(\n",
    "#         pl.DataFrame(\n",
    "#             data=((df_user_emb_tmp_test.select(['tos_30_pct']).to_numpy()) * df_user_emb_tmp_test.select([l_emb_cols]).to_numpy().T).T,\n",
    "#             schema=l_emb_cols,\n",
    "#         ).lazy()\n",
    "#     )\n",
    "# )\n",
    "# # info(f\"{df_user_emb_tmp_wt_test.shape} <- df.shape ToS with weighted embedddings\")\n",
    "\n",
    "\n",
    "# info(f\"Compute AGGREGATED user-level embedding...\")\n",
    "# df_user_emb_test = (\n",
    "#     pl.DataFrame(df_user_emb_tmp_wt_test_pd)\n",
    "#     .groupby(\n",
    "#         ['user_id']\n",
    "#     )\n",
    "#     .agg(\n",
    "#         [\n",
    "#             pl.col(l_emb_cols).mean()\n",
    "#         ]\n",
    "#     )\n",
    "#     .collect()\n",
    "# )\n",
    "# info(f\"{df_user_emb_test.shape} <- df.shape User weighted embedddings\")\n",
    "# print(df_user_emb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4fbb16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 8.84 s, total: 20.5 s\n",
      "Wall time: 3.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8982315, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_mult = np.multiply(\n",
    "    df_user_emb_tmp['tos_30_pct'].to_numpy(), df_user_emb_tmp[l_emb_cols].to_numpy().T\n",
    ").T\n",
    "test_mult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf7110e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 8.58 s, total: 20.2 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_mult_ = dask.delayed(np.multiply)(\n",
    "    dask.delayed(df_user_emb_tmp['tos_30_pct'].to_numpy()), \n",
    "    dask.delayed(df_user_emb_tmp[l_emb_cols].to_numpy().T)\n",
    ").T\n",
    "test_mult_.visualize()\n",
    "test_mult_del_ = test_mult_.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d58c63aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982315, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mult_del_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "516462a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 10.8 s, total: 30.1 s\n",
      "Wall time: 12.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.allclose(\n",
    "    test_mult,\n",
    "    df_user_emb_tmp_wt[l_emb_cols].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b940936",
   "metadata": {},
   "source": [
    "### We would need the weight computation in polars instead of numpy\n",
    "\n",
    "\n",
    "a) I tried using .transpose() and multiplying, but for some reason this results in `Panic` errors.\n",
    "\n",
    "b) Exploding & aggregating _could_ be faster (maybe) but in reality we lose a lot of time exploding & re-aggregating.\n",
    "\n",
    "- https://stackoverflow.com/questions/74372173/python-polars-how-to-multiply-each-element-in-a-list-with-a-value-in-a-differen\n",
    "- https://stackoverflow.com/questions/72490297/rust-polars-is-it-possible-to-explode-a-list-column-into-multiple-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c355f",
   "metadata": {},
   "source": [
    "#### A - Transpose (breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18098b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 67217)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_emb_tmp_test[l_emb_cols].transpose().shape  # [:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a10fedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67217,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_emb_tmp_test['tos_30_pct'].shape  # [:50] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## this one uses all cores for a while, but doesn't finish processsing (I've had to shut down the kernel)\n",
    "pl.DataFrame(\n",
    "            data=((df_user_emb_tmp['tos_30_pct']) * df_user_emb_tmp[l_emb_cols].transpose()).transpose(),\n",
    "            schema=l_emb_cols,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf3b79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose multiplication works for a subset of rows (50, 5000), but it stops working for some reason\n",
    "#. when we try on all rows\n",
    "\n",
    "# df_user_emb_tmp_test['tos_30_pct'][:5000] * df_user_emb_tmp_test[l_emb_cols].transpose()[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec04a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # try with only polars (skip .to_numpy)\n",
    "\n",
    "# _pl = (\n",
    "#     df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "#     .with_columns(\n",
    "#         pl.DataFrame(\n",
    "#             data=((df_user_emb_tmp_test['tos_30_pct']) * df_user_emb_tmp_test[l_emb_cols].transpose()).transpose(),\n",
    "#             schema=l_emb_cols,\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "# print(_pl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727e9a7",
   "metadata": {},
   "source": [
    "#### B - Explode + Aggregate (too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eaf6d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:26:06 | INFO | \"Append subreddit embeddings to user ToS\"\n",
      "05:26:23 | INFO | \"(67217, 4) <- df.shape ToS with raw embeddings (no nulls)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 214 ms, total: 18 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp_test_pl = (\n",
    "    df_user_tos\n",
    "    .filter(\n",
    "        df_user_tos['user_id'].is_in(sample_users)\n",
    "    )\n",
    "    .join(\n",
    "        pl.DataFrame(df_sub_emb_raw).drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_test_pl.shape} <- df.shape ToS with raw embeddings (no nulls)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9039a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub_emb_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ca43866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 s, sys: 1.05 s, total: 4.97 s\n",
      "Wall time: 285 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (67217, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>subreddit_id</th><th>embedding_weighted</th></tr><tr><td>str</td><td>str</td><td>list[list[f64]]</td></tr></thead><tbody><tr><td>&quot;t2_100mfj&quot;</td><td>&quot;t5_2sa3m&quot;</td><td>[[-0.000067, 0.00002, … 0.000013]]</td></tr><tr><td>&quot;t2_100mfj&quot;</td><td>&quot;t5_2vz7o&quot;</td><td>[[0.000039, 0.000017, … 0.000075]]</td></tr><tr><td>&quot;t2_10cirt&quot;</td><td>&quot;t5_5pi0ey&quot;</td><td>[[-0.000163, -0.000038, … 0.000118]]</td></tr><tr><td>&quot;t2_10d6rk&quot;</td><td>&quot;t5_2qh87&quot;</td><td>[[-0.000071, 0.000045, … 0.000072]]</td></tr><tr><td>&quot;t2_10lgjt&quot;</td><td>&quot;t5_48t85&quot;</td><td>[[-0.000166, 0.00008, … 0.000018]]</td></tr><tr><td>&quot;t2_10jwj7&quot;</td><td>&quot;t5_2v0i1&quot;</td><td>[[-0.000375, 0.000083, … 0.000145]]</td></tr><tr><td>&quot;t2_10obml&quot;</td><td>&quot;t5_5m9f4s&quot;</td><td>[[-0.00058, 0.000096, … -0.000055]]</td></tr><tr><td>&quot;t2_10s6suic&quot;</td><td>&quot;t5_2qnwb&quot;</td><td>[[-0.000425, 0.000027, … 0.000334]]</td></tr><tr><td>&quot;t2_11418c&quot;</td><td>&quot;t5_2qhor&quot;</td><td>[[-0.000061, 0.000008, … 0.000122]]</td></tr><tr><td>&quot;t2_11onq0&quot;</td><td>&quot;t5_2uao3&quot;</td><td>[[-0.01, 0.004394, … -0.000414]]</td></tr><tr><td>&quot;t2_11gkiu&quot;</td><td>&quot;t5_2s7xv&quot;</td><td>[[-0.00138, 0.00169, … -0.000484]]</td></tr><tr><td>&quot;t2_113gbl&quot;</td><td>&quot;t5_2qig7&quot;</td><td>[[0.000063, 0.000166, … 0.000379]]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;t2_14u8aw7&quot;</td><td>&quot;t5_3auim&quot;</td><td>[[0.000136, 0.000261, … 0.000094]]</td></tr><tr><td>&quot;t2_14sw8s98&quot;</td><td>&quot;t5_2s5ei&quot;</td><td>[[-0.000091, -0.000059, … 0.000065]]</td></tr><tr><td>&quot;t2_14t2r9&quot;</td><td>&quot;t5_5bm4a4&quot;</td><td>[[-0.000093, 0.000066, … 0.000038]]</td></tr><tr><td>&quot;t2_14fevy&quot;</td><td>&quot;t5_2sxpk&quot;</td><td>[[-0.002865, -0.001526, … 0.000654]]</td></tr><tr><td>&quot;t2_151c1j&quot;</td><td>&quot;t5_2s8fe&quot;</td><td>[[-0.00004, 0.000262, … 0.000267]]</td></tr><tr><td>&quot;t2_14hz76&quot;</td><td>&quot;t5_2tpr6&quot;</td><td>[[-0.000334, 0.000136, … -0.000729]]</td></tr><tr><td>&quot;t2_14hz76&quot;</td><td>&quot;t5_2rgzp&quot;</td><td>[[-0.000031, 0.000021, … -0.000029]]</td></tr><tr><td>&quot;t2_14hrssg0&quot;</td><td>&quot;t5_56ibls&quot;</td><td>[[0.000038, 0.000032, … 0.000019]]</td></tr><tr><td>&quot;t2_153r3h&quot;</td><td>&quot;t5_2v28g&quot;</td><td>[[0.000046, 0.000324, … 0.000599]]</td></tr><tr><td>&quot;t2_153r3h&quot;</td><td>&quot;t5_pcxm7&quot;</td><td>[[0.000061, 0.000036, … 0.000011]]</td></tr><tr><td>&quot;t2_15duwx&quot;</td><td>&quot;t5_5gug25&quot;</td><td>[[-0.000092, -0.000202, … 0.000211]]</td></tr><tr><td>&quot;t2_15duwx&quot;</td><td>&quot;t5_2xbus&quot;</td><td>[[-0.00083, 0.000293, … 0.000502]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (67_217, 3)\n",
       "┌───────────┬──────────────┬───────────────────────────────────┐\n",
       "│ user_id   ┆ subreddit_id ┆ embedding_weighted                │\n",
       "│ ---       ┆ ---          ┆ ---                               │\n",
       "│ str       ┆ str          ┆ list[list[f64]]                   │\n",
       "╞═══════════╪══════════════╪═══════════════════════════════════╡\n",
       "│ t2_100mfj ┆ t5_2sa3m     ┆ [[-0.000067, 0.00002, … 0.000013… │\n",
       "│ t2_100mfj ┆ t5_2vz7o     ┆ [[0.000039, 0.000017, … 0.000075… │\n",
       "│ t2_10cirt ┆ t5_5pi0ey    ┆ [[-0.000163, -0.000038, … 0.0001… │\n",
       "│ t2_10d6rk ┆ t5_2qh87     ┆ [[-0.000071, 0.000045, … 0.00007… │\n",
       "│ …         ┆ …            ┆ …                                 │\n",
       "│ t2_153r3h ┆ t5_2v28g     ┆ [[0.000046, 0.000324, … 0.000599… │\n",
       "│ t2_153r3h ┆ t5_pcxm7     ┆ [[0.000061, 0.000036, … 0.000011… │\n",
       "│ t2_15duwx ┆ t5_5gug25    ┆ [[-0.000092, -0.000202, … 0.0002… │\n",
       "│ t2_15duwx ┆ t5_2xbus     ┆ [[-0.00083, 0.000293, … 0.000502… │\n",
       "└───────────┴──────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "c_emb_weighted = 'embedding_weighted'\n",
    "\n",
    "(\n",
    "    df_user_emb_tmp_test_pl  #.head()\n",
    "    # reshape to get weighted embeddings (long). Transpose the embeddings to long\n",
    "    .explode('embedding')\n",
    "    .with_columns(\n",
    "        **{c_emb_weighted: pl.col('tos_30_pct') * pl.col('embedding')}\n",
    "    )\n",
    "    # reshape back to get subreddit-level embedding. Transpose the embeddings back to wide\n",
    "    .groupby(['user_id', 'subreddit_id'])\n",
    "    .agg([\n",
    "        pl.col(c_emb_weighted).list(),\n",
    "        # pl.col('embedding').list()  # delete after testing\n",
    "    ])\n",
    "    # expand embeddings column-wise to get 1 col per embedding element \n",
    "    # get mean per user\n",
    "#     .groupby(\n",
    "#         ['user_id']\n",
    "#     )\n",
    "#     .agg(\n",
    "#         [\n",
    "#             pl.col(l_emb_cols).mean()\n",
    "#         ]\n",
    "#     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b9a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
