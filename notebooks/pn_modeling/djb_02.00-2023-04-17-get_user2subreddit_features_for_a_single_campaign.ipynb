{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a946c124",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "\n",
    "**2023-04-17**:<br>\n",
    "<br> In this notebook we'll try an end-to-end process to get ANN for a single campaign\n",
    "\n",
    "- Pull data:\n",
    "    - User Time on Subreddit (save to local now, GCS in prod)\n",
    "    - Get subreddit-level embeddings (save to local, GCS in prod)\n",
    "- Reshape to get user-embeddings\n",
    "    - For each ToSub file, get weighted embeddings for a user\n",
    "        - Save user-level (weighted) embeddings to file\n",
    "- Calculate ANN distance to input subreddit(s)\n",
    "    - Create an index with ANNOY: per COUNTRY + subreddit\n",
    "        - This way we ensure a minimum # of users per country\n",
    "    - Load subreddit-embedding file\n",
    "    - For each input target subreddit, get the ANN users\n",
    "        - Try nearest 3 million users\n",
    "    - Save raw ANN file\n",
    "- Save file ready for model training\n",
    "\n",
    "TODO later / out of scope\n",
    "- Split training file into k-fold validation or train/test\n",
    "- Train base models (do it a separate notebook)\n",
    "- [later/tbd] Reshape ANN table & upload to BQ table\n",
    "    - Reshape ANN file for BigQuery format\n",
    "    - Upload ANN file to BQ for table creation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6348d9d",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9f6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be3e730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "google.cloud.bigquery\tv: 2.13.1\n",
      "numpy\t\tv: 1.19.5\n",
      "pandas\t\tv: 1.2.4\n",
      "polars\t\tv: 0.17.1\n",
      "plotly\t\tv: 5.11.0\n",
      "mlflow\t\tv: 1.16.0\n",
      "subclu\t\tv: 0.6.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "import logging\n",
    "from logging import info\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import dask\n",
    "import mlflow\n",
    "\n",
    "import subclu\n",
    "from subclu.eda.aggregates import compare_raw_v_weighted_language\n",
    "from subclu.utils import set_working_directory, get_project_subfolder\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric, reorder_array,\n",
    ")\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.utils.data_irl_style import (\n",
    "    get_colormap, theme_dirl, \n",
    "    get_color_dict, base_colors_for_manual_labels,\n",
    "    check_colors_used,\n",
    ")\n",
    "from subclu.data.data_loaders import LoadPosts, LoadSubreddits, create_sub_level_aggregates\n",
    "\n",
    "\n",
    "# ===\n",
    "# imports specific to this notebook\n",
    "from google.cloud import bigquery\n",
    "from subclu.pn_models import get_data\n",
    "\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "print_lib_versions([bigquery, np, pd, pl, plotly, mlflow, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f76a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778d21d",
   "metadata": {},
   "source": [
    "# Define local path for this model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a3f211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/subreddit_clustering_i18n/data/models/pn_model/pn_manual_test_2023-04-17_161043\n"
     ]
    }
   ],
   "source": [
    "manual_model_timestamp = datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')\n",
    "path_this_model = get_project_subfolder(\n",
    "    f\"data/models/pn_model/pn_manual_test_{manual_model_timestamp}\"\n",
    ")\n",
    "Path.mkdir(path_this_model, parents=True, exist_ok=True)\n",
    "print(path_this_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677b901",
   "metadata": {},
   "source": [
    "# Define key inputs\n",
    "Use these throughout the process to filter/target specific subreddits, geos, & users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ee28568",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_target_subreddits = ['de']\n",
    "\n",
    "# We would want to add an automated way to pick these ANN subs, but look them up manually for now\n",
    "l_target_ann_subreddits = ['fragreddit', 'ich_iel']\n",
    "\n",
    "l_target_geos = ['DE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2918c9",
   "metadata": {},
   "source": [
    "# Get subreddit-level embeddings\n",
    "\n",
    "For this part BigQuery + pandas is fast enough.\n",
    "\n",
    "Model partition (`pt`) is generally trained on a Tuesday.\n",
    "\n",
    "ETA: \n",
    "- ~ 20 seconds to pull the data\n",
    "- ~15 seconds to conver to wide format\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "pt\n",
    "0\t2022-11-22\n",
    "1\t2022-11-29\n",
    "2\t2022-12-06\n",
    "3\t2022-12-13\n",
    "4\t2022-12-20\n",
    "5\t2022-12-27\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a43f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Query is running:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query executing stage S00: Input and status COMPLETE : 0.00s:   0%|          | 0/2 [00:00<?, ?query/s]\u001b[A\n",
      "Query complete after 0.00s: 100%|██████████| 2/2 [00:00<00:00, 651.14query/s]                         \u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/236115 [00:00<?, ?rows/s]\u001b[A\n",
      "Downloading:   0%|          | 1024/236115 [00:00<03:35, 1092.05rows/s]\u001b[A\n",
      "Downloading:   3%|▎         | 6144/236115 [00:01<00:29, 7710.81rows/s]\u001b[A\n",
      "Downloading:  10%|▉         | 23552/236115 [00:01<00:06, 33326.75rows/s]\u001b[A\n",
      "Downloading:  17%|█▋        | 40960/236115 [00:01<00:03, 58582.00rows/s]\u001b[A\n",
      "Downloading:  25%|██▍       | 58368/236115 [00:01<00:02, 81529.48rows/s]\u001b[A\n",
      "Downloading:  32%|███▏      | 74752/236115 [00:01<00:01, 97983.22rows/s]\u001b[A\n",
      "Downloading:  39%|███▊      | 91136/236115 [00:01<00:01, 113646.75rows/s]\u001b[A\n",
      "Downloading:  46%|████▌     | 107520/236115 [00:01<00:01, 125917.21rows/s]\u001b[A\n",
      "Downloading:  53%|█████▎    | 124928/236115 [00:01<00:00, 134822.45rows/s]\u001b[A\n",
      "Downloading:  60%|█████▉    | 141312/236115 [00:01<00:00, 142546.31rows/s]\u001b[A\n",
      "Downloading:  67%|██████▋   | 157696/236115 [00:01<00:00, 148157.52rows/s]\u001b[A\n",
      "Downloading:  74%|███████▎  | 174080/236115 [00:02<00:00, 151062.06rows/s]\u001b[A\n",
      "Downloading:  81%|████████  | 191488/236115 [00:02<00:00, 154954.36rows/s]\u001b[A\n",
      "Downloading:  88%|████████▊ | 207872/236115 [00:02<00:00, 156632.69rows/s]\u001b[A\n",
      "Downloading: 100%|██████████| 236115/236115 [00:02<00:00, 82239.13rows/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 698 ms, sys: 413 ms, total: 1.11 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery df_sub_emb_raw --project data-science-prod-218515 \n",
    "\n",
    "-- We do the un-nesting in pandas because with UNNESTING in SQL ORDER IS NOT GUARANTEED!\n",
    "SELECT\n",
    "  subreddit_id\n",
    "  , subreddit_name\n",
    "  , ARRAY_CONCAT(embedding) AS embedding\n",
    "\n",
    "FROM `data-prod-165221.ml_content.subreddit_embeddings_ft2`\n",
    "WHERE DATE(pt) = '2022-11-29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4cfebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 236115 entries, 0 to 236114\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   subreddit_id    236115 non-null  object\n",
      " 1   subreddit_name  236115 non-null  object\n",
      " 2   embedding       236115 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_sub_emb_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06f414",
   "metadata": {},
   "source": [
    "## Transform - sub embeddings to wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93382050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:22:23 | INFO | \"100\"\n",
      "16:22:23 | INFO | \"Converting to wide format...\"\n",
      "16:22:36 | INFO | \"236,115, 102 <- embeddings WIDE shape\"\n",
      "16:22:36 | INFO | \"Save wide embeddings to parquet\"\n",
      "16:22:38 | INFO | \"Converting to polars df...\"\n",
      "16:22:38 | INFO | \"Subreddit embeddings pre-processing done\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 1.84 s, total: 15.8 s\n",
      "Wall time: 16.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_emb_cols = df_sub_emb_raw['embedding'].iloc[0].shape[0]\n",
    "assert np.alltrue(n_emb_cols == df_sub_emb_raw['embedding'].str.len())\n",
    "\n",
    "\n",
    "embedding_col_prefix = 'embedding_'\n",
    "l_emb_cols = [f\"{embedding_col_prefix}{i:03,.0f}\" for i in range(n_emb_cols)]\n",
    "info(f\"{len(l_emb_cols):,.0f} <- Embedding columns\")\n",
    "\n",
    "info(f\"Converting to wide format...\")\n",
    "df_sub_emb_pd = pd.DataFrame(\n",
    "    list(df_sub_emb_raw['embedding']),\n",
    "    columns=l_emb_cols,\n",
    "    index=pd.MultiIndex.from_frame(df_sub_emb_raw[['subreddit_id', 'subreddit_name']]),\n",
    ").reset_index()\n",
    "\n",
    "r_, c_ = df_sub_emb_pd.shape \n",
    "info(f\"{r_:,.0f}, {c_:,.0f} <- embeddings WIDE shape\")\n",
    "\n",
    "info(f\"Save wide embeddings to parquet\")\n",
    "df_sub_emb_pd.to_parquet(\n",
    "    path_this_model / f\"df_subreddit_embeddings_wide-{r_}_{c_}.parquet\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "info(f\"Converting to polars df...\")\n",
    "df_sub_emb = pl.from_pandas(df_sub_emb_pd)\n",
    "\n",
    "info(f\"Subreddit embeddings pre-processing done\")\n",
    "# delete the pandas df to free up memory\n",
    "del df_sub_emb_pd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f0bb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subreddit_id</th><th>subreddit_name</th><th>embedding_000</th><th>embedding_001</th><th>embedding_002</th><th>embedding_003</th><th>embedding_004</th><th>embedding_005</th><th>embedding_006</th><th>embedding_007</th><th>embedding_008</th><th>embedding_009</th><th>embedding_010</th><th>embedding_011</th><th>embedding_012</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;t5_70sqiq&quot;</td><td>&quot;00000009999&quot;</td><td>0.203673</td><td>0.41239</td><td>0.293499</td><td>0.219038</td><td>0.067274</td><td>-0.074245</td><td>-0.515791</td><td>0.701312</td><td>0.733369</td><td>-0.565209</td><td>-0.096566</td><td>0.40944</td><td>0.082867</td></tr><tr><td>&quot;t5_6z8n1c&quot;</td><td>&quot;0010110zion&quot;</td><td>0.020021</td><td>0.08691</td><td>0.152663</td><td>-0.412483</td><td>0.074653</td><td>-0.005074</td><td>-0.308147</td><td>0.06658</td><td>0.161424</td><td>-0.091454</td><td>-0.170465</td><td>-0.028941</td><td>-0.293559</td></tr><tr><td>&quot;t5_71a1re&quot;</td><td>&quot;002timmynfts&quot;</td><td>-0.066831</td><td>-0.024192</td><td>0.682037</td><td>-0.081226</td><td>0.046117</td><td>0.285765</td><td>-0.226684</td><td>0.041478</td><td>0.301822</td><td>-0.476785</td><td>0.068413</td><td>0.070616</td><td>0.234421</td></tr><tr><td>&quot;t5_6xknxn&quot;</td><td>&quot;007desinsfw&quot;</td><td>0.028375</td><td>-0.157933</td><td>0.722232</td><td>-0.22489</td><td>-0.123628</td><td>0.128553</td><td>0.227722</td><td>0.294614</td><td>-0.090551</td><td>-0.643633</td><td>0.347671</td><td>0.175447</td><td>0.225121</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 15)\n",
       "┌────────────┬────────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ subreddit_ ┆ subreddit_ ┆ embedding_ ┆ embedding_ ┆ … ┆ embedding_ ┆ embedding_ ┆ embedding_ ┆ embedding_ │\n",
       "│ id         ┆ name       ┆ 000        ┆ 001        ┆   ┆ 009        ┆ 010        ┆ 011        ┆ 012        │\n",
       "│ ---        ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ str        ┆ str        ┆ f64        ┆ f64        ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
       "╞════════════╪════════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ t5_70sqiq  ┆ 0000000999 ┆ 0.203673   ┆ 0.41239    ┆ … ┆ -0.565209  ┆ -0.096566  ┆ 0.40944    ┆ 0.082867   │\n",
       "│            ┆ 9          ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t5_6z8n1c  ┆ 0010110zio ┆ 0.020021   ┆ 0.08691    ┆ … ┆ -0.091454  ┆ -0.170465  ┆ -0.028941  ┆ -0.293559  │\n",
       "│            ┆ n          ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t5_71a1re  ┆ 002timmynf ┆ -0.066831  ┆ -0.024192  ┆ … ┆ -0.476785  ┆ 0.068413   ┆ 0.070616   ┆ 0.234421   │\n",
       "│            ┆ ts         ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t5_6xknxn  ┆ 007desinsf ┆ 0.028375   ┆ -0.157933  ┆ … ┆ -0.643633  ┆ 0.347671   ┆ 0.175447   ┆ 0.225121   │\n",
       "│            ┆ w          ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "└────────────┴────────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_emb[:4, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c722de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>subreddit_id</th><th>subreddit_name</th><th>embedding_000</th><th>embedding_001</th><th>embedding_002</th><th>embedding_003</th><th>embedding_004</th><th>embedding_005</th><th>embedding_006</th><th>embedding_007</th><th>embedding_008</th><th>embedding_009</th><th>embedding_010</th><th>embedding_011</th><th>embedding_012</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;t5_3oco2x&quot;</td><td>&quot;007nightfire&quot;</td><td>-0.04789</td><td>0.518611</td><td>0.222843</td><td>-0.142906</td><td>0.122165</td><td>0.196806</td><td>-0.053357</td><td>-0.257993</td><td>0.720489</td><td>-0.112393</td><td>-0.089884</td><td>0.070488</td><td>-0.129584</td></tr><tr><td>&quot;t5_6xv2qd&quot;</td><td>&quot;00_x&quot;</td><td>0.047263</td><td>-0.227378</td><td>0.631552</td><td>-0.263744</td><td>-0.619626</td><td>0.251459</td><td>-0.253667</td><td>0.059874</td><td>0.414293</td><td>-1.12446</td><td>0.003658</td><td>0.32285</td><td>0.144135</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 15)\n",
       "┌────────────┬────────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ subreddit_ ┆ subreddit_ ┆ embedding_ ┆ embedding_ ┆ … ┆ embedding_ ┆ embedding_ ┆ embedding_ ┆ embedding_ │\n",
       "│ id         ┆ name       ┆ 000        ┆ 001        ┆   ┆ 009        ┆ 010        ┆ 011        ┆ 012        │\n",
       "│ ---        ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ str        ┆ str        ┆ f64        ┆ f64        ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
       "╞════════════╪════════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ t5_3oco2x  ┆ 007nightfi ┆ -0.04789   ┆ 0.518611   ┆ … ┆ -0.112393  ┆ -0.089884  ┆ 0.070488   ┆ -0.129584  │\n",
       "│            ┆ re         ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t5_6xv2qd  ┆ 00_x       ┆ 0.047263   ┆ -0.227378  ┆ … ┆ -1.12446   ┆ 0.003658   ┆ 0.32285    ┆ 0.144135   │\n",
       "└────────────┴────────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_emb[4:6, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22037ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target users\n",
    "Create a table for these target users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f33807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Query is running:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 788.40query/s] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 971 ms, sys: 99.8 ms, total: 1.07 s\n",
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery _ --project data-science-prod-218515 \n",
    "\n",
    "-- Get test users for end-to-end embeddings\n",
    "-- TODO(djb): Filters\n",
    "--   - remove users who only viewed one subreddit\n",
    "--   - keep users with an iOS or Android event\n",
    "DECLARE PT_DT DATE DEFAULT \"2022-12-01\";\n",
    "DECLARE PT_WINDOW_START DATE DEFAULT PT_DT - 13;\n",
    "\n",
    "DECLARE TARGET_COUNTRIES DEFAULT [\n",
    "    \"DE\", \"AT\", \"CH\"\n",
    "];\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `reddit-employee-datasets.david_bermejo.pn_test_users_de_campaign_20230417` AS (\n",
    "SELECT\n",
    "    PT_DT AS pt\n",
    "    , PT_WINDOW_START AS pt_window_start\n",
    "    , v.subreddit_id\n",
    "    , v.subreddit_name\n",
    "    , v.user_id\n",
    "    , g.geo_country_code\n",
    "\n",
    "    , COUNT(DISTINCT post_id) AS view_and_consume_unique_count\n",
    "    , COUNT(DISTINCT(IF(v.action='consume', post_id, NULL))) AS consume_unique_count\n",
    "    , SUM(IF(v.action='view', 1, 0)) AS view_count\n",
    "    , SUM(IF(v.action='consume', 1, 0)) AS consume_count\n",
    "    , SUM(IF(v.action='consume' AND app_name='ios', 1, 0)) AS consume_ios_count\n",
    "    , SUM(IF(v.action='consume' AND app_name='android', 1, 0)) AS consume_android_count\n",
    "\n",
    "FROM (\n",
    "    SELECT\n",
    "        user_id\n",
    "        , geo_country_code\n",
    "    FROM `data-prod-165221.channels.user_geo_6mo_lookback`\n",
    "    WHERE\n",
    "        DATE(pt) = PT_DT\n",
    "        AND geo_country_code IN UNNEST(TARGET_COUNTRIES)\n",
    ") AS g\n",
    "    INNER JOIN `data-prod-165221.fact_tables.post_consume_post_detail_view_events` AS v\n",
    "        on g.user_id = v.user_id\n",
    "\n",
    "    -- TODO(djb): join to get only users who have at least 1 ios or android event in ANY sub\n",
    "WHERE DATE(v.pt) BETWEEN PT_WINDOW_START AND PT_DT\n",
    "    AND v.user_id IS NOT NULL\n",
    "    AND v.subreddit_name IN (\n",
    "        -- 1st sub is a target from a campaign & following ones are most similar based on ft2 embeddings:\n",
    "        'de', 'fragreddit', 'ich_iel'\n",
    "    )\n",
    "    AND action IN ('consume', 'view')\n",
    "GROUP BY 1,2,3,4,5,6\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7379d265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Query is running:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query executing stage S00: Input and status COMPLETE : 0.00s:   0%|          | 0/5 [00:00<?, ?query/s]\u001b[A\n",
      "Query complete after 0.00s: 100%|██████████| 5/5 [00:00<00:00, 1418.53query/s]                        \u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?rows/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.18s/rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.2 ms, sys: 11.9 ms, total: 64.1 ms\n",
      "Wall time: 4.41 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery df_target_user_counts --project data-science-prod-218515 \n",
    "\n",
    "SELECT\n",
    "    COUNT(*) AS row_count\n",
    "    , COUNT(DISTINCT user_id) AS user_id_count\n",
    "FROM `reddit-employee-datasets.david_bermejo.pn_test_users_de_campaign_20230417`\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "403a0e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>user_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3601830</td>\n",
       "      <td>2041130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_count  user_id_count\n",
       "0    3601830        2041130"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_user_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8230a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_target_user_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70829ae",
   "metadata": {},
   "source": [
    "## Get Time-on-Subreddit (ToS) for target user IDs\n",
    "\n",
    "We'll use these to create weighted embeddings.\n",
    "\n",
    "I'm caching the `selected users` in a table so that it's easier/faster to re-use while testing.\n",
    "\n",
    "The time-on-subreddit feature is already in gazette features so we don't need to waste time computing them.\n",
    "\n",
    "Key tables:\n",
    "- Select users:\n",
    "    - `data-prod-165221.fact_tables.post_consume_post_detail_view_events` \n",
    "- Get ToS:\n",
    "    - `data-prod-165221.user_feature_platform.time_on_subreddit_pct_time_over_30_day_v1`\n",
    "    \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b82d566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Query is running:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 740.26query/s] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 118 ms, sys: 34.2 ms, total: 152 ms\n",
      "Wall time: 14.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery _ --project data-science-prod-218515 \n",
    "\n",
    "-- Pull the user IDs + time on subreddit\n",
    "--   Using temp function because it's much faster to expand ToS in BQ than in python\n",
    "--   But we need to write the data to a table because the response can be huge (billions of rows)\n",
    "DECLARE PT_TOS DATE DEFAULT \"2022-12-01\";\n",
    "\n",
    "CREATE TEMP FUNCTION\n",
    "tosParser(tosString STRING)\n",
    "RETURNS STRUCT<\n",
    "    subreddit_id STRING,\n",
    "    tos_30_pct FLOAT64\n",
    ">\n",
    "LANGUAGE js AS \"\"\"\n",
    "   arr = tosString.split(':');\n",
    "   this.subreddit_id = arr[0].slice(1, -1);\n",
    "   this.tos_30_pct = parseFloat(arr[1]);\n",
    "   return this;\n",
    "\"\"\"\n",
    ";\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE `reddit-employee-datasets.david_bermejo.pn_test_users_de_campaign_tos_30_pct_20230417`\n",
    "CLUSTER BY user_id\n",
    "AS (\n",
    "WITH tos_filtered AS (\n",
    "SELECT\n",
    "    u.user_id\n",
    "    , t.feature_value\n",
    "FROM `data-prod-165221.user_feature_platform.time_on_subreddit_pct_time_over_30_day_v1` AS t\n",
    "    INNER JOIN(\n",
    "        SELECT\n",
    "            DISTINCT user_id\n",
    "        FROM `reddit-employee-datasets.david_bermejo.pn_test_users_de_campaign_20230417`\n",
    "        -- LIMIT 10\n",
    "    ) AS u\n",
    "        ON u.user_id = t.entity_id\n",
    "WHERE DATE(t.pt) = PT_TOS\n",
    ")\n",
    ", tos_exploded AS (\n",
    "SELECT\n",
    "    user_id\n",
    "    , tosParser(feature_array_exploded).*\n",
    "FROM(\n",
    "    SELECT user_id, feature_array_exploded\n",
    "    FROM (\n",
    "        SELECT user_id, SPLIT(RTRIM(LTRIM(feature_value, '{'), '}')) AS feature_array FROM tos_filtered\n",
    "    ), UNNEST(feature_array) AS feature_array_exploded\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "SELECT *\n",
    "FROM tos_exploded\n",
    "\n",
    "WHERE\n",
    "    -- Limit smallest sub b/c at some point it's a waste to try to aggregate such small embeddings\n",
    "    tos_30_pct >= 0.0001\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "198dab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Query is running:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query executing stage S00: Input and status COMPLETE : 0.00s:   0%|          | 0/4 [00:00<?, ?query/s]\u001b[A\n",
      "Query complete after 0.00s: 100%|██████████| 4/4 [00:00<00:00, 1271.10query/s]                        \u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?rows/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.24s/rows]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.8 ms, sys: 9.96 ms, total: 55.7 ms\n",
      "Wall time: 4.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery df_tos_counts --project data-science-prod-218515 \n",
    "-- Count checks\n",
    "SELECT\n",
    "    COUNT(*) AS row_count\n",
    "    , COUNT(DISTINCT user_id) AS user_id_count\n",
    "FROM `reddit-employee-datasets.david_bermejo.pn_test_users_de_campaign_tos_30_pct_20230417`\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dca6319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>user_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83382651</td>\n",
       "      <td>1844353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_count  user_id_count\n",
       "0   83382651        1844353"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b7fd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_tos_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a756921",
   "metadata": {},
   "source": [
    "\n",
    "### Load data from GCS because loading straight from BQ takes too long (stopped after waiting for 30 minutes). \n",
    "\n",
    "```bash\n",
    "# Copy: the -n flag only copies \"NEW\" files\n",
    "# !gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m cp -r -n $remote_gs_path $local_f\n",
    "\n",
    "# rsync can be nicer because with `-n` you get a preview (dry run) before running a command\n",
    "# !gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m rsync -r -n $gcs_tos $local_path_tos\n",
    "```\n",
    "\n",
    "Note that it's better to run this command from the CLI because it can crash jupyter (fast log info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f3caffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Query is running:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query executing stage S00: Output and status COMPLETE : 0.00s:   0%|          | 0/1 [00:00<?, ?query/s]\u001b[A\n",
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 415.28query/s]                          \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.6 ms, sys: 1.33 ms, total: 60 ms\n",
      "Wall time: 3.63 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bigquery _ --project data-science-prod-218515 \n",
    "\n",
    "-- Export data to GCS because querying such a huge table takes forever and a half\n",
    "EXPORT DATA OPTIONS(\n",
    "    uri='gs://i18n-subreddit-clustering/pn_model/runs/de_only_20230417/user_tos_30_pct/*.parquet',\n",
    "    format='PARQUET',\n",
    "    overwrite=true\n",
    ") AS\n",
    "SELECT *\n",
    "FROM `reddit-employee-datasets.david_bermejo.pn_test_users_de_campaign_tos_30_pct_20230417`\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aecb6e",
   "metadata": {},
   "source": [
    "Note:\n",
    "- The `-r` flag creates a \"dry-run\" (only tells you what will happen)\n",
    "- for final CLI command remove this flat (`-r`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04628fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS URI:\n",
      "  gs://i18n-subreddit-clustering/pn_model/runs/de_only_20230417/user_tos_30_pct/\n",
      "Local path:\n",
      "  /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/pn_model/runs/de_only_20230417/user_tos_30_pct\n",
      "\n",
      "CLI command:\n",
      "gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m rsync -r -n gs://i18n-subreddit-clustering/pn_model/runs/de_only_20230417/user_tos_30_pct/ /home/jupyter/subreddit_clustering_i18n/data/local_cache/i18n-subreddit-clustering/pn_model/runs/de_only_20230417/user_tos_30_pct\n"
     ]
    }
   ],
   "source": [
    "bucket_name_ = 'i18n-subreddit-clustering'\n",
    "gcs_tos = f\"gs://{bucket_name_}/pn_model/runs/de_only_20230417/user_tos_30_pct/\"\n",
    "\n",
    "path_local_cache = Path(f\"/home/jupyter/subreddit_clustering_i18n/data/local_cache/{bucket_name_}/\")\n",
    "local_path_tos = (\n",
    "    path_local_cache / \n",
    "    gcs_tos.split(f\"{bucket_name_}/\")[1]\n",
    ")\n",
    "Path.mkdir(local_path_tos, exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "print(f\"GCS URI:\\n  {gcs_tos}\\nLocal path:\\n  {local_path_tos}\")\n",
    "\n",
    "print(f\"\\nCLI command:\")\n",
    "!echo gsutil -o GSUtil:parallel_thread_count=20 -o GSUtil:parallel_process_count=20 -m rsync -r -n $gcs_tos $local_path_tos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f0d8a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 3.95 s, total: 14.1 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_user_tos = pl.read_parquet(f\"{local_path_tos}/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "301b85f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83382651, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_tos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ad8a170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 582 ms, total: 1.76 s\n",
      "Wall time: 969 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>user_id</th><th>subreddit_id</th><th>tos_30_pct</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;1000000&quot;</td><td>&quot;1000000&quot;</td><td>1e6</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>0.021946</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>0.094023</td></tr><tr><td>&quot;min&quot;</td><td>&quot;t2_100078&quot;</td><td>&quot;t5_100eoi&quot;</td><td>0.0001</td></tr><tr><td>&quot;max&quot;</td><td>&quot;t2_zzz70&quot;</td><td>&quot;t5_zzszh&quot;</td><td>1.0</td></tr><tr><td>&quot;median&quot;</td><td>null</td><td>null</td><td>0.00126</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 4)\n",
       "┌────────────┬───────────┬──────────────┬────────────┐\n",
       "│ describe   ┆ user_id   ┆ subreddit_id ┆ tos_30_pct │\n",
       "│ ---        ┆ ---       ┆ ---          ┆ ---        │\n",
       "│ str        ┆ str       ┆ str          ┆ f64        │\n",
       "╞════════════╪═══════════╪══════════════╪════════════╡\n",
       "│ count      ┆ 1000000   ┆ 1000000      ┆ 1e6        │\n",
       "│ null_count ┆ 0         ┆ 0            ┆ 0.0        │\n",
       "│ mean       ┆ null      ┆ null         ┆ 0.021946   │\n",
       "│ std        ┆ null      ┆ null         ┆ 0.094023   │\n",
       "│ min        ┆ t2_100078 ┆ t5_100eoi    ┆ 0.0001     │\n",
       "│ max        ┆ t2_zzz70  ┆ t5_zzszh     ┆ 1.0        │\n",
       "│ median     ┆ null      ┆ null         ┆ 0.00126    │\n",
       "└────────────┴───────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_user_tos.sample(n=int(1e6)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fc33628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>subreddit_id</th><th>tos_30_pct</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;t2_10jcae&quot;</td><td>&quot;t5_33x33&quot;</td><td>0.008</td></tr><tr><td>&quot;t2_10ojl9oz&quot;</td><td>&quot;t5_2zzfr&quot;</td><td>0.01521</td></tr><tr><td>&quot;t2_10ovevbv&quot;</td><td>&quot;t5_2xp2o&quot;</td><td>0.0008</td></tr><tr><td>&quot;t2_10nfxs&quot;</td><td>&quot;t5_12h1g7&quot;</td><td>0.00059</td></tr><tr><td>&quot;t2_10nfxs&quot;</td><td>&quot;t5_7dqr4z&quot;</td><td>0.00015</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────────┬──────────────┬────────────┐\n",
       "│ user_id     ┆ subreddit_id ┆ tos_30_pct │\n",
       "│ ---         ┆ ---          ┆ ---        │\n",
       "│ str         ┆ str          ┆ f64        │\n",
       "╞═════════════╪══════════════╪════════════╡\n",
       "│ t2_10jcae   ┆ t5_33x33     ┆ 0.008      │\n",
       "│ t2_10ojl9oz ┆ t5_2zzfr     ┆ 0.01521    │\n",
       "│ t2_10ovevbv ┆ t5_2xp2o     ┆ 0.0008     │\n",
       "│ t2_10nfxs   ┆ t5_12h1g7    ┆ 0.00059    │\n",
       "│ t2_10nfxs   ┆ t5_7dqr4z    ┆ 0.00015    │\n",
       "└─────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_tos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cbcde",
   "metadata": {},
   "source": [
    "# Other Transforms \n",
    "\n",
    "We don't need these transforms for ANN, but will use it for modeling step\n",
    "- Count of subreddits per user for ToS\n",
    "\n",
    "Other data:\n",
    "- Subscribed to target subreddit\n",
    "- Age of account\n",
    "- Activity in L14 days\n",
    "- etc...\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f07c65",
   "metadata": {},
   "source": [
    "# Compute user-level weighted embedding\n",
    "\n",
    "now that we have the long df, let's compute the user-level weighted embedding (multiply weights x embedding & groupby user-id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf616a1d",
   "metadata": {},
   "source": [
    "## Run on all users\n",
    "\n",
    "Batches of 150-million user<>subreddit weighted embeddings was too much memory (over 1.2 TB). So the kernel gets killed/dies.\n",
    "\n",
    "Instead, try to run in batches of\n",
    "- 80 to 100 million ToS rows\n",
    "\n",
    "---\n",
    "\n",
    "Example ETAs:\n",
    "\n",
    "```bash\n",
    "# 800k users | 130 million ToS rows\n",
    "07:08:08 | INFO | \"Append subreddit embeddings to user ToS\"\n",
    "07:08:13 | INFO | \"Compute weighted avg embedding for user-level\"\n",
    "07:09:49 | INFO | \"Compute AGGREGATED user-level embedding...\"\n",
    "(130601209, 100)\n",
    "(856211, 101)\n",
    "CPU times: user 18min 9s, sys: 8min 21s, total: 26min 31s\n",
    "Wall time: 4min 52s\n",
    "\n",
    "\n",
    "# 9.8 million users |  764.5 million ToS rows\n",
    "10:49:14 | INFO | \"Compute weighted ToS embeddings...\"\n",
    "10:58:43 | INFO | \"(764565252, 102) <- df.shape ToS with weighted embedddings\"\n",
    "10:58:43 | INFO | \"Delete temp df to free up memory\"\n",
    "10:58:43 | INFO | \"Compute AGGREGATED user-level embedding...\"\n",
    "10:59:50 | INFO | \"(9874962, 101) <- df.shape User weighted embedddings\"\n",
    "(764565252, 102)\n",
    "(9874962, 101)\n",
    "CPU times: user 2h 53min 2s, sys: 44min 23s, total: 3h 37min 25s\n",
    "Wall time: 23min 30s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01da497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:00:04 | INFO | \"83,382,651 <- Data Size\"\n",
      "17:00:04 | INFO | \"80,000,000 <- Batch Size\"\n",
      "17:00:04 | INFO | \"0,000,002 <- Chunks\"\n",
      "17:00:04 | INFO | \"Append subreddit embeddings to user ToS\"\n",
      "17:00:12 | INFO | \"(70649280, 103) <- df.shape ToS with raw embeddings (no nulls)\"\n",
      "17:00:13 | INFO | \"Create DAG for weighted avg embedding per user+subreddit\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690e1e8b2c354be687ecf342c56bc48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:01:06 | INFO | \"Dask visualize DAG\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAADbCAIAAADCuXnGAAAABmJLR0QA/wD/AP+gvaeTAAATWUlEQVR4nO2deVBTxx/A9yUkAZM0QEBAwGgFAqgUakEQUQM42AHHo6LVCqOV1uq0VG0VHWfKH3Yca7UejBfOiAIqYB2xeNRBAmqVGuvR0QoJWgXUciXlSICEHL8/3pRfCCHkeNkc7ucPxtm3b/ebjzub9zbvfRfTaDQAARGSrQN460DGYYOMw8aFwLbKysoIbM2umDFjRkBAADFtaQhCpVIRE5Bdcu7cOaJEETyrEBiZnUD4SELzOGyQcdgg47BBxmGDjMMGGYcNMg4bZBw2yDhskHHYIOOwQcZhg4zDBhmHDTIOG2QcNsg4bJBx2CDjsEHGYYOMwwYZhw0yDhtkHDbIOGyQcdgg47BBxmGDjMMGGYcNMg4bZBw2yDhskHHYIOOwQcZhg4zDBhmHDTIOG2QcNsg4bJBx2CDjsEHGYeOoxiUSia1DMBMi86sAAGpra9VqNbFt6uXQoUNZWVk0Gs3aHWkIzyNmt1kx7AoCs5gQZhwmBQUFAIC0tDRbB2IOmMYBs+8lJyfz+XwymdzW1ubh4WHrcEzD8b4529vba2pq8PFy/vx5W4djMo5nvLS0FMMwAIBGoyksLLR1OCbjeLNKTEzMH3/8gYeNYVhzc7O/v7+tgzIBBxvjjY2Ng7oBAGQy2eFS/jmY8TNnzri4/P8eQqVSnTp1yobxmIGDzSphYWH19fU6hUKhMCQkxCbxmIEjjfG6urrhuqlUaklJiU3iMQ9HMl5cXEyhUHQKFQqFY00sjjSrcDicpqYmvYcePHgQFRUFOR7zcJgxXltbO5JuKpV69uxZyPGYjcMYxydrmj6USmVxcTGcNUvLIXi11npwOJycnBz83/jd5pw5czgczmCFjo6OsWPH2ig6E3CkeXwQtVpNJpPPnTu3ZMkSW8diMg4zqzgNyDhskHHYIOOwQcZhg4zDBhmHDTIOG2QcNsg4bJBx2CDjsEHGYYOMwwYZhw0yDhtkHDbIOGyQcdgg47BBxmGDjMMGGYcNMg4bZBw2yDhskHHYIOOwQcZhg4zDBhmHjb0/sS+XyxsaGoRCoVgs7uzslEqlMpmsp6cHAHD48OGbN2/S6XR3d3cGg8FkMrlcbkhIiJ3nRrC7J/ZbW1urq6sFAkFdXZ1IJGpsbFSpVCQSicViubu70+l0Op3OZDJZLJZMJpPJZFKptKurq6enp6enp7+/HwDg7e0dGhrK5XIjIiISExPDw8Px9/jtBLswLpFIampqqqur+Xz+06dPXVxcoqKiwsLC8DGL/x01XZBarW5sbBSJREKhsL6+XiQS3b9/v7Oz08fHh8fj8Xi8xMTEoKAgOJ/IEDbI6fIfcrn8l19+SU9Pp1KpZDI5PDz8888/Lysr6+zsJKR9pVL55MmTY8eOpaens1gsAEB4ePiuXbtaWloIad88bGP87t2769ev9/T0JJFIycnJhYWF//77r1V7VCgUVVVVq1evfueddygUyvz588vKyuRyuVU71Qts47du3UpLSwMAhIWF5ebm/v3335AD6OvrKysrS0tLo1Ao48eP379/v0wmgxkAJONqtbq8vDw6OhoAkJKScuPGDTj9GqCpqSk7O9vNzc3Hx2f37t3d3d1w+oVhvKamJiIiAsOwhQsX3rt3D0KPxtPS0pKTk8NkMj09PQ8cOKBUKq3do3WNt7S0ZGRkYBiWmpr65MkTq/ZlCRKJZNu2bTQaLTIy8s6dO1bty1rG8VwzbDbb39//1KlTVuqFWBoaGubNm4dhWEZGRltbm5V6sYrxly9fxsXFUanUrVu3Qv5espySkpJx48Z5e3tfvnzZGu0Tb7y8vNzT03PKlCn2PI0YpqurC58Mt2zZMjAwQGzjRBofGBjIzc0lkUgZGRlSqZTAlm3CqVOn6HR6dHQ0sZewhBlvaWmJiYlhMBhFRUVEtWlznjx5Eh4ezmazq6qqiGqTGOPPnz8PCgoKDg6ur68npEH7QSqVLlu2jEajlZWVEdIgAcYfP37s7+8/bdq01tZWy1uzQ9Rq9TfffINh2N69ey1vzVLjN27cYLFYPB6vq6vL8mjsmV27dmEYlpOTY2E7Fhmvqalxc3NLT0/v7++3MA6H4Pjx42Qy2ULp5ht/9OgRi8VKT09XqVSWROBYFBYWYhj2008/md2CmcafP3/u5+fH4/HektGtTV5eHoZhBQUF5p1ujvG2traQkJCIiAhrL2rbLZs3b6ZQKFeuXDHjXJONy+XymJiY4OBgZ70yMQa1Wp2ZmclgMP766y9TzzXZ+IYNGxgMRl1dnaknOhkKhSI2Nnby5MmmLhyZZryiogLDsMLCQpPOclaamprYbPann35q0lkmGMc7yMrKMjEwZ+bSpUsYhpm0HG2scaVSGRcXN2XKFIdbfbU2mzZtYjAYIpHIyPrGGs/Ly6NSqY67AGs9FApFZGRkcnKykfWNMt7S0uLu7r5t2zYLAnNmBAIBiUQqLS01prJRxleuXDl+/HgnWPK2HmvWrPH19TXm2abRjd+8eRPDsPLyciICc1rEYrGXl9emTZtGrTmKcbVaHRERkZqaSlBgzkx+fr6Li4tQKDRcbRTjFy5cwDDs8ePHxAXmtCiVSi6Xu3r1asPVRjEeHR29aNEi4qJycgoKCigUyosXLwzUMWT82rVrAACBQEBwXM6LQqGYOHHi+vXrDdQxZHzWrFkpKSlER+XkHD58mEajvX79eqQKIxoXCAQAAHt4JNOx6O/vHzdunIF7lxHfvDp58mR4ePisWbMgvTngLNBotNWrVxcWFo64DbLe/we5XO7l5bVr1y6rDQVnRiQSAQCuX7+u96j+MX7p0iWJRLJixQrrjQUnJjg4OCYmpqioSO9R/caLioqSkpICAwOtGZgzk5GR8fPPP0ulUj3Hhg97sVhMpVLRzw6W0N7eTqVSi4uLhx/SM8arq6tVKhX+tg7CPLy8vOLj4ysrK4cf0m88KirKzt/8tX94PN7169eHl+s3npiYaP2QnJzExMTXr183NDTolOsab2trq6ur4/F4sAJzWqZPn85kMvl8vk65rvGqqioymRwfHw8rMKfFxcVl5syZ1dXVOuW6xgUCQVRUFJPJhBWYM5OQkHD37l2dQl3j9fX1YWFhsEJyckJDQ5uamnp7e7ULdY0LhUIulwsxKmeGy+Wq1epnz55pFw4xLpfLm5qakHGiCAoKwn+H0y4cYryhoUGlUiHjREGlUjkcjiHjIpGIRCLZRdoXuDAYjJkzZ1qj5dDQUEPG29ra3N3dXV1drdH320lAQMCbN2+0S4YY7+npQdeFxOLl5SUWi7VLhhiXSqXEGheLxZs2bZo0aRKNRgsICEhOTj558mRfX9/wClQq1cPD48MPPxy8ZSgvL8f+4+XLl8uWLXN3d2ez2Wlpac+fPze+F6VSWVpaOnfuXF9fXzc3t6lTpx44cECtVuNH9+zZg2GYTCa7ffs23peLC5H58dhsto7xIau1GzdujIuLI2rF8p9//pk4caKvr29FRUV3d3dLS8uOHTsAAPv27dOu4OPjU1FR0dXVJRQKFy9ejGHY8ePHBxtZsGABAGDBggV37tyRSqWVlZVubm7R0dHG91JRUQEA2Llzp0QiaW9vP3jwIIlE+vbbb7VDpdPp8fHxRH1wbY4ePerp6aldMsR4VlbW3Llzieps1apVAACd5x/nzZs36AKvcPbs2cGj+M+ybm5ug8nDcOMVFRWDdZYsWQIAaG9vN7KXioqKOXPmaB9duXIlhULRfgHVesZPnDhBp9O1S4YYX758+cKFC4nqDE+/ZiAbkt4KGRkZAIDBZ+Bx49rZ2zZu3AgA+PPPP43sZTg//vgjAEA7c431jJ8+fZpCoWiXDJmzaDRaZ2cnIfOXXC7v6upydXUd6YthpAo+Pj64Yu1CXCsOlUoFAOAT8ai9AAC6urr27t174cKFV69eaX86nZtvKzEwMKDzxTDkm5PJZOr/ac50aDQai8Xq7+/HM54aX6G1tRUA4OvrS0gvAID58+fv2LHjs88+E4lEarVao9Hs27cPAKDRSq1pvaSfCoVCJzemrnEDoZvKokWLAABXrlzRLoyKisKnhcEKly9fHjwql8urqqrc3NxSUlII6UWlUt2+fdvX1zc7O9vb2xs3q32xhDNmzBiFQoH/m8vl5ufnG/shR6O/v1/3/kZ7itm5c+ekSZOImsLwqwg/P79Lly51d3c3NzevW7fOx8ensbFRuwJ+rdLd3T14rZKfnz/YCD6P9/X1DZbk5OQAAB4+fGhkL/jvWbt3725vb+/t7eXz+ePHjwcAVFZWDrY5b948FovV1NR0584dFxeXp0+fEiUhNzc3PDxcu2SI8by8vLFjxxLVmUaj6ejo2LBhw8SJEykUip+f38cff6zzhpJ2BRaLlZKSMpg7pra2VntkbN++XTM0xe7gU+2Ge2lvb1+7dm1gYCCFQvHx8Vm1atXWrVvxFqZNm4bXqa+vT0hIoNPpgYGBhw4dItDAl19+mZCQoF0yJFNwcXFxVlZWb28viYTykhPDsmXLlErl+fPnB0uGmA0KCsIXbKEH5rQIhcLg4GDtkiHG8XVanbUuhNloNJpnz57prH4PMe7h4eHt7Y2ME0Vzc7NMJjNkHADA5XKRcaLATY5ifPgKOsJsRCIRm81ms9nahbrG33vvvfv374/4tDnCFAQCQWRkpE6hrvE5c+Z0dnY+fPgQVlTODJ/PH/50m67xyZMn+/r6Dn+SCGEqIpHo1atXw5/g1DWOYdjs2bORccupqqpiMpkffPCBTrmee0sej3fr1q2BgQEogTkt1dXVCQkJFApFp1yP8aSkJKlUeuvWLSiBOScKhYLP5yclJQ0/pMd4UFDQtGnTTp8+bf3AnJarV69KJJLFixcPP6R/xQp/cQjOryROSVFREY/HmzBhgp5jetcYW1tbKRTK6dOnCVy3fHuQSCQ0Gm2kpJ/6x/jYsWNTUlJGeiMRYZjS0lISiYT/OKWHkf6jysrKyGRyU1OTtUaC8xITE7N8+fKRjo5oXKFQcDic7Oxs60TltFRVVQEAfvvtt5EqGMr2cfDgQVdX1zdv3lghMKclMTExKSnJQAVDxvv6+vz8/CzPKv/28PvvvwMA+Hy+gTqjZG364YcfmEymRCIhNDCnJTU1NTY21nCdUYx3d3d7enqi3JLGUFtbi2HYqDtljZ7vcP/+/VQqFaW/NoxKpYqJiZk9ezb+2JcBRjeuVCqjoqKMz8v6doLn9TXm0SKjsqjieVlLSkosDsw5MSmvr7G5mbOyssaNGwdto1bHIiMjg8PhGJkn3FjjHR0d3t7emZmZFgTmnJw/fx7DsIsXLxpZ34Qc+1euXCGRSCdOnDArMOeksbHR09Pziy++MP4U0/aR2LJli6ur66NHj0wMzDlRKBRxcXFTp07t7e01/izTjA8MDMTHx4eEhKAJXaPRfP3112ZsG2Py7jT4/h1Lly59qzYXG05xcTGGYWfOnDH1RHP2vKqqqqLRaF999ZUZ5zoHv/76K4VC2bJlixnnmrmv24ULF8hk8vfff2/e6Q7NvXv3GAzGJ598MurtpV7M37vwyJEjOm+7vg00NDT4+PikpqaaveO1Rftzfvfdd2Qy+e3JRSkUCjkcTmxsrCWbmFi6B21OTg6GYXv27LGwHftHIBB4e3vHxsaKxWJL2iFgn+UDBw6QSKTs7Gwnvnq5fv06k8lMTk62/LKYmL3Ei4qKKBRKZmamQqEgpEG7gthPR4xxjUZz9epVBoMRGxv78uVLotq0OXK5PDs7G8OwzZs3m3dlMhzCjGs0mrq6uoiICBaLde7cOQKbtRWNjY1xcXEMBkNvimWzIdK4RqPp6+vDB0V2drZcLie2cZiUl5d7eHiEh4cTvnkgwcZxCgsLGQxGZGSkdkYNR6GlpQXPOLJ27Vrtt9OJwirGNRqNSCRKTk4mkUhr1qwZzD5j5yiVyry8PHd3dw6HY/x6t6lYyzhOSUmJv78/m83Oz8+382vH2tra999/n0qlbt261aq7vlrXuEajkUqlubm5VCr13XffPXbsmB1ePj548CA9PR1/HQfClq9WN44jFApXrVpFoVAmTJhw+PDh/v5+OP0ahs/n429GzZgxY9TnTIgCknGcFy9erFu3ztXV1c/Pb/v27fX19TB7H0QikRw9enT69OkAgKSkJMMPrREOVOM4b9682bZtW0BAAABg+vTpeXl5HR0dEPqVy+UXL1786KOPaDTamDFjVqxYUVtbC6FfHWxgHEelUlVWVmZmZjIYDCqVyuPxduzYcfv2bbNXQUdCKBQeOXJk6dKlbDabRCLxeLyCggIb/mo4JKONTZDJZBcvXrx27Rqfz3/16hWDwUhISEhISAgNDeVyuUFBQXjyNyPRaDTNzc0ikUgoFAoEgsE2Z82alZiYmJ6ejqdssiG2N66NSCSqrq6urq6+e/cunujKxcVlwoQJISEhgYGBXl5ebDabTqdjGObu7t7b2yuXywcGBjo6OsRicWtrq0gkEolE+AtjbDY7MjKSx+PxeLyYmBhic6Nagn0Z16avrw8fqvjf169fSyQSsVgsk8nUanVXV9eYMWNoNBqFQsHzO3h7ewcHB3O5XC6XGxoaqpPxwX6wX+POCspABhtkHDbIOGz+BxJfLboJtgqoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:01:08 | INFO | \"Compute weighted ToS embeddings...\"\n",
      "17:01:29 | INFO | \"(70649280, 102) <- df.shape ToS with weighted embedddings\"\n",
      "17:01:29 | INFO | \"Delete temp df to free up memory\"\n",
      "17:01:29 | INFO | \"Compute AGGREGATED user-level embedding...\"\n",
      "17:01:35 | INFO | \"(1831171, 101) <- df.shape User weighted embedddings\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1831171, 101)\n",
      "CPU times: user 13min 44s, sys: 4min 33s, total: 18min 18s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# when computing on full data set, we need to batch b/c the computer will run out of RAM if/when converting parquet to numpy\n",
    "n_sample = None  # None  # None  int(10e6)\n",
    "\n",
    "if n_sample is None:\n",
    "    df_user_tos_proc_ = df_user_tos\n",
    "else:\n",
    "    df_user_tos_proc_ = df_user_tos.sample(n=n_sample, seed=42)\n",
    "n_size = len(df_user_tos_proc_)\n",
    "\n",
    "batch_size = int(80e6)  # int(110e6), 9e6 = 9Million\n",
    "n_chunks = 1 + n_size // batch_size\n",
    "iter_chunks = range(n_chunks)\n",
    "\n",
    "info(f\"{n_size:09,.0f} <- Data Size\")\n",
    "info(f\"{batch_size:09,.0f} <- Batch Size\")\n",
    "info(f\"{n_chunks:09,.0f} <- Chunks\")\n",
    "\n",
    "\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp = (\n",
    "    df_user_tos_proc_\n",
    "    .join(\n",
    "        df_sub_emb.drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp.shape} <- df.shape ToS with raw embeddings (no nulls)\")\n",
    "gc.collect()\n",
    "\n",
    "info(f\"Create DAG for weighted avg embedding per user+subreddit\")\n",
    "# Use dask.delayed() to compute these in parallel & prevent some OOM errors\n",
    "l_dfs_tos_wt = list()\n",
    "for i in tqdm(iter_chunks, mininterval=0.1, ascii=True):\n",
    "    l_dfs_tos_wt.append(\n",
    "        df_user_emb_tmp[['user_id', 'subreddit_id']][:batch_size]\n",
    "        .with_columns(\n",
    "            pl.DataFrame(\n",
    "                data=(\n",
    "                    np.multiply(\n",
    "                        df_user_emb_tmp['tos_30_pct'][:batch_size].to_numpy(),\n",
    "                        df_user_emb_tmp[l_emb_cols][:batch_size].to_numpy().T\n",
    "                    ).T\n",
    "                ),\n",
    "                schema=l_emb_cols,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # delete processed rows to save space\n",
    "    df_user_emb_tmp = df_user_emb_tmp[batch_size:]\n",
    "    gc.collect()\n",
    "\n",
    "df_user_emb_tmp_wt_del = dask.delayed(pl.concat)(\n",
    "    l_dfs_tos_wt, how='vertical', parallel=True\n",
    ")\n",
    "info(f\"Dask visualize DAG\")\n",
    "display(df_user_emb_tmp_wt_del.visualize(\n",
    "    f\"dask_DAG-{n_size}_data-{batch_size}_batch_size-{n_chunks}_chunks.png\"\n",
    "))\n",
    "gc.collect()\n",
    "info(f\"Compute weighted ToS embeddings...\")\n",
    "df_user_emb_tmp_wt = df_user_emb_tmp_wt_del.compute()\n",
    "info(f\"{df_user_emb_tmp_wt.shape} <- df.shape ToS with weighted embedddings\")\n",
    "\n",
    "info(f\"Delete temp df to free up memory\")\n",
    "del df_user_emb_tmp\n",
    "gc.collect()\n",
    "\n",
    "info(f\"Compute AGGREGATED user-level embedding...\")\n",
    "df_user_emb = (\n",
    "    df_user_emb_tmp_wt\n",
    "    .groupby(\n",
    "        ['user_id']\n",
    "    )\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(l_emb_cols).mean()\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "info(f\"{df_user_emb.shape} <- df.shape User weighted embedddings\")\n",
    "gc.collect()\n",
    "print(df_user_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d834aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>embedding_000</th><th>embedding_001</th><th>embedding_002</th><th>embedding_003</th><th>embedding_004</th><th>embedding_005</th><th>embedding_006</th><th>embedding_007</th><th>embedding_008</th><th>embedding_009</th><th>embedding_010</th><th>embedding_011</th><th>embedding_012</th><th>embedding_013</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;t2_3dztlwp1&quot;</td><td>-0.003785</td><td>0.003161</td><td>0.000537</td><td>-0.001168</td><td>-0.002078</td><td>0.000971</td><td>-0.002561</td><td>0.000304</td><td>0.005292</td><td>0.001673</td><td>0.000477</td><td>0.004041</td><td>-0.000734</td><td>-0.001473</td></tr><tr><td>&quot;t2_27a2p6nm&quot;</td><td>-0.000677</td><td>0.001204</td><td>0.001093</td><td>-0.001587</td><td>0.000388</td><td>0.001258</td><td>-0.001489</td><td>-0.000857</td><td>0.003769</td><td>0.000087</td><td>-0.001806</td><td>0.00205</td><td>0.002429</td><td>-0.001141</td></tr><tr><td>&quot;t2_e63nvy24&quot;</td><td>-0.02928</td><td>0.065853</td><td>0.076953</td><td>0.003259</td><td>-0.289421</td><td>-0.114375</td><td>-0.210511</td><td>0.01387</td><td>0.29104</td><td>-0.068335</td><td>-0.178308</td><td>-0.097683</td><td>-0.060427</td><td>-0.172378</td></tr><tr><td>&quot;t2_7bkn2dls&quot;</td><td>-0.000502</td><td>0.014535</td><td>-0.005488</td><td>0.006118</td><td>-0.010488</td><td>-0.00213</td><td>-0.007979</td><td>0.007133</td><td>0.007597</td><td>0.014688</td><td>0.013892</td><td>0.007289</td><td>0.006347</td><td>-0.008107</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 15)\n",
       "┌───┬────────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ u ┆ embedding_ ┆ embedding_ ┆ embedding_ ┆ … ┆ embedding_ ┆ embedding_ ┆ embedding_ ┆ embedding_ │\n",
       "│ s ┆ 000        ┆ 001        ┆ 002        ┆   ┆ 010        ┆ 011        ┆ 012        ┆ 013        │\n",
       "│ e ┆ ---        ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ r ┆ f64        ┆ f64        ┆ f64        ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ i ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ - ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ s ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ r ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "╞═══╪════════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ t ┆ -0.003785  ┆ 0.003161   ┆ 0.000537   ┆ … ┆ 0.000477   ┆ 0.004041   ┆ -0.000734  ┆ -0.001473  │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 3 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ z ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ l ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ w ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ p ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 1 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆ -0.000677  ┆ 0.001204   ┆ 0.001093   ┆ … ┆ -0.001806  ┆ 0.00205    ┆ 0.002429   ┆ -0.001141  │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 7 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ a ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ p ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 6 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ m ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆ -0.02928   ┆ 0.065853   ┆ 0.076953   ┆ … ┆ -0.178308  ┆ -0.097683  ┆ -0.060427  ┆ -0.172378  │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ e ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 6 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 3 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ v ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ y ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 4 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ t ┆ -0.000502  ┆ 0.014535   ┆ -0.005488  ┆ … ┆ 0.013892   ┆ 0.007289   ┆ 0.006347   ┆ -0.008107  │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ _ ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 7 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ b ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ k ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ n ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ 2 ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ d ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ l ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "│ s ┆            ┆            ┆            ┆   ┆            ┆            ┆            ┆            │\n",
       "└───┴────────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_emb[:4, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d0498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbe9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21239df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/subreddit_clustering_i18n/data/models/pn_model/pn_manual_test_2023-04-17_161043/df_user_embedding-2023-04-17_1705-1831171_by_101.parquet\n",
      "CPU times: user 15.5 s, sys: 1.08 s, total: 16.6 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r_, c_ = df_user_emb.shape\n",
    "\n",
    "f_user_embedding_name = (\n",
    "    path_this_model / \n",
    "    f\"df_user_embedding-{datetime.utcnow().strftime('%Y-%m-%d_%H%M')}\"\n",
    "    f\"-{r_}_by_{c_}.parquet\"\n",
    ")\n",
    "print(f_user_embedding_name)\n",
    "\n",
    "df_user_emb.write_parquet(\n",
    "    f_user_embedding_name\n",
    ")\n",
    "\n",
    "del r_, c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8889da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d05c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea449b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:02:42 | INFO | \"9,889,351 Users in run\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 5.79 s, total: 32.3 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_users_sample = df_user_tos['user_id'].n_unique()\n",
    "info(f\"{n_users_sample:,.0f} Users in run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec3ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:55:51 | INFO | \"2.73  Hours to get user-embeddings SEQUENTIALLY\"\n"
     ]
    }
   ],
   "source": [
    "n_mins_sample = 24 + 3\n",
    "n_users_target = 60e6\n",
    "\n",
    "seq_proc_time = n_mins_sample * (n_users_target / n_users_sample) / 60\n",
    "info(f\"{seq_proc_time:,.2f}  Hours to get user-embeddings SEQUENTIALLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d77f7",
   "metadata": {},
   "source": [
    "# Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delaying the multiplication doesn't help with the OOM error\n",
    "l_dfs_tos_wt = list()\n",
    "for i in tqdm(iter_chunks, mininterval=0.1, ascii=True):\n",
    "    l_dfs_tos_wt.append(\n",
    "        dask.delayed(pl.select)(\n",
    "            df_user_emb_tmp[['user_id', 'subreddit_id']]\n",
    "            [i * batch_size:(i + 1) * batch_size]\n",
    "        )\n",
    "        .with_columns(\n",
    "            dask.delayed(pl.DataFrame)(\n",
    "                data=(\n",
    "                    dask.delayed(np.multiply)(\n",
    "                        dask.delayed(df_user_emb_tmp['tos_30_pct'][i * batch_size:(i + 1) * batch_size].to_numpy()),\n",
    "                        dask.delayed(df_user_emb_tmp[l_emb_cols][i * batch_size:(i + 1) * batch_size].to_numpy().T)\n",
    "                    ).T\n",
    "                ),\n",
    "                schema=l_emb_cols,\n",
    "            )\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c76b5",
   "metadata": {},
   "source": [
    "## compare creating weighted matrix in pandas v. polars\n",
    "polars is faster. but part of it might be that I'm creating an index and then resetting it...\n",
    "\n",
    "in any case, sticking to polars as much as possible is probably better for consistency and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "72f7ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:32:34 | INFO | \"Append subreddit embeddings to user ToS\"\n",
      "06:32:36 | INFO | \"(67217, 103) <- df.shape ToS with raw embeddings (no nulls)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 18s, sys: 12.1 s, total: 2min 30s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp_test = (\n",
    "    df_user_tos\n",
    "    .join(\n",
    "        pl.DataFrame(sample_users),\n",
    "        how='inner',\n",
    "        on=['user_id'],\n",
    "    )\n",
    "    .join(\n",
    "        df_sub_emb.drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_test.shape} <- df.shape ToS with raw embeddings (no nulls)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2ed9336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67217, 102)\n",
      "CPU times: user 337 ms, sys: 248 ms, total: 585 ms\n",
      "Wall time: 69.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_pl = (\n",
    "    df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "    .with_columns(\n",
    "        pl.DataFrame(\n",
    "            data=((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "            schema=l_emb_cols,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e9a8a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67217, 102)\n",
      "CPU times: user 423 ms, sys: 42.5 ms, total: 466 ms\n",
      "Wall time: 88.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_pd = pd.DataFrame(\n",
    "    ((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "    index=pd.MultiIndex.from_frame(df_user_emb_tmp_test[['user_id', 'subreddit_id']].to_pandas()),\n",
    "    columns=l_emb_cols,\n",
    ").reset_index()\n",
    "print(_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e70d4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pd.equals(_pl.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88d35317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.5 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "_pl = (\n",
    "    df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "    .with_columns(\n",
    "        pl.DataFrame(\n",
    "            data=((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "            schema=l_emb_cols,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af2cb233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.3 ms ± 820 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_pd = pd.DataFrame(\n",
    "    ((df_user_emb_tmp_test['tos_30_pct'].to_numpy()) * df_user_emb_tmp_test[l_emb_cols].to_numpy().T).T,\n",
    "    index=pd.MultiIndex.from_frame(df_user_emb_tmp_test[['user_id', 'subreddit_id']].to_pandas()),\n",
    "    columns=l_emb_cols,\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d7f56",
   "metadata": {},
   "source": [
    "## Lazy calculation for weighted embeddings \n",
    "Lazy give us less info in between stages, but it *could* be faster (in theory) because the query optimizer would have more/better info for the whole DAG.\n",
    "\n",
    "However, NVM for now. I'm using numpy in the middle to get the weighted values, so we need eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac6fc528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# info(f\"Append subreddit embeddings to user ToS\")\n",
    "# df_user_emb_tmp_test = (\n",
    "#     df_user_tos\n",
    "#     .lazy()\n",
    "#     .filter(\n",
    "#         df_user_tos['user_id'].is_in(sample_users)\n",
    "#     )\n",
    "#     .join(\n",
    "#         df_sub_emb.drop('subreddit_name').lazy(),\n",
    "#         how='left',\n",
    "#         on=['subreddit_id']\n",
    "#     )\n",
    "#     # Drop or replace nulls as early as possible\n",
    "#     .drop_nulls()\n",
    "# )\n",
    "# # Can't get shape of a lazy df\n",
    "# # info(f\"{df_user_emb_tmp_test.shape} <- df.shape ToS with raw embeddings (no nulls)\")\n",
    "\n",
    "# info(f\"Compute weighted avg embedding for user-level\")\n",
    "# df_user_emb_tmp_wt_test = (\n",
    "#     df_user_emb_tmp_test\n",
    "#     .select(['user_id', 'subreddit_id'])\n",
    "#     .with_columns(\n",
    "#         pl.DataFrame(\n",
    "#             data=((df_user_emb_tmp_test.select(['tos_30_pct']).to_numpy()) * df_user_emb_tmp_test.select([l_emb_cols]).to_numpy().T).T,\n",
    "#             schema=l_emb_cols,\n",
    "#         ).lazy()\n",
    "#     )\n",
    "# )\n",
    "# # info(f\"{df_user_emb_tmp_wt_test.shape} <- df.shape ToS with weighted embedddings\")\n",
    "\n",
    "\n",
    "# info(f\"Compute AGGREGATED user-level embedding...\")\n",
    "# df_user_emb_test = (\n",
    "#     pl.DataFrame(df_user_emb_tmp_wt_test_pd)\n",
    "#     .groupby(\n",
    "#         ['user_id']\n",
    "#     )\n",
    "#     .agg(\n",
    "#         [\n",
    "#             pl.col(l_emb_cols).mean()\n",
    "#         ]\n",
    "#     )\n",
    "#     .collect()\n",
    "# )\n",
    "# info(f\"{df_user_emb_test.shape} <- df.shape User weighted embedddings\")\n",
    "# print(df_user_emb_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c48de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 8.84 s, total: 20.5 s\n",
      "Wall time: 3.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8982315, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_mult = np.multiply(\n",
    "    df_user_emb_tmp['tos_30_pct'].to_numpy(), df_user_emb_tmp[l_emb_cols].to_numpy().T\n",
    ").T\n",
    "test_mult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41e412b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 8.58 s, total: 20.2 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_mult_ = dask.delayed(np.multiply)(\n",
    "    dask.delayed(df_user_emb_tmp['tos_30_pct'].to_numpy()), \n",
    "    dask.delayed(df_user_emb_tmp[l_emb_cols].to_numpy().T)\n",
    ").T\n",
    "test_mult_.visualize()\n",
    "test_mult_del_ = test_mult_.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67401938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8982315, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mult_del_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3339f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 10.8 s, total: 30.1 s\n",
      "Wall time: 12.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.allclose(\n",
    "    test_mult,\n",
    "    df_user_emb_tmp_wt[l_emb_cols].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8be32",
   "metadata": {},
   "source": [
    "### We would need the weight computation in polars instead of numpy\n",
    "\n",
    "\n",
    "a) I tried using .transpose() and multiplying, but for some reason this results in `Panic` errors.\n",
    "\n",
    "b) Exploding & aggregating _could_ be faster (maybe) but in reality we lose a lot of time exploding & re-aggregating.\n",
    "\n",
    "- https://stackoverflow.com/questions/74372173/python-polars-how-to-multiply-each-element-in-a-list-with-a-value-in-a-differen\n",
    "- https://stackoverflow.com/questions/72490297/rust-polars-is-it-possible-to-explode-a-list-column-into-multiple-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff73f87",
   "metadata": {},
   "source": [
    "#### A - Transpose (breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c23dcd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 67217)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_emb_tmp_test[l_emb_cols].transpose().shape  # [:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8882b015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67217,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_emb_tmp_test['tos_30_pct'].shape  # [:50] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## this one uses all cores for a while, but doesn't finish processsing (I've had to shut down the kernel)\n",
    "pl.DataFrame(\n",
    "            data=((df_user_emb_tmp['tos_30_pct']) * df_user_emb_tmp[l_emb_cols].transpose()).transpose(),\n",
    "            schema=l_emb_cols,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6c4a789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose multiplication works for a subset of rows (50, 5000), but it stops working for some reason\n",
    "#. when we try on all rows\n",
    "\n",
    "# df_user_emb_tmp_test['tos_30_pct'][:5000] * df_user_emb_tmp_test[l_emb_cols].transpose()[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5b17fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # try with only polars (skip .to_numpy)\n",
    "\n",
    "# _pl = (\n",
    "#     df_user_emb_tmp_test[['user_id', 'subreddit_id']]\n",
    "#     .with_columns(\n",
    "#         pl.DataFrame(\n",
    "#             data=((df_user_emb_tmp_test['tos_30_pct']) * df_user_emb_tmp_test[l_emb_cols].transpose()).transpose(),\n",
    "#             schema=l_emb_cols,\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "# print(_pl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f53088",
   "metadata": {},
   "source": [
    "#### B - Explode + Aggregate (too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "50132e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:26:06 | INFO | \"Append subreddit embeddings to user ToS\"\n",
      "05:26:23 | INFO | \"(67217, 4) <- df.shape ToS with raw embeddings (no nulls)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 214 ms, total: 18 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "info(f\"Append subreddit embeddings to user ToS\")\n",
    "df_user_emb_tmp_test_pl = (\n",
    "    df_user_tos\n",
    "    .filter(\n",
    "        df_user_tos['user_id'].is_in(sample_users)\n",
    "    )\n",
    "    .join(\n",
    "        pl.DataFrame(df_sub_emb_raw).drop('subreddit_name'),\n",
    "        how='left',\n",
    "        on=['subreddit_id']\n",
    "    )\n",
    "    # Drop or replace nulls as early as possible\n",
    "    .drop_nulls()\n",
    ")\n",
    "info(f\"{df_user_emb_tmp_test_pl.shape} <- df.shape ToS with raw embeddings (no nulls)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9ffb7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub_emb_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b384cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.92 s, sys: 1.05 s, total: 4.97 s\n",
      "Wall time: 285 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (67217, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>subreddit_id</th><th>embedding_weighted</th></tr><tr><td>str</td><td>str</td><td>list[list[f64]]</td></tr></thead><tbody><tr><td>&quot;t2_100mfj&quot;</td><td>&quot;t5_2sa3m&quot;</td><td>[[-0.000067, 0.00002, … 0.000013]]</td></tr><tr><td>&quot;t2_100mfj&quot;</td><td>&quot;t5_2vz7o&quot;</td><td>[[0.000039, 0.000017, … 0.000075]]</td></tr><tr><td>&quot;t2_10cirt&quot;</td><td>&quot;t5_5pi0ey&quot;</td><td>[[-0.000163, -0.000038, … 0.000118]]</td></tr><tr><td>&quot;t2_10d6rk&quot;</td><td>&quot;t5_2qh87&quot;</td><td>[[-0.000071, 0.000045, … 0.000072]]</td></tr><tr><td>&quot;t2_10lgjt&quot;</td><td>&quot;t5_48t85&quot;</td><td>[[-0.000166, 0.00008, … 0.000018]]</td></tr><tr><td>&quot;t2_10jwj7&quot;</td><td>&quot;t5_2v0i1&quot;</td><td>[[-0.000375, 0.000083, … 0.000145]]</td></tr><tr><td>&quot;t2_10obml&quot;</td><td>&quot;t5_5m9f4s&quot;</td><td>[[-0.00058, 0.000096, … -0.000055]]</td></tr><tr><td>&quot;t2_10s6suic&quot;</td><td>&quot;t5_2qnwb&quot;</td><td>[[-0.000425, 0.000027, … 0.000334]]</td></tr><tr><td>&quot;t2_11418c&quot;</td><td>&quot;t5_2qhor&quot;</td><td>[[-0.000061, 0.000008, … 0.000122]]</td></tr><tr><td>&quot;t2_11onq0&quot;</td><td>&quot;t5_2uao3&quot;</td><td>[[-0.01, 0.004394, … -0.000414]]</td></tr><tr><td>&quot;t2_11gkiu&quot;</td><td>&quot;t5_2s7xv&quot;</td><td>[[-0.00138, 0.00169, … -0.000484]]</td></tr><tr><td>&quot;t2_113gbl&quot;</td><td>&quot;t5_2qig7&quot;</td><td>[[0.000063, 0.000166, … 0.000379]]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;t2_14u8aw7&quot;</td><td>&quot;t5_3auim&quot;</td><td>[[0.000136, 0.000261, … 0.000094]]</td></tr><tr><td>&quot;t2_14sw8s98&quot;</td><td>&quot;t5_2s5ei&quot;</td><td>[[-0.000091, -0.000059, … 0.000065]]</td></tr><tr><td>&quot;t2_14t2r9&quot;</td><td>&quot;t5_5bm4a4&quot;</td><td>[[-0.000093, 0.000066, … 0.000038]]</td></tr><tr><td>&quot;t2_14fevy&quot;</td><td>&quot;t5_2sxpk&quot;</td><td>[[-0.002865, -0.001526, … 0.000654]]</td></tr><tr><td>&quot;t2_151c1j&quot;</td><td>&quot;t5_2s8fe&quot;</td><td>[[-0.00004, 0.000262, … 0.000267]]</td></tr><tr><td>&quot;t2_14hz76&quot;</td><td>&quot;t5_2tpr6&quot;</td><td>[[-0.000334, 0.000136, … -0.000729]]</td></tr><tr><td>&quot;t2_14hz76&quot;</td><td>&quot;t5_2rgzp&quot;</td><td>[[-0.000031, 0.000021, … -0.000029]]</td></tr><tr><td>&quot;t2_14hrssg0&quot;</td><td>&quot;t5_56ibls&quot;</td><td>[[0.000038, 0.000032, … 0.000019]]</td></tr><tr><td>&quot;t2_153r3h&quot;</td><td>&quot;t5_2v28g&quot;</td><td>[[0.000046, 0.000324, … 0.000599]]</td></tr><tr><td>&quot;t2_153r3h&quot;</td><td>&quot;t5_pcxm7&quot;</td><td>[[0.000061, 0.000036, … 0.000011]]</td></tr><tr><td>&quot;t2_15duwx&quot;</td><td>&quot;t5_5gug25&quot;</td><td>[[-0.000092, -0.000202, … 0.000211]]</td></tr><tr><td>&quot;t2_15duwx&quot;</td><td>&quot;t5_2xbus&quot;</td><td>[[-0.00083, 0.000293, … 0.000502]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (67_217, 3)\n",
       "┌───────────┬──────────────┬───────────────────────────────────┐\n",
       "│ user_id   ┆ subreddit_id ┆ embedding_weighted                │\n",
       "│ ---       ┆ ---          ┆ ---                               │\n",
       "│ str       ┆ str          ┆ list[list[f64]]                   │\n",
       "╞═══════════╪══════════════╪═══════════════════════════════════╡\n",
       "│ t2_100mfj ┆ t5_2sa3m     ┆ [[-0.000067, 0.00002, … 0.000013… │\n",
       "│ t2_100mfj ┆ t5_2vz7o     ┆ [[0.000039, 0.000017, … 0.000075… │\n",
       "│ t2_10cirt ┆ t5_5pi0ey    ┆ [[-0.000163, -0.000038, … 0.0001… │\n",
       "│ t2_10d6rk ┆ t5_2qh87     ┆ [[-0.000071, 0.000045, … 0.00007… │\n",
       "│ …         ┆ …            ┆ …                                 │\n",
       "│ t2_153r3h ┆ t5_2v28g     ┆ [[0.000046, 0.000324, … 0.000599… │\n",
       "│ t2_153r3h ┆ t5_pcxm7     ┆ [[0.000061, 0.000036, … 0.000011… │\n",
       "│ t2_15duwx ┆ t5_5gug25    ┆ [[-0.000092, -0.000202, … 0.0002… │\n",
       "│ t2_15duwx ┆ t5_2xbus     ┆ [[-0.00083, 0.000293, … 0.000502… │\n",
       "└───────────┴──────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "c_emb_weighted = 'embedding_weighted'\n",
    "\n",
    "(\n",
    "    df_user_emb_tmp_test_pl  #.head()\n",
    "    # reshape to get weighted embeddings (long). Transpose the embeddings to long\n",
    "    .explode('embedding')\n",
    "    .with_columns(\n",
    "        **{c_emb_weighted: pl.col('tos_30_pct') * pl.col('embedding')}\n",
    "    )\n",
    "    # reshape back to get subreddit-level embedding. Transpose the embeddings back to wide\n",
    "    .groupby(['user_id', 'subreddit_id'])\n",
    "    .agg([\n",
    "        pl.col(c_emb_weighted).list(),\n",
    "        # pl.col('embedding').list()  # delete after testing\n",
    "    ])\n",
    "    # expand embeddings column-wise to get 1 col per embedding element \n",
    "    # get mean per user\n",
    "#     .groupby(\n",
    "#         ['user_id']\n",
    "#     )\n",
    "#     .agg(\n",
    "#         [\n",
    "#             pl.col(l_emb_cols).mean()\n",
    "#         ]\n",
    "#     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fcae13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
