{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c65e37f",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-07-28: Run it on the top Subreddits + German subs. Ideally this should help us find counterpart subs in other languages.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b0ff4",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2aea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80e191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.3.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95d83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54d024",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af5c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2857437",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25dd7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d493341",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ed01c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "All devices:\n",
      "===\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6261311781606232107\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14842963934755766732\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13515039862361315005\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14676252416\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 269525325704656488\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "    f\"\\n\\nAll devices:\\n===\\n\"\n",
    "    f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff4c44",
   "metadata": {},
   "source": [
    "# Call function to vectorize text\n",
    "\n",
    "- Batch of: 3000 \n",
    "- Limit characters to: 1000\n",
    "Finally leaves enough room to use around 50% of RAM (of 60GB)\n",
    "\n",
    "The problem is that each iteration takes around 3 minutes, which means whole job for GERMAN only will tka around 4:42 hours:mins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e302d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.3.2_use_multi_inference_test'\n",
    "mlflow_experiment_full = 'v0.3.2_use_multi_inference'\n",
    "\n",
    "bucket_name = 'i18n-subreddit-clustering'\n",
    "subreddits_path = \"subreddits/top/2021-07-16\"\n",
    "posts_path = 'posts/top/2021-07-16'\n",
    "comments_path = 'comments/top/2021-07-09'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1fa25",
   "metadata": {},
   "source": [
    "```\n",
    "When subreddit_id column was missing:\n",
    "CPU times: user 75.8 ms, sys: 21.1 ms, total: 96.9 ms\n",
    "Wall time: 884 ms\n",
    "(3767, 28)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4104937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns in subreddit meta...\n",
    "# %%time\n",
    "\n",
    "# df_subs = pd.read_parquet(\n",
    "#     path=f\"gs://{bucket_name}/{subreddits_path}\",\n",
    "#     # columns=l_cols_subreddits,\n",
    "# )\n",
    "# df_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf9b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd97085",
   "metadata": {},
   "source": [
    "## Test on a `sample` of posts & comments to make sure entire process works first (before running long job)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune.\n",
    "\n",
    "```\n",
    "08:27:18 | INFO | \"Start vectorize function\"\n",
    "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
    "08:27:18 | INFO | \"Loading df_posts...\n",
    "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
    "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
    "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
    "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
    "08:27:27 | INFO | \"Load comments df...\"\n",
    "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
    "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
    "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
    "08:28:11 | INFO | \"Load subreddits df...\"\n",
    "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "...\n",
    "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.67s/it]\n",
    "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
    "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
    "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:00<00:00, 2.42it/s]\n",
    "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
    "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
    "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "3/3 [00:01<00:00, 1.95it/s]\n",
    "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1c8c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:27:18 | INFO | \"Start vectorize function\"\n",
      "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
      "08:27:18 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
      "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
      "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
      "08:27:27 | INFO | \"Load comments df...\"\n",
      "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
      "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
      "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
      "08:28:11 | INFO | \"Load subreddits df...\"\n",
      "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "08:28:12 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "08:28:13 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n",
      "08:28:15 | INFO | \"  0:00:02.332050 <- Load TF HUB model time elapsed\"\n",
      "08:28:15 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "08:28:15 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58afea80a013400398b2d1bb34c31612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
      "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb694fdaa5214dfbba2abb899debbec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
      "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4760437393924423a6bda0d222d5de81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='test_n_samples',\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2000,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=2500,\n",
    "    n_sample_comments=5100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf5d6916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3767, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pics</th>\n",
       "      <th>t5_2qh0u</th>\n",
       "      <td>-0.056925</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>-0.009723</td>\n",
       "      <td>-0.009849</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.045963</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>-0.061319</td>\n",
       "      <td>0.053243</td>\n",
       "      <td>-0.052809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <th>t5_2qh33</th>\n",
       "      <td>0.045474</td>\n",
       "      <td>-0.039333</td>\n",
       "      <td>-0.031790</td>\n",
       "      <td>-0.015574</td>\n",
       "      <td>0.074503</td>\n",
       "      <td>0.054003</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.061827</td>\n",
       "      <td>-0.050316</td>\n",
       "      <td>0.023417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memes</th>\n",
       "      <th>t5_2qjpg</th>\n",
       "      <td>-0.014767</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>-0.069566</td>\n",
       "      <td>-0.022420</td>\n",
       "      <td>0.063016</td>\n",
       "      <td>0.066394</td>\n",
       "      <td>-0.061886</td>\n",
       "      <td>0.040540</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.027958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <th>t5_2qh3l</th>\n",
       "      <td>-0.066339</td>\n",
       "      <td>0.056393</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>-0.021127</td>\n",
       "      <td>0.076642</td>\n",
       "      <td>0.040693</td>\n",
       "      <td>0.019423</td>\n",
       "      <td>0.054693</td>\n",
       "      <td>-0.012191</td>\n",
       "      <td>0.065671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interestingasfuck</th>\n",
       "      <th>t5_2qhsa</th>\n",
       "      <td>-0.020677</td>\n",
       "      <td>0.061429</td>\n",
       "      <td>-0.029565</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>0.066374</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>0.069265</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.044498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name    subreddit_id                                                                                                                                            \n",
       "pics              t5_2qh0u         -0.056925      0.027936     -0.009723     -0.009849      0.043200      0.045963      0.049922     -0.061319      0.053243     -0.052809\n",
       "funny             t5_2qh33          0.045474     -0.039333     -0.031790     -0.015574      0.074503      0.054003      0.007870      0.061827     -0.050316      0.023417\n",
       "memes             t5_2qjpg         -0.014767      0.018347     -0.069566     -0.022420      0.063016      0.066394     -0.061886      0.040540      0.019350      0.027958\n",
       "news              t5_2qh3l         -0.066339      0.056393      0.036245     -0.021127      0.076642      0.040693      0.019423      0.054693     -0.012191      0.065671\n",
       "interestingasfuck t5_2qhsa         -0.020677      0.061429     -0.029565      0.029978      0.066374      0.061271      0.069265      0.028228      0.004899      0.044498"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect_subs.shape)\n",
    "df_vect_subs.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b96b6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stocks</th>\n",
       "      <th>t5_2qjfk</th>\n",
       "      <th>t3_o9h0x8</th>\n",
       "      <td>-0.054105</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>-0.077626</td>\n",
       "      <td>-0.032931</td>\n",
       "      <td>-0.022623</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>-0.031575</td>\n",
       "      <td>-0.058212</td>\n",
       "      <td>0.042760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laptops</th>\n",
       "      <th>t5_2qoip</th>\n",
       "      <th>t3_o76rnd</th>\n",
       "      <td>-0.018935</td>\n",
       "      <td>-0.026588</td>\n",
       "      <td>0.071572</td>\n",
       "      <td>-0.015938</td>\n",
       "      <td>-0.083586</td>\n",
       "      <td>-0.082511</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>-0.015417</td>\n",
       "      <td>-0.043235</td>\n",
       "      <td>0.068179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luftraum</th>\n",
       "      <th>t5_q02q4</th>\n",
       "      <th>t3_nuwjin</th>\n",
       "      <td>-0.072257</td>\n",
       "      <td>0.020968</td>\n",
       "      <td>-0.012867</td>\n",
       "      <td>-0.042401</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.081959</td>\n",
       "      <td>-0.042031</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>-0.005116</td>\n",
       "      <td>0.025049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adultery</th>\n",
       "      <th>t5_2sjkv</th>\n",
       "      <th>t3_oi8tno</th>\n",
       "      <td>0.046559</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>0.039657</td>\n",
       "      <td>-0.066817</td>\n",
       "      <td>-0.096201</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>-0.074167</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>-0.066348</td>\n",
       "      <td>-0.029130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poopshitters</th>\n",
       "      <th>t5_wgmeb</th>\n",
       "      <th>t3_o2es6a</th>\n",
       "      <td>-0.056548</td>\n",
       "      <td>-0.018536</td>\n",
       "      <td>0.015383</td>\n",
       "      <td>-0.018324</td>\n",
       "      <td>-0.007343</td>\n",
       "      <td>0.037289</td>\n",
       "      <td>0.082599</td>\n",
       "      <td>0.029512</td>\n",
       "      <td>-0.039021</td>\n",
       "      <td>0.031015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name subreddit_id post_id                                                                                                                                              \n",
       "stocks         t5_2qjfk     t3_o9h0x8     -0.054105      0.070300     -0.077626     -0.032931     -0.022623     -0.004873      0.018130     -0.031575     -0.058212      0.042760\n",
       "laptops        t5_2qoip     t3_o76rnd     -0.018935     -0.026588      0.071572     -0.015938     -0.083586     -0.082511      0.044506     -0.015417     -0.043235      0.068179\n",
       "luftraum       t5_q02q4     t3_nuwjin     -0.072257      0.020968     -0.012867     -0.042401      0.024641      0.081959     -0.042031      0.010640     -0.005116      0.025049\n",
       "adultery       t5_2sjkv     t3_oi8tno      0.046559     -0.042573      0.039657     -0.066817     -0.096201      0.001347     -0.074167      0.002814     -0.066348     -0.029130\n",
       "poopshitters   t5_wgmeb     t3_o2es6a     -0.056548     -0.018536      0.015383     -0.018324     -0.007343      0.037289      0.082599      0.029512     -0.039021      0.031015"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect.shape)\n",
    "df_vect.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3bd9f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_502</th>\n",
       "      <th>embeddings_503</th>\n",
       "      <th>embeddings_504</th>\n",
       "      <th>embeddings_505</th>\n",
       "      <th>embeddings_506</th>\n",
       "      <th>embeddings_507</th>\n",
       "      <th>embeddings_508</th>\n",
       "      <th>embeddings_509</th>\n",
       "      <th>embeddings_510</th>\n",
       "      <th>embeddings_511</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <th>t5_2qhq6</th>\n",
       "      <th>t3_o9upy3</th>\n",
       "      <th>t1_h3djels</th>\n",
       "      <td>-0.049714</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>0.031363</td>\n",
       "      <td>-0.025456</td>\n",
       "      <td>0.020385</td>\n",
       "      <td>-0.009454</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.068287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldnews</th>\n",
       "      <th>t5_2qh13</th>\n",
       "      <th>t3_o7yf4y</th>\n",
       "      <th>t1_h333pw7</th>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.064295</td>\n",
       "      <td>-0.041862</td>\n",
       "      <td>-0.026075</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>-0.028089</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>-0.045100</td>\n",
       "      <td>0.064217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aww</th>\n",
       "      <th>t5_2qh1o</th>\n",
       "      <th>t3_nvidt5</th>\n",
       "      <th>t1_h13y1e4</th>\n",
       "      <td>0.022847</td>\n",
       "      <td>-0.084315</td>\n",
       "      <td>0.062082</td>\n",
       "      <td>-0.042768</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.021409</td>\n",
       "      <td>0.035140</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>0.112163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formula1</th>\n",
       "      <th>t5_2qimj</th>\n",
       "      <th>t3_o8beb8</th>\n",
       "      <th>t1_h33xaio</th>\n",
       "      <td>-0.042507</td>\n",
       "      <td>-0.012686</td>\n",
       "      <td>0.035909</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.034547</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>-0.020290</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.074332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaacccccccce</th>\n",
       "      <th>t5_3aa11</th>\n",
       "      <th>t3_o9igjh</th>\n",
       "      <th>t1_h3igwhh</th>\n",
       "      <td>0.045924</td>\n",
       "      <td>-0.043366</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>-0.062547</td>\n",
       "      <td>-0.043556</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>0.029582</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.036313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    embeddings_502  embeddings_503  embeddings_504  embeddings_505  embeddings_506  embeddings_507  embeddings_508  embeddings_509  embeddings_510  embeddings_511\n",
       "subreddit_name   subreddit_id post_id   comment_id                                                                                                                                                                \n",
       "yoga             t5_2qhq6     t3_o9upy3 t1_h3djels       -0.049714        0.039176        0.031363       -0.025456        0.020385       -0.009454        0.032895        0.048567        0.006216        0.068287\n",
       "worldnews        t5_2qh13     t3_o7yf4y t1_h333pw7       -0.000592       -0.064295       -0.041862       -0.026075        0.028604       -0.028089       -0.017898        0.031500       -0.045100        0.064217\n",
       "aww              t5_2qh1o     t3_nvidt5 t1_h13y1e4        0.022847       -0.084315        0.062082       -0.042768        0.049408        0.041870        0.021409        0.035140        0.034683        0.112163\n",
       "formula1         t5_2qimj     t3_o8beb8 t1_h33xaio       -0.042507       -0.012686        0.035909        0.104300        0.034547        0.009831       -0.020290        0.045840        0.037209        0.074332\n",
       "aaaaaaacccccccce t5_3aa11     t3_o9igjh t1_h3igwhh        0.045924       -0.043366        0.009658        0.004710       -0.062547       -0.043556        0.074638        0.029582        0.018934        0.036313"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect_comments.shape)\n",
    "df_vect_comments.iloc[10:15, -10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c8795",
   "metadata": {},
   "source": [
    "# Test new batching function\n",
    "\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Add new parameter to sample only first N files (we'll process each file individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbe6fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de25e4e7e7c41b19b43a33a8f94178c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts/top/2021-07-16/000000000000.parquet\n",
      "000000000000\n",
      "posts/top/2021-07-16/000000000001.parquet\n",
      "000000000001\n",
      "posts/top/2021-07-16/000000000002.parquet\n",
      "000000000002\n",
      "posts/top/2021-07-16/000000000003.parquet\n",
      "000000000003\n",
      "posts/top/2021-07-16/000000000004.parquet\n",
      "000000000004\n"
     ]
    }
   ],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "# folder = configs.gcp_storage_folder\n",
    "\n",
    "# print( str(configs.gcp_bucket) +\"/\"+ str(folder))\n",
    "for blob in tqdm(list(bucket.list_blobs(prefix=posts_path))[:5]):\n",
    "#     print(blob)\n",
    "    print(blob.name)\n",
    "    print(blob.name.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7368a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:16 | INFO | \"Start vectorize function\"\n",
      "11:22:16 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1122\"\n",
      "11:22:17 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "11:22:17 | INFO | \"  Saving config to local path...\"\n",
      "11:22:17 | INFO | \"  Logging config to mlflow...\"\n",
      "11:22:18 | INFO | \"Loading model use_multilingual...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0e75366320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:20 | INFO | \"  0:00:02.280150 <- Load TF HUB model time elapsed\"\n",
      "11:22:20 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "11:22:20 | INFO | \"Load subreddits df...\"\n",
      "11:22:21 | INFO | \"  0:00:00.619620 <- df_subs loading time elapsed\"\n",
      "11:22:21 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "11:22:21 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "11:22:21 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f7efd83a1c4e42a74681d65dd2431b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f11df85f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0c504f3b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:25 | INFO | \"  0:00:04.172970 <- df_subs vectorizing time elapsed\"\n",
      "11:22:25 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "11:22:25 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "11:22:25 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:22:25 | INFO | \"  Logging to mlflow...\"\n",
      "11:22:30 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "11:22:37 | INFO | \"  0:00:06.666186 <- df_post loading time elapsed\"\n",
      "11:22:37 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "11:22:38 | INFO | \"  Sampling posts down to: 3,500\"\n",
      "11:22:38 | INFO | \"  (3500, 6) <- df_posts.shape AFTER sampling\"\n",
      "11:22:38 | INFO | \"Vectorizing POSTS...\"\n",
      "11:22:38 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0010190095124e2abaeb9a819dd7af4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:40 | INFO | \"  0:00:01.568036 <- df_posts vectorizing time elapsed\"\n",
      "11:22:40 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "11:22:40 | INFO | \"     7.5 MB <- Memory usage\"\n",
      "11:22:40 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:22:40 | INFO | \"  Logging to mlflow...\"\n",
      "11:22:41 | INFO | \"Load comments df...\"\n",
      "11:23:11 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "11:23:23 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "11:23:26 | INFO | \"  (34463, 6) <- updated df_comments shape\"\n",
      "11:23:26 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
      "11:23:26 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
      "11:23:26 | INFO | \"Vectorizing COMMENTS...\"\n",
      "11:23:26 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe04bb0f51f4cbbbd0f50c62ff3ae98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:23:28 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "11:23:28 | INFO | \"    11.2 MB <- Memory usage\"\n",
      "11:23:28 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:23:28 | INFO | \"  Logging to mlflow...\"\n",
      "11:23:30 | INFO | \"  0:01:13.306714 <- Total vectorize fxn time elapsed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0e752c0680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=3500,\n",
    "    n_sample_comments=5100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4166c2",
   "metadata": {},
   "source": [
    "### Timing is super fast, even with a bigger sample size\n",
    "\n",
    "```\n",
    "11:40:06 | INFO | \"Start vectorize function\"\n",
    "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
    "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
    "11:40:07 | INFO | \"  Saving config to local path...\"\n",
    "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
    "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
    "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
    "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
    "11:40:10 | INFO | \"Load subreddits df...\"\n",
    "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
    "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.65s/it]\n",
    "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
    "...\n",
    "11:40:16 | INFO | \"Loading df_posts...\n",
    "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
    "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
    "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
    "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
    "100%\n",
    "5/5 [00:03<00:00, 1.59it/s]\n",
    "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
    "...\n",
    "11:40:30 | INFO | \"Load comments df...\"\n",
    "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
    "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
    "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
    "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "100%\n",
    "10/10 [00:05<00:00, 1.60it/s]\n",
    "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
    "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
    "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
    "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
    "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d668707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:06 | INFO | \"Start vectorize function\"\n",
      "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
      "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "11:40:07 | INFO | \"  Saving config to local path...\"\n",
      "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
      "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
      "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
      "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "11:40:10 | INFO | \"Load subreddits df...\"\n",
      "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
      "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "11:40:11 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a853b78b494da89d4c83135adfce67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
      "11:40:15 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "11:40:15 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "11:40:15 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:40:15 | INFO | \"  Logging to mlflow...\"\n",
      "11:40:16 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
      "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
      "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
      "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
      "11:40:24 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6938f9b6bbda43f499c61fff514fcf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
      "11:40:28 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "11:40:28 | INFO | \"    20.3 MB <- Memory usage\"\n",
      "11:40:28 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:40:28 | INFO | \"  Logging to mlflow...\"\n",
      "11:40:30 | INFO | \"Load comments df...\"\n",
      "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
      "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
      "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
      "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "11:41:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf694d1cb2c4541b466f2d1cd7b025b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
      "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
      "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
      "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=9500,\n",
    "    n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5585e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23e9b2",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=False`\n",
    "Let's see if the current refactor is good enough or if I really need to manually batch files...\n",
    "\n",
    "**answer**: no it wasn't good enough -- 60GB of RAM wasn't good enough for 19Million comments _lol_.\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c06a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:53 | INFO | \"Start vectorize function\"\n",
      "11:49:53 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1149\"\n",
      "11:49:53 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "11:49:54 | INFO | \"  Saving config to local path...\"\n",
      "11:49:54 | INFO | \"  Logging config to mlflow...\"\n",
      "11:49:54 | INFO | \"Loading model use_multilingual...\"\n",
      "11:49:57 | INFO | \"  0:00:02.616759 <- Load TF HUB model time elapsed\"\n",
      "11:49:57 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "11:49:57 | INFO | \"Load subreddits df...\"\n",
      "11:49:58 | INFO | \"  0:00:01.091284 <- df_subs loading time elapsed\"\n",
      "11:49:58 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "11:49:58 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "11:49:58 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8322b373616e456d953d36a4bf818fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:50:02 | INFO | \"  0:00:04.246728 <- df_subs vectorizing time elapsed\"\n",
      "11:50:02 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "11:50:02 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "11:50:02 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:50:03 | INFO | \"  Logging to mlflow...\"\n",
      "11:50:04 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "11:50:11 | INFO | \"  0:00:07.403893 <- df_post loading time elapsed\"\n",
      "11:50:11 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "11:50:12 | INFO | \"Vectorizing POSTS...\"\n",
      "11:50:12 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94f8bc54fd2401e9b8dfca27e236ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:00:12 | INFO | \"  0:10:00.597031 <- df_posts vectorizing time elapsed\"\n",
      "12:00:14 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "12:00:14 | INFO | \"  3,532.8 MB <- Memory usage\"\n",
      "12:00:14 | INFO | \"      48\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "12:00:33 | INFO | \"  Logging to mlflow...\"\n",
      "12:01:23 | INFO | \"Load comments df...\"\n",
      "12:01:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "12:02:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
      "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a34052a0d04e2c93683695ceea5071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9939c221b377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtf_batch_inference_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtf_limit_first_n_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     n_sample_posts=9500,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/vectorize_text_tf.py\u001b[0m in \u001b[0;36mvectorize_text_to_embeddings\u001b[0;34m(mlflow_experiment, model_name, run_name, tokenize_lowercase, bucket_name, subreddits_path, posts_path, comments_path, preprocess_text_folder, col_text_post, col_text_post_word_count, col_text_post_url, col_post_id, col_comment_id, col_text_comment, col_text_comment_word_count, col_subreddit_id, col_text_subreddit_description, col_text_subreddit_word_count, tf_batch_inference_rows, tf_limit_first_n_chars, n_sample_post_files, n_sample_comment_files, n_sample_posts, n_sample_comments)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mlowercase_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_lowercase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_batch_inference_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mlimit_first_n_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_limit_first_n_chars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         )\n\u001b[1;32m    282\u001b[0m         \u001b[0mtotal_time_comms_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_start_comms_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'df_posts vectorizing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/vectorize_text_tf.py\u001b[0m in \u001b[0;36mget_embeddings_as_df\u001b[0;34m(model, df, col_text, cols_index, col_embeddings_prefix, lowercase_text, batch_size, limit_first_n_chars)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 )\n\u001b[1;32m    393\u001b[0m             )\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcol_embeddings_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mdf_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_df_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 521\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc3df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c05721",
   "metadata": {},
   "source": [
    "## Test - Re-do comments with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56869767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:09 | INFO | \"Start vectorize function\"\n",
      "18:46:09 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1846\"\n",
      "18:46:10 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "18:46:10 | INFO | \"  Saving config to local path...\"\n",
      "18:46:10 | INFO | \"  Logging config to mlflow...\"\n",
      "18:46:10 | INFO | \"Loading model use_multilingual...\"\n",
      "18:46:13 | INFO | \"  0:00:02.357815 <- Load TF HUB model time elapsed\"\n",
      "18:46:13 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "18:46:13 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "18:46:13 | INFO | \"-- Loading & vectorizing COMMENTS in files: 5 --\n",
      "Expected batch size: 6100\"\n",
      "18:46:13 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84492a2519774182a86f2a4cd938e066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:14 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:14 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3221c94358c649c5a60bcb8fe7f785c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:19 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 9,821 Rows by 516 Cols\"\n",
      "18:46:20 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:20 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba1046f70d04300b9605bfb33a58e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:24 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 9,821 Rows by 516 Cols\"\n",
      "18:46:26 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:26 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a50db459df41b8a69005cbd9a49461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:30 | INFO | \"  Saving to local: df_vect_comments/000000000002 | 9,821 Rows by 516 Cols\"\n",
      "18:46:31 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:31 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c98bf643324447b7d5153db9045a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:35 | INFO | \"  Saving to local: df_vect_comments/000000000003 | 9,821 Rows by 516 Cols\"\n",
      "18:46:37 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:37 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb032f6527e4241991213c5d941c624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:41 | INFO | \"  Saving to local: df_vect_comments/000000000004 | 9,821 Rows by 516 Cols\"\n",
      "18:46:44 | INFO | \"  0:00:34.354815 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "#     subreddits_path=subreddits_path,\n",
    "#     posts_path=posts_path,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=6100,\n",
    "    tf_limit_first_n_chars=750,\n",
    "    \n",
    "    n_sample_comment_files=5,\n",
    "    n_sample_comments=49100,\n",
    "#     n_sample_posts=9500,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e1aee",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a27b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:59:48 | INFO | \"Start vectorize function\"\n",
      "18:59:48 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1859\"\n",
      "18:59:48 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "18:59:49 | INFO | \"  Saving config to local path...\"\n",
      "18:59:49 | INFO | \"  Logging config to mlflow...\"\n",
      "18:59:49 | INFO | \"Loading model use_multilingual...\"\n",
      "18:59:51 | INFO | \"  0:00:02.346687 <- Load TF HUB model time elapsed\"\n",
      "18:59:51 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "18:59:51 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "18:59:51 | INFO | \"-- Loading & vectorizing COMMENTS in files: 37 --\n",
      "Expected batch size: 6100\"\n",
      "18:59:51 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752cc60339d94a19825ef65a3537117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:59:51 | INFO | \"Processing: comments/top/2021-07-09/000000000000.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27748cdd4d44c4c8820ecb48b39c9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:03:19 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 546,915 Rows by 516 Cols\"\n",
      "19:03:29 | INFO | \"Processing: comments/top/2021-07-09/000000000001.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5cbe2edede4ed0b2064812b7b153b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:09 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 582,195 Rows by 516 Cols\"\n",
      "19:07:21 | INFO | \"Processing: comments/top/2021-07-09/000000000002.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d0f37b78aa4b4e9930fcbbb52ebc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:10:25 | INFO | \"  Saving to local: df_vect_comments/000000000002 | 478,463 Rows by 516 Cols\"\n",
      "19:10:36 | INFO | \"Processing: comments/top/2021-07-09/000000000003.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45edb34c46e44cb2837359623db1cf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:13:28 | INFO | \"  Saving to local: df_vect_comments/000000000003 | 467,274 Rows by 516 Cols\"\n",
      "19:13:39 | INFO | \"Processing: comments/top/2021-07-09/000000000004.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca214f227502467e8957c469d10ea908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:17:15 | INFO | \"  Saving to local: df_vect_comments/000000000004 | 584,222 Rows by 516 Cols\"\n",
      "19:17:26 | INFO | \"Processing: comments/top/2021-07-09/000000000005.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c9049da8ff4ba8aba91e9ffcc40d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:20:52 | INFO | \"  Saving to local: df_vect_comments/000000000005 | 551,542 Rows by 516 Cols\"\n",
      "19:21:03 | INFO | \"Processing: comments/top/2021-07-09/000000000006.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e255d7e48d474187836fdb9d800aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:24:43 | INFO | \"  Saving to local: df_vect_comments/000000000006 | 588,569 Rows by 516 Cols\"\n",
      "19:24:54 | INFO | \"Processing: comments/top/2021-07-09/000000000007.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cd631eccbc4f859c0dfb9790c0842e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:28:20 | INFO | \"  Saving to local: df_vect_comments/000000000007 | 549,341 Rows by 516 Cols\"\n",
      "19:28:32 | INFO | \"Processing: comments/top/2021-07-09/000000000008.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f476059bfbb140a5bbdc2f45042c4c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:31:19 | INFO | \"  Saving to local: df_vect_comments/000000000008 | 447,740 Rows by 516 Cols\"\n",
      "19:31:31 | INFO | \"Processing: comments/top/2021-07-09/000000000009.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daf73f13aa2432391a2292f471c4125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:34:57 | INFO | \"  Saving to local: df_vect_comments/000000000009 | 554,171 Rows by 516 Cols\"\n",
      "19:35:09 | INFO | \"Processing: comments/top/2021-07-09/000000000010.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f035ff2bc684aebbd6c214d4ce2eb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:38:08 | INFO | \"  Saving to local: df_vect_comments/000000000010 | 478,748 Rows by 516 Cols\"\n",
      "19:38:19 | INFO | \"Processing: comments/top/2021-07-09/000000000011.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d094c52a66409db867690eac9b509b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:41:27 | INFO | \"  Saving to local: df_vect_comments/000000000011 | 506,263 Rows by 516 Cols\"\n",
      "19:41:38 | INFO | \"Processing: comments/top/2021-07-09/000000000012.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777185ac74c64b558343a49e3327be3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:44:57 | INFO | \"  Saving to local: df_vect_comments/000000000012 | 538,231 Rows by 516 Cols\"\n",
      "19:45:09 | INFO | \"Processing: comments/top/2021-07-09/000000000013.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69855f7a246445d2b013bd344f096cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:48:23 | INFO | \"  Saving to local: df_vect_comments/000000000013 | 516,219 Rows by 516 Cols\"\n",
      "19:48:35 | INFO | \"Processing: comments/top/2021-07-09/000000000014.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f560264d35384e02879a3f9897947561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:51:58 | INFO | \"  Saving to local: df_vect_comments/000000000014 | 543,914 Rows by 516 Cols\"\n",
      "19:52:10 | INFO | \"Processing: comments/top/2021-07-09/000000000015.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841174e834d245d9b28b0707d911197e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:55:46 | INFO | \"  Saving to local: df_vect_comments/000000000015 | 583,580 Rows by 516 Cols\"\n",
      "19:55:59 | INFO | \"Processing: comments/top/2021-07-09/000000000016.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5d8bc513124c288a5c7557508fd6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:59:15 | INFO | \"  Saving to local: df_vect_comments/000000000016 | 533,066 Rows by 516 Cols\"\n",
      "19:59:27 | INFO | \"Processing: comments/top/2021-07-09/000000000017.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f65f86b7493464683333341eecf4896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:02:47 | INFO | \"  Saving to local: df_vect_comments/000000000017 | 536,540 Rows by 516 Cols\"\n",
      "20:02:58 | INFO | \"Processing: comments/top/2021-07-09/000000000018.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f43b10d96f34a9e9c7d720ead3e32e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:05:50 | INFO | \"  Saving to local: df_vect_comments/000000000018 | 455,375 Rows by 516 Cols\"\n",
      "20:06:02 | INFO | \"Processing: comments/top/2021-07-09/000000000019.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b85859175f94f05b22573509f8517cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:09:34 | INFO | \"  Saving to local: df_vect_comments/000000000019 | 570,490 Rows by 516 Cols\"\n",
      "20:09:45 | INFO | \"Processing: comments/top/2021-07-09/000000000020.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d87cdc78978491989919dfe64bf2484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:13:28 | INFO | \"  Saving to local: df_vect_comments/000000000020 | 600,022 Rows by 516 Cols\"\n",
      "20:13:40 | INFO | \"Processing: comments/top/2021-07-09/000000000021.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7b4b0acab4f349b5b8c88d7145f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:16:51 | INFO | \"  Saving to local: df_vect_comments/000000000021 | 504,174 Rows by 516 Cols\"\n",
      "20:17:03 | INFO | \"Processing: comments/top/2021-07-09/000000000022.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e3e29df38b4a0b84f48193f1f6b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:20:02 | INFO | \"  Saving to local: df_vect_comments/000000000022 | 485,072 Rows by 516 Cols\"\n",
      "20:20:14 | INFO | \"Processing: comments/top/2021-07-09/000000000023.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21018fd2a75243c88353ffd2031bf9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:23:14 | INFO | \"  Saving to local: df_vect_comments/000000000023 | 482,067 Rows by 516 Cols\"\n",
      "20:23:25 | INFO | \"Processing: comments/top/2021-07-09/000000000024.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd0bf6082f142cfbd830002668018dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:26:21 | INFO | \"  Saving to local: df_vect_comments/000000000024 | 473,130 Rows by 516 Cols\"\n",
      "20:26:32 | INFO | \"Processing: comments/top/2021-07-09/000000000025.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45678526bbad40678ee78b944b0bc47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:30:01 | INFO | \"  Saving to local: df_vect_comments/000000000025 | 560,305 Rows by 516 Cols\"\n",
      "20:30:13 | INFO | \"Processing: comments/top/2021-07-09/000000000026.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195167441eef4663b78261b332c0a7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:33:21 | INFO | \"  Saving to local: df_vect_comments/000000000026 | 507,379 Rows by 516 Cols\"\n",
      "20:33:32 | INFO | \"Processing: comments/top/2021-07-09/000000000027.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2d00769f284ab885331708ed0153d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:36:48 | INFO | \"  Saving to local: df_vect_comments/000000000027 | 527,642 Rows by 516 Cols\"\n",
      "20:36:59 | INFO | \"Processing: comments/top/2021-07-09/000000000028.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcf8fdf77104f91952c5354c0642858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:07 | INFO | \"  Saving to local: df_vect_comments/000000000028 | 505,776 Rows by 516 Cols\"\n",
      "20:40:19 | INFO | \"Processing: comments/top/2021-07-09/000000000029.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc66f940d11417ab05c74e8b5682983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:43:22 | INFO | \"  Saving to local: df_vect_comments/000000000029 | 490,102 Rows by 516 Cols\"\n",
      "20:43:34 | INFO | \"Processing: comments/top/2021-07-09/000000000030.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ade99dfc8114542a0740180c43799fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:47:08 | INFO | \"  Saving to local: df_vect_comments/000000000030 | 572,696 Rows by 516 Cols\"\n",
      "20:47:20 | INFO | \"Processing: comments/top/2021-07-09/000000000031.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b745157947b14365bcf5dd3f39074bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:49:42 | INFO | \"  Saving to local: df_vect_comments/000000000031 | 375,509 Rows by 516 Cols\"\n",
      "20:49:53 | INFO | \"Processing: comments/top/2021-07-09/000000000032.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3cd4bb7d764cb692e4cf10a4b0d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:53:17 | INFO | \"  Saving to local: df_vect_comments/000000000032 | 539,335 Rows by 516 Cols\"\n",
      "20:53:29 | INFO | \"Processing: comments/top/2021-07-09/000000000033.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74abf71b64e94f219ccedd3db1134867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:56:23 | INFO | \"  Saving to local: df_vect_comments/000000000033 | 468,589 Rows by 516 Cols\"\n",
      "20:56:34 | INFO | \"Processing: comments/top/2021-07-09/000000000034.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2ed9fecac64df69421baffa6c635ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"new_batch_fxn_{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "#     subreddits_path=subreddits_path,\n",
    "#     posts_path=posts_path,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=6100,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    n_sample_comments=None,\n",
    "#     n_sample_posts=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcaa3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a025209b",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c1cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c06906",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd01536",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f47b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf1743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489cec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eaf6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37128a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4178c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a453eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149bc585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743cd47d",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e02d8131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f30f9",
   "metadata": {},
   "source": [
    "# Check mlflow experiment & Read artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ee22978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mlf_exp = mlf.list_experiment_meta(output_format='pandas')\n",
    "df_mlf_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d1e04",
   "metadata": {},
   "source": [
    "## Check runs in experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c8f5e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.df_vect_subreddits_description_cols</th>\n",
       "      <th>metrics.df_vect_subreddits_description_rows</th>\n",
       "      <th>metrics.vectorizing_time_minutes</th>\n",
       "      <th>metrics.df_vect_comments_rows</th>\n",
       "      <th>metrics.df_vect_comments_cols</th>\n",
       "      <th>metrics.df_vect_posts_cols</th>\n",
       "      <th>metrics.df_vect_posts_rows</th>\n",
       "      <th>params.model_location</th>\n",
       "      <th>params.col_post_id</th>\n",
       "      <th>params.bucket_name</th>\n",
       "      <th>params.tokenize_lowercase</th>\n",
       "      <th>params.posts_path</th>\n",
       "      <th>params.col_comment_id</th>\n",
       "      <th>params.col_text_post</th>\n",
       "      <th>params.preprocess_text_folder</th>\n",
       "      <th>params.tf_limit_first_n_chars</th>\n",
       "      <th>params.subreddits_path</th>\n",
       "      <th>params.n_sample_comments</th>\n",
       "      <th>params.col_subreddit_id</th>\n",
       "      <th>params.col_text_subreddit_word_count</th>\n",
       "      <th>params.col_text_comment_word_count</th>\n",
       "      <th>params.comments_path</th>\n",
       "      <th>params.tokenize_function</th>\n",
       "      <th>params.col_text_post_word_count</th>\n",
       "      <th>params.col_text_subreddit_description</th>\n",
       "      <th>params.col_text_post_url</th>\n",
       "      <th>params.host_name</th>\n",
       "      <th>params.model_name</th>\n",
       "      <th>params.n_sample_posts</th>\n",
       "      <th>params.tf_batch_inference_rows</th>\n",
       "      <th>params.col_text_comment</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>tags.host_name</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e97065c83674451acca4eb66ea8b5f7</td>\n",
       "      <td>9</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9/5e97065c83674451acca4eb66ea8b5f7/artifacts</td>\n",
       "      <td>2021-07-29 08:28:12.764000+00:00</td>\n",
       "      <td>2021-07-29 08:28:25.132000+00:00</td>\n",
       "      <td>512.0</td>\n",
       "      <td>3767.0</td>\n",
       "      <td>1.109042</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>https://tfhub.dev/google/universal-sentence-encoder-multilingual/3</td>\n",
       "      <td>post_id</td>\n",
       "      <td>i18n-subreddit-clustering</td>\n",
       "      <td>True</td>\n",
       "      <td>posts/top/2021-07-16</td>\n",
       "      <td>comment_id</td>\n",
       "      <td>text</td>\n",
       "      <td>None</td>\n",
       "      <td>1000</td>\n",
       "      <td>subreddits/top/2021-07-16</td>\n",
       "      <td>5100</td>\n",
       "      <td>subreddit_id</td>\n",
       "      <td>subreddit_name_title_and_clean_descriptions_word_count</td>\n",
       "      <td>comment_text_word_count</td>\n",
       "      <td>comments/top/2021-07-09</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>text_word_count</td>\n",
       "      <td>subreddit_name_title_and_clean_descriptions</td>\n",
       "      <td>post_url_for_embeddings</td>\n",
       "      <td>djb-subclu-inference-tf-2-3-20210630</td>\n",
       "      <td>use_multilingual</td>\n",
       "      <td>2500</td>\n",
       "      <td>2000</td>\n",
       "      <td>comment_body_text</td>\n",
       "      <td>636ffe8ca480035297dfc650c1c002676ceb5aa6</td>\n",
       "      <td>test_n_samples</td>\n",
       "      <td>djb-subclu-inference-tf-2-3-20210630</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>jupyter</td>\n",
       "      <td>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4034ea26c5bf45f980a3132e9b28f677</td>\n",
       "      <td>9</td>\n",
       "      <td>KILLED</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9/4034ea26c5bf45f980a3132e9b28f677/artifacts</td>\n",
       "      <td>2021-07-29 08:24:53.166000+00:00</td>\n",
       "      <td>2021-07-29 08:27:18.525000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://tfhub.dev/google/universal-sentence-encoder-multilingual/3</td>\n",
       "      <td>post_id</td>\n",
       "      <td>i18n-subreddit-clustering</td>\n",
       "      <td>True</td>\n",
       "      <td>posts/top/2021-07-16</td>\n",
       "      <td>comment_id</td>\n",
       "      <td>text</td>\n",
       "      <td>None</td>\n",
       "      <td>1000</td>\n",
       "      <td>subreddits/top/2021-07-16</td>\n",
       "      <td>2100</td>\n",
       "      <td>subreddit_id</td>\n",
       "      <td>subreddit_name_title_and_clean_descriptions_word_count</td>\n",
       "      <td>comment_text_word_count</td>\n",
       "      <td>comments/top/2021-07-09</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>text_word_count</td>\n",
       "      <td>subreddit_name_title_and_clean_descriptions</td>\n",
       "      <td>post_url_for_embeddings</td>\n",
       "      <td>djb-subclu-inference-tf-2-3-20210630</td>\n",
       "      <td>use_multilingual</td>\n",
       "      <td>1500</td>\n",
       "      <td>3000</td>\n",
       "      <td>comment_body_text</td>\n",
       "      <td>636ffe8ca480035297dfc650c1c002676ceb5aa6</td>\n",
       "      <td>test_n_samples</td>\n",
       "      <td>djb-subclu-inference-tf-2-3-20210630</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>jupyter</td>\n",
       "      <td>/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id experiment_id    status                                                                               artifact_uri                       start_time                         end_time  metrics.df_vect_subreddits_description_cols  \\\n",
       "0  5e97065c83674451acca4eb66ea8b5f7             9  FINISHED  gs://i18n-subreddit-clustering/mlflow/mlruns/9/5e97065c83674451acca4eb66ea8b5f7/artifacts 2021-07-29 08:28:12.764000+00:00 2021-07-29 08:28:25.132000+00:00                                        512.0   \n",
       "1  4034ea26c5bf45f980a3132e9b28f677             9    KILLED  gs://i18n-subreddit-clustering/mlflow/mlruns/9/4034ea26c5bf45f980a3132e9b28f677/artifacts 2021-07-29 08:24:53.166000+00:00 2021-07-29 08:27:18.525000+00:00                                          NaN   \n",
       "\n",
       "   metrics.df_vect_subreddits_description_rows  metrics.vectorizing_time_minutes  metrics.df_vect_comments_rows  metrics.df_vect_comments_cols  metrics.df_vect_posts_cols  metrics.df_vect_posts_rows                                               params.model_location params.col_post_id  \\\n",
       "0                                       3767.0                          1.109042                         5100.0                          512.0                       512.0                      2500.0  https://tfhub.dev/google/universal-sentence-encoder-multilingual/3            post_id   \n",
       "1                                          NaN                               NaN                            NaN                            NaN                         NaN                         NaN  https://tfhub.dev/google/universal-sentence-encoder-multilingual/3            post_id   \n",
       "\n",
       "          params.bucket_name params.tokenize_lowercase     params.posts_path params.col_comment_id params.col_text_post params.preprocess_text_folder params.tf_limit_first_n_chars     params.subreddits_path params.n_sample_comments params.col_subreddit_id  \\\n",
       "0  i18n-subreddit-clustering                      True  posts/top/2021-07-16            comment_id                 text                          None                          1000  subreddits/top/2021-07-16                     5100            subreddit_id   \n",
       "1  i18n-subreddit-clustering                      True  posts/top/2021-07-16            comment_id                 text                          None                          1000  subreddits/top/2021-07-16                     2100            subreddit_id   \n",
       "\n",
       "                     params.col_text_subreddit_word_count params.col_text_comment_word_count     params.comments_path params.tokenize_function params.col_text_post_word_count        params.col_text_subreddit_description params.col_text_post_url                      params.host_name  \\\n",
       "0  subreddit_name_title_and_clean_descriptions_word_count            comment_text_word_count  comments/top/2021-07-09                  sklearn                 text_word_count  subreddit_name_title_and_clean_descriptions  post_url_for_embeddings  djb-subclu-inference-tf-2-3-20210630   \n",
       "1  subreddit_name_title_and_clean_descriptions_word_count            comment_text_word_count  comments/top/2021-07-09                  sklearn                 text_word_count  subreddit_name_title_and_clean_descriptions  post_url_for_embeddings  djb-subclu-inference-tf-2-3-20210630   \n",
       "\n",
       "  params.model_name params.n_sample_posts params.tf_batch_inference_rows params.col_text_comment             tags.mlflow.source.git.commit tags.mlflow.runName                        tags.host_name tags.mlflow.source.type tags.mlflow.user  \\\n",
       "0  use_multilingual                  2500                           2000       comment_body_text  636ffe8ca480035297dfc650c1c002676ceb5aa6      test_n_samples  djb-subclu-inference-tf-2-3-20210630                   LOCAL          jupyter   \n",
       "1  use_multilingual                  1500                           3000       comment_body_text  636ffe8ca480035297dfc650c1c002676ceb5aa6      test_n_samples  djb-subclu-inference-tf-2-3-20210630                   LOCAL          jupyter   \n",
       "\n",
       "                                        tags.mlflow.source.name  \n",
       "0  /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py  \n",
       "1  /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_id = df_mlf_exp.loc[df_mlf_exp['name'] == mlflow_experiment_test, \n",
    "                        'experiment_id'].values[0]\n",
    "\n",
    "mlf.search_all_runs(experiment_ids=[exp_id]).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fc915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 512)\n",
      "CPU times: user 169 ms, sys: 0 ns, total: 169 ms\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_id = '45201072143a4d7fbb86a2f2b7d85520'\n",
    "\n",
    "df_v_subs = mlf.read_run_artifact(\n",
    "    run_id=run_id,\n",
    "    artifact_folder='df_vect_subreddits_description',\n",
    "    read_function=pd.read_parquet,\n",
    ")\n",
    "print(df_v_subs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c73f11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(df_vect_subs, df_v_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d623b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <th>t5_22i0</th>\n",
       "      <td>-0.018191</td>\n",
       "      <td>-0.045794</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>-0.036392</td>\n",
       "      <td>0.033076</td>\n",
       "      <td>0.013654</td>\n",
       "      <td>-0.013067</td>\n",
       "      <td>-0.031252</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ich_iel</th>\n",
       "      <th>t5_37k29</th>\n",
       "      <td>-0.019543</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.002255</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>-0.080638</td>\n",
       "      <td>0.053921</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>0.038154</td>\n",
       "      <td>0.016369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nicoledobrikov1</th>\n",
       "      <th>t5_3oioc0</th>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>-0.030162</td>\n",
       "      <td>-0.023365</td>\n",
       "      <td>0.051887</td>\n",
       "      <td>0.050446</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>-0.049501</td>\n",
       "      <td>-0.059686</td>\n",
       "      <td>-0.068271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germany</th>\n",
       "      <th>t5_2qi4z</th>\n",
       "      <td>0.030575</td>\n",
       "      <td>-0.057457</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>0.029543</td>\n",
       "      <td>-0.003699</td>\n",
       "      <td>0.064915</td>\n",
       "      <td>-0.033345</td>\n",
       "      <td>-0.066493</td>\n",
       "      <td>-0.019160</td>\n",
       "      <td>0.014145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germansgonewild</th>\n",
       "      <th>t5_37g5b</th>\n",
       "      <td>0.022604</td>\n",
       "      <td>-0.032705</td>\n",
       "      <td>-0.016022</td>\n",
       "      <td>0.066290</td>\n",
       "      <td>0.052799</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>0.008364</td>\n",
       "      <td>-0.049809</td>\n",
       "      <td>-0.004913</td>\n",
       "      <td>-0.056319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name  subreddit_id                                                                                                                                            \n",
       "de              t5_22i0          -0.018191     -0.045794      0.035795     -0.036392      0.033076      0.013654     -0.013067     -0.031252      0.001283      0.024700\n",
       "ich_iel         t5_37k29         -0.019543     -0.002590     -0.002255      0.009640     -0.080638      0.053921      0.068653     -0.051635      0.038154      0.016369\n",
       "nicoledobrikov1 t5_3oioc0         0.000240      0.043700     -0.030162     -0.023365      0.051887      0.050446      0.013388     -0.049501     -0.059686     -0.068271\n",
       "germany         t5_2qi4z          0.030575     -0.057457      0.007206      0.029543     -0.003699      0.064915     -0.033345     -0.066493     -0.019160      0.014145\n",
       "germansgonewild t5_37g5b          0.022604     -0.032705     -0.016022      0.066290      0.052799      0.029996      0.008364     -0.049809     -0.004913     -0.056319"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v_subs.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c93fdad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 512)\n",
      "CPU times: user 99.4 ms, sys: 92 ms, total: 191 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_v_posts = mlf.read_run_artifact(\n",
    "    run_id=run_id,\n",
    "    artifact_folder='df_vect_posts',\n",
    "    read_function=pd.read_parquet,\n",
    ")\n",
    "print(df_v_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cd64c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(df_vect, df_v_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c23c14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pcbaumeister</th>\n",
       "      <th>t5_4c1x98</th>\n",
       "      <th>t3_nlj0xz</th>\n",
       "      <td>-0.028007</td>\n",
       "      <td>0.018425</td>\n",
       "      <td>0.025313</td>\n",
       "      <td>-0.084277</td>\n",
       "      <td>-0.044653</td>\n",
       "      <td>0.036714</td>\n",
       "      <td>-0.084001</td>\n",
       "      <td>-0.060739</td>\n",
       "      <td>0.022273</td>\n",
       "      <td>-0.055197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dagibeehot</th>\n",
       "      <th>t5_wv7c1</th>\n",
       "      <th>t3_mzpji1</th>\n",
       "      <td>0.079956</td>\n",
       "      <td>0.062191</td>\n",
       "      <td>0.042096</td>\n",
       "      <td>0.028683</td>\n",
       "      <td>-0.014719</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>-0.016711</td>\n",
       "      <td>0.036436</td>\n",
       "      <td>-0.033169</td>\n",
       "      <td>-0.016168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>germansgonewild</th>\n",
       "      <th>t5_37g5b</th>\n",
       "      <th>t3_nkhuwl</th>\n",
       "      <td>-0.045880</td>\n",
       "      <td>0.025566</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-0.086096</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>-0.003725</td>\n",
       "      <td>0.049456</td>\n",
       "      <td>-0.073738</td>\n",
       "      <td>-0.021704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">de</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">t5_22i0</th>\n",
       "      <th>t3_nkm3hr</th>\n",
       "      <td>-0.054990</td>\n",
       "      <td>0.009562</td>\n",
       "      <td>0.011608</td>\n",
       "      <td>0.017721</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.058977</td>\n",
       "      <td>0.061449</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>-0.010647</td>\n",
       "      <td>0.038405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3_mpc8ai</th>\n",
       "      <td>-0.056767</td>\n",
       "      <td>-0.073990</td>\n",
       "      <td>0.057309</td>\n",
       "      <td>0.051738</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.081643</td>\n",
       "      <td>-0.010165</td>\n",
       "      <td>0.045042</td>\n",
       "      <td>-0.045683</td>\n",
       "      <td>-0.015345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huebi</th>\n",
       "      <th>t5_29zucx</th>\n",
       "      <th>t3_mubs9j</th>\n",
       "      <td>0.154780</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>0.066280</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>-0.080812</td>\n",
       "      <td>0.075854</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.078940</td>\n",
       "      <td>-0.122165</td>\n",
       "      <td>-0.002068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name  subreddit_id post_id                                                                                                                                              \n",
       "pcbaumeister    t5_4c1x98    t3_nlj0xz     -0.028007      0.018425      0.025313     -0.084277     -0.044653      0.036714     -0.084001     -0.060739      0.022273     -0.055197\n",
       "dagibeehot      t5_wv7c1     t3_mzpji1      0.079956      0.062191      0.042096      0.028683     -0.014719      0.002513     -0.016711      0.036436     -0.033169     -0.016168\n",
       "germansgonewild t5_37g5b     t3_nkhuwl     -0.045880      0.025566      0.004287      0.020000     -0.086096      0.016448     -0.003725      0.049456     -0.073738     -0.021704\n",
       "de              t5_22i0      t3_nkm3hr     -0.054990      0.009562      0.011608      0.017721      0.014710      0.058977      0.061449      0.020423     -0.010647      0.038405\n",
       "                             t3_mpc8ai     -0.056767     -0.073990      0.057309      0.051738      0.019686      0.081643     -0.010165      0.045042     -0.045683     -0.015345\n",
       "huebi           t5_29zucx    t3_mubs9j      0.154780      0.007660      0.066280     -0.002162     -0.080812      0.075854      0.000574      0.078940     -0.122165     -0.002068"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v_posts.iloc[14:20, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7928fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 512)\n",
      "CPU times: user 441 ms, sys: 64.8 ms, total: 506 ms\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_v_comments = mlf.read_run_artifact(\n",
    "    run_id=run_id,\n",
    "    artifact_folder='df_vect_comments',\n",
    "    read_function=pd.read_parquet,\n",
    ")\n",
    "print(df_v_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8761dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(df_vect_comments, df_v_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b594260e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <th>t5_22i0</th>\n",
       "      <th>t3_n1db9m</th>\n",
       "      <th>t1_gwgumip</th>\n",
       "      <td>-0.050319</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>-0.035954</td>\n",
       "      <td>0.058837</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>-0.077046</td>\n",
       "      <td>-0.080273</td>\n",
       "      <td>-0.007283</td>\n",
       "      <td>0.020304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ich_iel</th>\n",
       "      <th>t5_37k29</th>\n",
       "      <th>t3_muosjc</th>\n",
       "      <th>t1_gv7rlal</th>\n",
       "      <td>-0.020838</td>\n",
       "      <td>0.016757</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.046645</td>\n",
       "      <td>0.074642</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>-0.041156</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>-0.068941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buenzli</th>\n",
       "      <th>t5_2xbtv</th>\n",
       "      <th>t3_ngs8bj</th>\n",
       "      <th>t1_gysjscj</th>\n",
       "      <td>-0.038592</td>\n",
       "      <td>-0.034569</td>\n",
       "      <td>-0.045555</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>-0.044613</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>0.023125</td>\n",
       "      <td>-0.062052</td>\n",
       "      <td>-0.024423</td>\n",
       "      <td>-0.032473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nicoledobrikovof</th>\n",
       "      <th>t5_3k1wb9</th>\n",
       "      <th>t3_noa9fo</th>\n",
       "      <th>t1_h0dfyag</th>\n",
       "      <td>0.020388</td>\n",
       "      <td>-0.063959</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>-0.057574</td>\n",
       "      <td>0.054215</td>\n",
       "      <td>0.060140</td>\n",
       "      <td>-0.015974</td>\n",
       "      <td>-0.032665</td>\n",
       "      <td>-0.087324</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <th>t5_22i0</th>\n",
       "      <th>t3_ngydq1</th>\n",
       "      <th>t1_gyu585z</th>\n",
       "      <td>-0.052664</td>\n",
       "      <td>0.042260</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.053029</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>0.046601</td>\n",
       "      <td>-0.062652</td>\n",
       "      <td>-0.046233</td>\n",
       "      <td>-0.016664</td>\n",
       "      <td>0.081627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name   subreddit_id post_id   comment_id                                                                                                                                            \n",
       "de               t5_22i0      t3_n1db9m t1_gwgumip     -0.050319      0.019366      0.008127     -0.035954      0.058837      0.018773     -0.077046     -0.080273     -0.007283      0.020304\n",
       "ich_iel          t5_37k29     t3_muosjc t1_gv7rlal     -0.020838      0.016757     -0.027872      0.005312      0.046645      0.074642      0.022860     -0.041156      0.009235     -0.068941\n",
       "buenzli          t5_2xbtv     t3_ngs8bj t1_gysjscj     -0.038592     -0.034569     -0.045555      0.006089     -0.044613      0.008128      0.023125     -0.062052     -0.024423     -0.032473\n",
       "nicoledobrikovof t5_3k1wb9    t3_noa9fo t1_h0dfyag      0.020388     -0.063959      0.013214     -0.057574      0.054215      0.060140     -0.015974     -0.032665     -0.087324      0.022982\n",
       "de               t5_22i0      t3_ngydq1 t1_gyu585z     -0.052664      0.042260      0.013913      0.053029      0.043332      0.046601     -0.062652     -0.046233     -0.016664      0.081627"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v_comments.iloc[:5, :10]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
