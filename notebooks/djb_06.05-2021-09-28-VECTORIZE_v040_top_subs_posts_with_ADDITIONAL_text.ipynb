{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1725e2",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-09-28: Run inference on posts for v0.4.0 POSTS.\n",
    "\n",
    "Diff from before: instead of only using `text` (post title + post body), we'll be using multiple columns to concat and get the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a129a4a",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c4e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95456cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807e479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4b3b4",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12a3680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03700455",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8304e0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e5be9",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33959d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41df070",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da7f421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.0 inputs - Top Subreddits (no Geo) + Geo-relevant subs, comments: TBD',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-09-24',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-09-27',\n",
       " 'folder_comments_text_and_meta': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v040 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.0_19k_top_subs_and_geo_relevant_2021_09_27',\n",
    "#     config_name='top_subreddits_2021_07_16',\n",
    ")\n",
    "config_data_v040.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f874d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.0_use_multi_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.0_use_multi_inference'\n",
    "\n",
    "bucket_name = config_data_v040.config_dict['bucket_name']\n",
    "subreddits_path = config_data_v040.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = config_data_v040.config_dict['folder_posts_text_and_meta']\n",
    "comments_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92022a",
   "metadata": {},
   "source": [
    "# Call function to vectorize text\n",
    "\n",
    "- Batch of: 3000 \n",
    "- Limit characters to: 1000\n",
    "Finally leaves enough room to use around 50% of RAM (of 60GB)\n",
    "\n",
    "The problem is that each iteration takes around 3 minutes, which means whole job for GERMAN only will tka around 4:42 hours:mins..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6de0d",
   "metadata": {},
   "source": [
    "## Test on a `sample` of posts & comments to make sure entire process works first (before running long job)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune.\n",
    "\n",
    "```\n",
    "08:27:18 | INFO | \"Start vectorize function\"\n",
    "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
    "08:27:18 | INFO | \"Loading df_posts...\n",
    "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
    "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
    "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
    "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
    "08:27:27 | INFO | \"Load comments df...\"\n",
    "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
    "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
    "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
    "08:28:11 | INFO | \"Load subreddits df...\"\n",
    "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "...\n",
    "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.67s/it]\n",
    "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
    "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
    "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:00<00:00, 2.42it/s]\n",
    "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
    "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
    "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "3/3 [00:01<00:00, 1.95it/s]\n",
    "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0253df1e",
   "metadata": {},
   "source": [
    "# Test new batching function\n",
    "\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Add new parameter to sample only first N files (we'll process each file individually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28165cd",
   "metadata": {},
   "source": [
    "### Timing is super fast, even with a bigger sample size\n",
    "\n",
    "```\n",
    "11:40:06 | INFO | \"Start vectorize function\"\n",
    "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
    "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
    "11:40:07 | INFO | \"  Saving config to local path...\"\n",
    "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
    "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
    "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
    "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
    "11:40:10 | INFO | \"Load subreddits df...\"\n",
    "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
    "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.65s/it]\n",
    "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
    "...\n",
    "11:40:16 | INFO | \"Loading df_posts...\n",
    "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
    "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
    "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
    "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
    "100%\n",
    "5/5 [00:03<00:00, 1.59it/s]\n",
    "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
    "...\n",
    "11:40:30 | INFO | \"Load comments df...\"\n",
    "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
    "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
    "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
    "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "100%\n",
    "10/10 [00:05<00:00, 1.60it/s]\n",
    "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
    "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
    "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
    "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
    "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0d5cf",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=False`\n",
    "Let's see if the current refactor is good enough or if I really need to manually batch files...\n",
    "\n",
    "**answer**: no it wasn't good enough -- 60GB of RAM wasn't good enough for 19Million comments _lol_.\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af21b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:52:09 | INFO | \"Start vectorize function\"\n",
      "17:52:09 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_175209\"\n",
      "17:52:09 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "17:52:10 | INFO | \"  Saving config to local path...\"\n",
      "17:52:10 | INFO | \"  Logging config to mlflow...\"\n",
      "17:52:10 | INFO | \"Loading model use_multilingual...\"\n",
      "17:52:13 | INFO | \"  0:00:02.347350 <- Load TF HUB model time elapsed\"\n",
      "17:52:13 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "17:52:13 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "17:52:13 | INFO | \"-- Loading & vectorizing COMMENTS in files: 2 --\n",
      "Expected batch size: 2200\"\n",
      "17:52:13 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]17:52:13 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      "17:52:15 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.03it/s]\n",
      "17:52:16 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id']\"\n",
      "17:52:16 | INFO | \"col_text: flair_post_ocr_url_text\"\n",
      "17:52:16 | INFO | \"lowercase_text: False\"\n",
      "17:52:16 | INFO | \"limit_first_n_chars: 900\"\n",
      "17:52:16 | INFO | \"Getting embeddings in batches of size: 2200\"\n",
      " 55%|#######################1                  | 74/134 [01:32<01:13,  1.23s/it]17:53:59 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[575381,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_5582594]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 134/134 [02:54<00:00,  1.30s/it]\n",
      "17:55:13 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      " 50%|#####     | 1/2 [03:10<03:10, 190.06s/it]17:55:23 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "17:55:26 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  4.98it/s]\n",
      "17:55:27 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id']\"\n",
      "17:55:27 | INFO | \"col_text: flair_post_ocr_url_text\"\n",
      "17:55:27 | INFO | \"lowercase_text: False\"\n",
      "17:55:27 | INFO | \"limit_first_n_chars: 900\"\n",
      "17:55:28 | INFO | \"Getting embeddings in batches of size: 2200\"\n",
      "100%|#########################################| 187/187 [03:50<00:00,  1.24s/it]\n",
      "17:59:22 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "100%|##########| 2/2 [07:19<00:00, 219.57s/it]\n",
      "17:59:32 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "17:59:47 | INFO | \"  0:07:37.460369 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_full_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "    \n",
    "    tf_batch_inference_rows=2400,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    n_sample_comment_files=2,\n",
    "    get_embeddings_verbose=True,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd7cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475b919",
   "metadata": {},
   "source": [
    "## Re-do with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b90d3",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:02:40 | INFO | \"Start vectorize function\"\n",
      "18:02:40 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_180240\"\n",
      "18:02:41 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "18:02:41 | INFO | \"  Saving config to local path...\"\n",
      "18:02:41 | INFO | \"  Logging config to mlflow...\"\n",
      "18:02:42 | INFO | \"Loading model use_multilingual...\"\n",
      "18:02:44 | INFO | \"  0:00:02.315347 <- Load TF HUB model time elapsed\"\n",
      "18:02:44 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "18:02:44 | INFO | \"Load subreddits df...\"\n",
      "18:02:45 | INFO | \"  0:00:01.482778 <- df_subs loading time elapsed\"\n",
      "18:02:45 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "18:02:45 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "18:02:46 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]18:02:57 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[575014,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_8409590]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:26<00:00,  3.34s/it]\n",
      "18:03:13 | INFO | \"  0:00:27.349485 <- df_subs vectorizing time elapsed\"\n",
      "18:03:13 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "18:03:13 | INFO | \"Converting pandas to dask...\"\n",
      "18:03:13 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "18:03:13 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "18:03:13 | INFO | \"  Logging to mlflow...\"\n",
      "18:03:15 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "18:03:15 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2450\"\n",
      "18:03:15 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]18:03:15 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      "18:03:18 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.19it/s]\n",
      "18:03:19 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 120/120 [02:36<00:00,  1.31s/it]\n",
      "18:05:57 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:51<1:14:21, 171.58s/it]18:06:07 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "18:06:10 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  5.17it/s]\n",
      "18:06:11 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 168/168 [03:43<00:00,  1.33s/it]\n",
      "18:09:58 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:52<1:28:31, 212.48s/it]18:10:08 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "18:10:12 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.24it/s]\n",
      "18:10:13 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 135/135 [02:55<00:00,  1.30s/it]\n",
      "18:13:11 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [10:05<1:21:29, 203.72s/it]18:13:21 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "18:13:25 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  8.48it/s]\n",
      "18:13:26 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 104/104 [02:23<00:00,  1.38s/it]\n",
      "18:15:50 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [12:44<1:11:13, 185.81s/it]18:16:00 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "18:16:03 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  7.98it/s]\n",
      "18:16:04 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 113/113 [02:29<00:00,  1.32s/it]\n",
      "18:18:35 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [15:28<1:05:19, 178.15s/it]18:18:44 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "18:18:48 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.02it/s]\n",
      "18:18:50 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      "100%|#########################################| 146/146 [03:17<00:00,  1.35s/it]\n",
      "18:22:10 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [19:05<1:06:53, 191.11s/it]18:22:21 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "18:22:25 | INFO | \"Create merged text column\"\n",
      "100%|##########| 4/4 [00:00<00:00,  6.77it/s]\n",
      "18:22:26 | INFO | \"Getting embeddings in batches of size: 2450\"\n",
      " 46%|###################1                      | 61/134 [01:24<01:34,  1.30s/it]"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_concat_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "    \n",
    "    tf_batch_inference_rows=2450,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c87d2c",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_concat_text_lowercase-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "    \n",
    "    tf_batch_inference_rows=2450,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de4b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4aefa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a074be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718262c",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c27f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
