{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e4801f",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "2021-07-28: Run it on the top Subreddits + German subs. Ideally this should help us find counterpart subs in other languages.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e805a29",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c7610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078de24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.3.2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de67c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e21737",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf6849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a0e8b",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0c0bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b48c4c",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802ba3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccb181",
   "metadata": {},
   "source": [
    "# Call function to vectorize text\n",
    "\n",
    "- Batch of: 3000 \n",
    "- Limit characters to: 1000\n",
    "Finally leaves enough room to use around 50% of RAM (of 60GB)\n",
    "\n",
    "The problem is that each iteration takes around 3 minutes, which means whole job for GERMAN only will tka around 4:42 hours:mins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d73603",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.3.2_use_multi_inference_test'\n",
    "mlflow_experiment_full = 'v0.3.2_use_multi_inference'\n",
    "\n",
    "bucket_name = 'i18n-subreddit-clustering'\n",
    "subreddits_path = \"subreddits/top/2021-07-16\"\n",
    "posts_path = 'posts/top/2021-07-16'\n",
    "comments_path = 'comments/top/2021-07-09'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf271b07",
   "metadata": {},
   "source": [
    "```\n",
    "When subreddit_id column was missing:\n",
    "CPU times: user 75.8 ms, sys: 21.1 ms, total: 96.9 ms\n",
    "Wall time: 884 ms\n",
    "(3767, 28)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e499e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns in subreddit meta...\n",
    "# %%time\n",
    "\n",
    "# df_subs = pd.read_parquet(\n",
    "#     path=f\"gs://{bucket_name}/{subreddits_path}\",\n",
    "#     # columns=l_cols_subreddits,\n",
    "# )\n",
    "# df_subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56feb79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40f74b",
   "metadata": {},
   "source": [
    "## Test on a `sample` of posts & comments to make sure entire process works first (before running long job)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune.\n",
    "\n",
    "```\n",
    "08:27:18 | INFO | \"Start vectorize function\"\n",
    "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
    "08:27:18 | INFO | \"Loading df_posts...\n",
    "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
    "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
    "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
    "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
    "08:27:27 | INFO | \"Load comments df...\"\n",
    "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
    "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
    "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
    "08:28:11 | INFO | \"Load subreddits df...\"\n",
    "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "...\n",
    "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.67s/it]\n",
    "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
    "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
    "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "2/2 [00:00<00:00, 2.42it/s]\n",
    "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
    "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
    "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n",
    "100%\n",
    "3/3 [00:01<00:00, 1.95it/s]\n",
    "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
    "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c1d9633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:27:18 | INFO | \"Start vectorize function\"\n",
      "08:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_0827\"\n",
      "08:27:18 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "08:27:26 | INFO | \"  0:00:07.773679 <- df_post time elapsed\"\n",
      "08:27:26 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "08:27:27 | INFO | \"  Sampling posts down to: 2,500\"\n",
      "08:27:27 | INFO | \"  (2500, 6) <- df_posts.shape AFTER sampling\"\n",
      "08:27:27 | INFO | \"Load comments df...\"\n",
      "08:27:57 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "08:28:08 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "08:28:11 | INFO | \"  (31630, 6) <- updated df_comments shape\"\n",
      "08:28:11 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
      "08:28:11 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
      "08:28:11 | INFO | \"Load subreddits df...\"\n",
      "08:28:12 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "08:28:12 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "08:28:13 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n",
      "08:28:15 | INFO | \"  0:00:02.332050 <- Load TF HUB model time elapsed\"\n",
      "08:28:15 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "08:28:15 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "08:28:15 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58afea80a013400398b2d1bb34c31612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:19 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "08:28:19 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:20 | INFO | \"Vectorizing POSTS...\"\n",
      "08:28:20 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb694fdaa5214dfbba2abb899debbec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:21 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "08:28:21 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:22 | INFO | \"Vectorizing COMMENTS...\"\n",
      "08:28:22 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4760437393924423a6bda0d222d5de81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:28:24 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "08:28:24 | INFO | \"  Logging to mlflow...\"\n",
      "08:28:25 | INFO | \"  0:01:06.542544 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='test_n_samples',\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2000,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=2500,\n",
    "    n_sample_comments=5100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60bb6331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3767, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pics</th>\n",
       "      <th>t5_2qh0u</th>\n",
       "      <td>-0.056925</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>-0.009723</td>\n",
       "      <td>-0.009849</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.045963</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>-0.061319</td>\n",
       "      <td>0.053243</td>\n",
       "      <td>-0.052809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <th>t5_2qh33</th>\n",
       "      <td>0.045474</td>\n",
       "      <td>-0.039333</td>\n",
       "      <td>-0.031790</td>\n",
       "      <td>-0.015574</td>\n",
       "      <td>0.074503</td>\n",
       "      <td>0.054003</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.061827</td>\n",
       "      <td>-0.050316</td>\n",
       "      <td>0.023417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memes</th>\n",
       "      <th>t5_2qjpg</th>\n",
       "      <td>-0.014767</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>-0.069566</td>\n",
       "      <td>-0.022420</td>\n",
       "      <td>0.063016</td>\n",
       "      <td>0.066394</td>\n",
       "      <td>-0.061886</td>\n",
       "      <td>0.040540</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.027958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <th>t5_2qh3l</th>\n",
       "      <td>-0.066339</td>\n",
       "      <td>0.056393</td>\n",
       "      <td>0.036245</td>\n",
       "      <td>-0.021127</td>\n",
       "      <td>0.076642</td>\n",
       "      <td>0.040693</td>\n",
       "      <td>0.019423</td>\n",
       "      <td>0.054693</td>\n",
       "      <td>-0.012191</td>\n",
       "      <td>0.065671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interestingasfuck</th>\n",
       "      <th>t5_2qhsa</th>\n",
       "      <td>-0.020677</td>\n",
       "      <td>0.061429</td>\n",
       "      <td>-0.029565</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>0.066374</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>0.069265</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.044498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name    subreddit_id                                                                                                                                            \n",
       "pics              t5_2qh0u         -0.056925      0.027936     -0.009723     -0.009849      0.043200      0.045963      0.049922     -0.061319      0.053243     -0.052809\n",
       "funny             t5_2qh33          0.045474     -0.039333     -0.031790     -0.015574      0.074503      0.054003      0.007870      0.061827     -0.050316      0.023417\n",
       "memes             t5_2qjpg         -0.014767      0.018347     -0.069566     -0.022420      0.063016      0.066394     -0.061886      0.040540      0.019350      0.027958\n",
       "news              t5_2qh3l         -0.066339      0.056393      0.036245     -0.021127      0.076642      0.040693      0.019423      0.054693     -0.012191      0.065671\n",
       "interestingasfuck t5_2qhsa         -0.020677      0.061429     -0.029565      0.029978      0.066374      0.061271      0.069265      0.028228      0.004899      0.044498"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect_subs.shape)\n",
    "df_vect_subs.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a5c0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_0</th>\n",
       "      <th>embeddings_1</th>\n",
       "      <th>embeddings_2</th>\n",
       "      <th>embeddings_3</th>\n",
       "      <th>embeddings_4</th>\n",
       "      <th>embeddings_5</th>\n",
       "      <th>embeddings_6</th>\n",
       "      <th>embeddings_7</th>\n",
       "      <th>embeddings_8</th>\n",
       "      <th>embeddings_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stocks</th>\n",
       "      <th>t5_2qjfk</th>\n",
       "      <th>t3_o9h0x8</th>\n",
       "      <td>-0.054105</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>-0.077626</td>\n",
       "      <td>-0.032931</td>\n",
       "      <td>-0.022623</td>\n",
       "      <td>-0.004873</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>-0.031575</td>\n",
       "      <td>-0.058212</td>\n",
       "      <td>0.042760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laptops</th>\n",
       "      <th>t5_2qoip</th>\n",
       "      <th>t3_o76rnd</th>\n",
       "      <td>-0.018935</td>\n",
       "      <td>-0.026588</td>\n",
       "      <td>0.071572</td>\n",
       "      <td>-0.015938</td>\n",
       "      <td>-0.083586</td>\n",
       "      <td>-0.082511</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>-0.015417</td>\n",
       "      <td>-0.043235</td>\n",
       "      <td>0.068179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luftraum</th>\n",
       "      <th>t5_q02q4</th>\n",
       "      <th>t3_nuwjin</th>\n",
       "      <td>-0.072257</td>\n",
       "      <td>0.020968</td>\n",
       "      <td>-0.012867</td>\n",
       "      <td>-0.042401</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.081959</td>\n",
       "      <td>-0.042031</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>-0.005116</td>\n",
       "      <td>0.025049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adultery</th>\n",
       "      <th>t5_2sjkv</th>\n",
       "      <th>t3_oi8tno</th>\n",
       "      <td>0.046559</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>0.039657</td>\n",
       "      <td>-0.066817</td>\n",
       "      <td>-0.096201</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>-0.074167</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>-0.066348</td>\n",
       "      <td>-0.029130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poopshitters</th>\n",
       "      <th>t5_wgmeb</th>\n",
       "      <th>t3_o2es6a</th>\n",
       "      <td>-0.056548</td>\n",
       "      <td>-0.018536</td>\n",
       "      <td>0.015383</td>\n",
       "      <td>-0.018324</td>\n",
       "      <td>-0.007343</td>\n",
       "      <td>0.037289</td>\n",
       "      <td>0.082599</td>\n",
       "      <td>0.029512</td>\n",
       "      <td>-0.039021</td>\n",
       "      <td>0.031015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       embeddings_0  embeddings_1  embeddings_2  embeddings_3  embeddings_4  embeddings_5  embeddings_6  embeddings_7  embeddings_8  embeddings_9\n",
       "subreddit_name subreddit_id post_id                                                                                                                                              \n",
       "stocks         t5_2qjfk     t3_o9h0x8     -0.054105      0.070300     -0.077626     -0.032931     -0.022623     -0.004873      0.018130     -0.031575     -0.058212      0.042760\n",
       "laptops        t5_2qoip     t3_o76rnd     -0.018935     -0.026588      0.071572     -0.015938     -0.083586     -0.082511      0.044506     -0.015417     -0.043235      0.068179\n",
       "luftraum       t5_q02q4     t3_nuwjin     -0.072257      0.020968     -0.012867     -0.042401      0.024641      0.081959     -0.042031      0.010640     -0.005116      0.025049\n",
       "adultery       t5_2sjkv     t3_oi8tno      0.046559     -0.042573      0.039657     -0.066817     -0.096201      0.001347     -0.074167      0.002814     -0.066348     -0.029130\n",
       "poopshitters   t5_wgmeb     t3_o2es6a     -0.056548     -0.018536      0.015383     -0.018324     -0.007343      0.037289      0.082599      0.029512     -0.039021      0.031015"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect.shape)\n",
    "df_vect.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19ab8338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embeddings_502</th>\n",
       "      <th>embeddings_503</th>\n",
       "      <th>embeddings_504</th>\n",
       "      <th>embeddings_505</th>\n",
       "      <th>embeddings_506</th>\n",
       "      <th>embeddings_507</th>\n",
       "      <th>embeddings_508</th>\n",
       "      <th>embeddings_509</th>\n",
       "      <th>embeddings_510</th>\n",
       "      <th>embeddings_511</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yoga</th>\n",
       "      <th>t5_2qhq6</th>\n",
       "      <th>t3_o9upy3</th>\n",
       "      <th>t1_h3djels</th>\n",
       "      <td>-0.049714</td>\n",
       "      <td>0.039176</td>\n",
       "      <td>0.031363</td>\n",
       "      <td>-0.025456</td>\n",
       "      <td>0.020385</td>\n",
       "      <td>-0.009454</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.068287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldnews</th>\n",
       "      <th>t5_2qh13</th>\n",
       "      <th>t3_o7yf4y</th>\n",
       "      <th>t1_h333pw7</th>\n",
       "      <td>-0.000592</td>\n",
       "      <td>-0.064295</td>\n",
       "      <td>-0.041862</td>\n",
       "      <td>-0.026075</td>\n",
       "      <td>0.028604</td>\n",
       "      <td>-0.028089</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>-0.045100</td>\n",
       "      <td>0.064217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aww</th>\n",
       "      <th>t5_2qh1o</th>\n",
       "      <th>t3_nvidt5</th>\n",
       "      <th>t1_h13y1e4</th>\n",
       "      <td>0.022847</td>\n",
       "      <td>-0.084315</td>\n",
       "      <td>0.062082</td>\n",
       "      <td>-0.042768</td>\n",
       "      <td>0.049408</td>\n",
       "      <td>0.041870</td>\n",
       "      <td>0.021409</td>\n",
       "      <td>0.035140</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>0.112163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formula1</th>\n",
       "      <th>t5_2qimj</th>\n",
       "      <th>t3_o8beb8</th>\n",
       "      <th>t1_h33xaio</th>\n",
       "      <td>-0.042507</td>\n",
       "      <td>-0.012686</td>\n",
       "      <td>0.035909</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.034547</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>-0.020290</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.074332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaaaacccccccce</th>\n",
       "      <th>t5_3aa11</th>\n",
       "      <th>t3_o9igjh</th>\n",
       "      <th>t1_h3igwhh</th>\n",
       "      <td>0.045924</td>\n",
       "      <td>-0.043366</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>-0.062547</td>\n",
       "      <td>-0.043556</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>0.029582</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.036313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    embeddings_502  embeddings_503  embeddings_504  embeddings_505  embeddings_506  embeddings_507  embeddings_508  embeddings_509  embeddings_510  embeddings_511\n",
       "subreddit_name   subreddit_id post_id   comment_id                                                                                                                                                                \n",
       "yoga             t5_2qhq6     t3_o9upy3 t1_h3djels       -0.049714        0.039176        0.031363       -0.025456        0.020385       -0.009454        0.032895        0.048567        0.006216        0.068287\n",
       "worldnews        t5_2qh13     t3_o7yf4y t1_h333pw7       -0.000592       -0.064295       -0.041862       -0.026075        0.028604       -0.028089       -0.017898        0.031500       -0.045100        0.064217\n",
       "aww              t5_2qh1o     t3_nvidt5 t1_h13y1e4        0.022847       -0.084315        0.062082       -0.042768        0.049408        0.041870        0.021409        0.035140        0.034683        0.112163\n",
       "formula1         t5_2qimj     t3_o8beb8 t1_h33xaio       -0.042507       -0.012686        0.035909        0.104300        0.034547        0.009831       -0.020290        0.045840        0.037209        0.074332\n",
       "aaaaaaacccccccce t5_3aa11     t3_o9igjh t1_h3igwhh        0.045924       -0.043366        0.009658        0.004710       -0.062547       -0.043556        0.074638        0.029582        0.018934        0.036313"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_vect_comments.shape)\n",
    "df_vect_comments.iloc[10:15, -10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8acbe1",
   "metadata": {},
   "source": [
    "# Test new batching function\n",
    "\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Add new parameter to sample only first N files (we'll process each file individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3fcac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de25e4e7e7c41b19b43a33a8f94178c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts/top/2021-07-16/000000000000.parquet\n",
      "000000000000\n",
      "posts/top/2021-07-16/000000000001.parquet\n",
      "000000000001\n",
      "posts/top/2021-07-16/000000000002.parquet\n",
      "000000000002\n",
      "posts/top/2021-07-16/000000000003.parquet\n",
      "000000000003\n",
      "posts/top/2021-07-16/000000000004.parquet\n",
      "000000000004\n"
     ]
    }
   ],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "# folder = configs.gcp_storage_folder\n",
    "\n",
    "# print( str(configs.gcp_bucket) +\"/\"+ str(folder))\n",
    "for blob in tqdm(list(bucket.list_blobs(prefix=posts_path))[:5]):\n",
    "#     print(blob)\n",
    "    print(blob.name)\n",
    "    print(blob.name.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d54709c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:16 | INFO | \"Start vectorize function\"\n",
      "11:22:16 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1122\"\n",
      "11:22:17 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "11:22:17 | INFO | \"  Saving config to local path...\"\n",
      "11:22:17 | INFO | \"  Logging config to mlflow...\"\n",
      "11:22:18 | INFO | \"Loading model use_multilingual...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0e75366320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:20 | INFO | \"  0:00:02.280150 <- Load TF HUB model time elapsed\"\n",
      "11:22:20 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "11:22:20 | INFO | \"Load subreddits df...\"\n",
      "11:22:21 | INFO | \"  0:00:00.619620 <- df_subs loading time elapsed\"\n",
      "11:22:21 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "11:22:21 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "11:22:21 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f7efd83a1c4e42a74681d65dd2431b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0f11df85f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0c504f3b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:25 | INFO | \"  0:00:04.172970 <- df_subs vectorizing time elapsed\"\n",
      "11:22:25 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "11:22:25 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "11:22:25 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:22:25 | INFO | \"  Logging to mlflow...\"\n",
      "11:22:30 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "11:22:37 | INFO | \"  0:00:06.666186 <- df_post loading time elapsed\"\n",
      "11:22:37 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "11:22:38 | INFO | \"  Sampling posts down to: 3,500\"\n",
      "11:22:38 | INFO | \"  (3500, 6) <- df_posts.shape AFTER sampling\"\n",
      "11:22:38 | INFO | \"Vectorizing POSTS...\"\n",
      "11:22:38 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0010190095124e2abaeb9a819dd7af4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:22:40 | INFO | \"  0:00:01.568036 <- df_posts vectorizing time elapsed\"\n",
      "11:22:40 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "11:22:40 | INFO | \"     7.5 MB <- Memory usage\"\n",
      "11:22:40 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:22:40 | INFO | \"  Logging to mlflow...\"\n",
      "11:22:41 | INFO | \"Load comments df...\"\n",
      "11:23:11 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "11:23:23 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "11:23:26 | INFO | \"  (34463, 6) <- updated df_comments shape\"\n",
      "11:23:26 | INFO | \"  Sampling COMMENTS down to: 5,100\"\n",
      "11:23:26 | INFO | \"  (5100, 6) <- df_comments.shape AFTER sampling\"\n",
      "11:23:26 | INFO | \"Vectorizing COMMENTS...\"\n",
      "11:23:26 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe04bb0f51f4cbbbd0f50c62ff3ae98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:23:28 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "11:23:28 | INFO | \"    11.2 MB <- Memory usage\"\n",
      "11:23:28 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:23:28 | INFO | \"  Logging to mlflow...\"\n",
      "11:23:30 | INFO | \"  0:01:13.306714 <- Total vectorize fxn time elapsed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f0e752c0680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=3500,\n",
    "    n_sample_comments=5100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b5584",
   "metadata": {},
   "source": [
    "### Timing is super fast, even with a bigger sample size\n",
    "\n",
    "```\n",
    "11:40:06 | INFO | \"Start vectorize function\"\n",
    "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
    "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
    "11:40:07 | INFO | \"  Saving config to local path...\"\n",
    "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
    "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
    "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
    "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
    "11:40:10 | INFO | \"Load subreddits df...\"\n",
    "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
    "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
    "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
    "100%\n",
    "2/2 [00:03<00:00, 1.65s/it]\n",
    "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
    "...\n",
    "11:40:16 | INFO | \"Loading df_posts...\n",
    "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
    "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
    "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
    "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
    "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
    "100%\n",
    "5/5 [00:03<00:00, 1.59it/s]\n",
    "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
    "...\n",
    "11:40:30 | INFO | \"Load comments df...\"\n",
    "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
    "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
    "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
    "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
    "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
    "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "100%\n",
    "10/10 [00:05<00:00, 1.60it/s]\n",
    "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
    "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
    "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
    "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
    "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
    "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87ba14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:06 | INFO | \"Start vectorize function\"\n",
      "11:40:06 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1140\"\n",
      "11:40:07 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "11:40:07 | INFO | \"  Saving config to local path...\"\n",
      "11:40:07 | INFO | \"  Logging config to mlflow...\"\n",
      "11:40:08 | INFO | \"Loading model use_multilingual...\"\n",
      "11:40:10 | INFO | \"  0:00:02.417308 <- Load TF HUB model time elapsed\"\n",
      "11:40:10 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "11:40:10 | INFO | \"Load subreddits df...\"\n",
      "11:40:11 | INFO | \"  0:00:00.519934 <- df_subs loading time elapsed\"\n",
      "11:40:11 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "11:40:11 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "11:40:11 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a853b78b494da89d4c83135adfce67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:15 | INFO | \"  0:00:04.080246 <- df_subs vectorizing time elapsed\"\n",
      "11:40:15 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "11:40:15 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "11:40:15 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:40:15 | INFO | \"  Logging to mlflow...\"\n",
      "11:40:16 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "11:40:23 | INFO | \"  0:00:06.460565 <- df_post loading time elapsed\"\n",
      "11:40:23 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "11:40:24 | INFO | \"  Sampling posts down to: 9,500\"\n",
      "11:40:24 | INFO | \"  (9500, 6) <- df_posts.shape AFTER sampling\"\n",
      "11:40:24 | INFO | \"Vectorizing POSTS...\"\n",
      "11:40:24 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6938f9b6bbda43f499c61fff514fcf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:40:28 | INFO | \"  0:00:03.774021 <- df_posts vectorizing time elapsed\"\n",
      "11:40:28 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "11:40:28 | INFO | \"    20.3 MB <- Memory usage\"\n",
      "11:40:28 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:40:28 | INFO | \"  Logging to mlflow...\"\n",
      "11:40:30 | INFO | \"Load comments df...\"\n",
      "11:40:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "11:41:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "11:41:14 | INFO | \"  (95313, 6) <- updated df_comments shape\"\n",
      "11:41:14 | INFO | \"  Sampling COMMENTS down to: 19,100\"\n",
      "11:41:14 | INFO | \"  (19100, 6) <- df_comments.shape AFTER sampling\"\n",
      "11:41:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "11:41:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf694d1cb2c4541b466f2d1cd7b025b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:41:20 | INFO | \"  0:00:06.239953 <- df_posts vectorizing time elapsed\"\n",
      "11:41:20 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "11:41:20 | INFO | \"    42.1 MB <- Memory usage\"\n",
      "11:41:20 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:41:21 | INFO | \"  Logging to mlflow...\"\n",
      "11:41:23 | INFO | \"  0:01:16.130234 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    n_sample_posts=9500,\n",
    "    n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5adfd7",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=False`\n",
    "Let's see if the current refactor is good enough or if I really need to manually batch files...\n",
    "\n",
    "**answer**: no it wasn't good enough -- 60GB of RAM wasn't good enough for 19Million comments _lol_.\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335d18ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:53 | INFO | \"Start vectorize function\"\n",
      "11:49:53 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1149\"\n",
      "11:49:53 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "11:49:54 | INFO | \"  Saving config to local path...\"\n",
      "11:49:54 | INFO | \"  Logging config to mlflow...\"\n",
      "11:49:54 | INFO | \"Loading model use_multilingual...\"\n",
      "11:49:57 | INFO | \"  0:00:02.616759 <- Load TF HUB model time elapsed\"\n",
      "11:49:57 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "11:49:57 | INFO | \"Load subreddits df...\"\n",
      "11:49:58 | INFO | \"  0:00:01.091284 <- df_subs loading time elapsed\"\n",
      "11:49:58 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "11:49:58 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "11:49:58 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8322b373616e456d953d36a4bf818fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:50:02 | INFO | \"  0:00:04.246728 <- df_subs vectorizing time elapsed\"\n",
      "11:50:02 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "11:50:02 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "11:50:02 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "11:50:03 | INFO | \"  Logging to mlflow...\"\n",
      "11:50:04 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "11:50:11 | INFO | \"  0:00:07.403893 <- df_post loading time elapsed\"\n",
      "11:50:11 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "11:50:12 | INFO | \"Vectorizing POSTS...\"\n",
      "11:50:12 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94f8bc54fd2401e9b8dfca27e236ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:00:12 | INFO | \"  0:10:00.597031 <- df_posts vectorizing time elapsed\"\n",
      "12:00:14 | INFO | \"  Saving to local... df_vect_posts...\"\n",
      "12:00:14 | INFO | \"  3,532.8 MB <- Memory usage\"\n",
      "12:00:14 | INFO | \"      48\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "12:00:33 | INFO | \"  Logging to mlflow...\"\n",
      "12:01:23 | INFO | \"Load comments df...\"\n",
      "12:01:58 | INFO | \"  (19200854, 6) <- df_comments shape\"\n",
      "12:02:10 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
      "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a34052a0d04e2c93683695ceea5071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9939c221b377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtf_batch_inference_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtf_limit_first_n_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     n_sample_posts=9500,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/vectorize_text_tf.py\u001b[0m in \u001b[0;36mvectorize_text_to_embeddings\u001b[0;34m(mlflow_experiment, model_name, run_name, tokenize_lowercase, bucket_name, subreddits_path, posts_path, comments_path, preprocess_text_folder, col_text_post, col_text_post_word_count, col_text_post_url, col_post_id, col_comment_id, col_text_comment, col_text_comment_word_count, col_subreddit_id, col_text_subreddit_description, col_text_subreddit_word_count, tf_batch_inference_rows, tf_limit_first_n_chars, n_sample_post_files, n_sample_comment_files, n_sample_posts, n_sample_comments)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mlowercase_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_lowercase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_batch_inference_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mlimit_first_n_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_limit_first_n_chars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         )\n\u001b[1;32m    282\u001b[0m         \u001b[0mtotal_time_comms_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_start_comms_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'df_posts vectorizing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/vectorize_text_tf.py\u001b[0m in \u001b[0;36mget_embeddings_as_df\u001b[0;34m(model, df, col_text, cols_index, col_embeddings_prefix, lowercase_text, batch_size, limit_first_n_chars)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 )\n\u001b[1;32m    393\u001b[0m             )\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcol_embeddings_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mdf_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_df_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    296\u001b[0m     )\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 521\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36mconcat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2100,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47056e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae7d49",
   "metadata": {},
   "source": [
    "## Test - Re-do comments with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "259eef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:09 | INFO | \"Start vectorize function\"\n",
      "18:46:09 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1846\"\n",
      "18:46:10 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "18:46:10 | INFO | \"  Saving config to local path...\"\n",
      "18:46:10 | INFO | \"  Logging config to mlflow...\"\n",
      "18:46:10 | INFO | \"Loading model use_multilingual...\"\n",
      "18:46:13 | INFO | \"  0:00:02.357815 <- Load TF HUB model time elapsed\"\n",
      "18:46:13 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "18:46:13 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "18:46:13 | INFO | \"-- Loading & vectorizing COMMENTS in files: 5 --\n",
      "Expected batch size: 6100\"\n",
      "18:46:13 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84492a2519774182a86f2a4cd938e066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:14 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:14 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3221c94358c649c5a60bcb8fe7f785c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:19 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 9,821 Rows by 516 Cols\"\n",
      "18:46:20 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:20 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba1046f70d04300b9605bfb33a58e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:24 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 9,821 Rows by 516 Cols\"\n",
      "18:46:26 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:26 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a50db459df41b8a69005cbd9a49461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:30 | INFO | \"  Saving to local: df_vect_comments/000000000002 | 9,821 Rows by 516 Cols\"\n",
      "18:46:31 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:31 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c98bf643324447b7d5153db9045a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:35 | INFO | \"  Saving to local: df_vect_comments/000000000003 | 9,821 Rows by 516 Cols\"\n",
      "18:46:37 | INFO | \"  Sampling COMMENTS down to: 49,100     Samples PER FILE: 9,821\"\n",
      "18:46:37 | INFO | \"  (9821, 6) <- df_comments.shape AFTER sampling\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb032f6527e4241991213c5d941c624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:46:41 | INFO | \"  Saving to local: df_vect_comments/000000000004 | 9,821 Rows by 516 Cols\"\n",
      "18:46:44 | INFO | \"  0:00:34.354815 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"test_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "#     subreddits_path=subreddits_path,\n",
    "#     posts_path=posts_path,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=6100,\n",
    "    tf_limit_first_n_chars=750,\n",
    "    \n",
    "    n_sample_comment_files=5,\n",
    "    n_sample_comments=49100,\n",
    "#     n_sample_posts=9500,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55e4e4",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01fdf539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:59:48 | INFO | \"Start vectorize function\"\n",
      "18:59:48 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_1859\"\n",
      "18:59:48 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "18:59:49 | INFO | \"  Saving config to local path...\"\n",
      "18:59:49 | INFO | \"  Logging config to mlflow...\"\n",
      "18:59:49 | INFO | \"Loading model use_multilingual...\"\n",
      "18:59:51 | INFO | \"  0:00:02.346687 <- Load TF HUB model time elapsed\"\n",
      "18:59:51 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "18:59:51 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "18:59:51 | INFO | \"-- Loading & vectorizing COMMENTS in files: 37 --\n",
      "Expected batch size: 6100\"\n",
      "18:59:51 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752cc60339d94a19825ef65a3537117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:59:51 | INFO | \"Processing: comments/top/2021-07-09/000000000000.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27748cdd4d44c4c8820ecb48b39c9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:03:19 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 546,915 Rows by 516 Cols\"\n",
      "19:03:29 | INFO | \"Processing: comments/top/2021-07-09/000000000001.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5cbe2edede4ed0b2064812b7b153b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:09 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 582,195 Rows by 516 Cols\"\n",
      "19:07:21 | INFO | \"Processing: comments/top/2021-07-09/000000000002.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d0f37b78aa4b4e9930fcbbb52ebc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:10:25 | INFO | \"  Saving to local: df_vect_comments/000000000002 | 478,463 Rows by 516 Cols\"\n",
      "19:10:36 | INFO | \"Processing: comments/top/2021-07-09/000000000003.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45edb34c46e44cb2837359623db1cf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:13:28 | INFO | \"  Saving to local: df_vect_comments/000000000003 | 467,274 Rows by 516 Cols\"\n",
      "19:13:39 | INFO | \"Processing: comments/top/2021-07-09/000000000004.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca214f227502467e8957c469d10ea908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:17:15 | INFO | \"  Saving to local: df_vect_comments/000000000004 | 584,222 Rows by 516 Cols\"\n",
      "19:17:26 | INFO | \"Processing: comments/top/2021-07-09/000000000005.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c9049da8ff4ba8aba91e9ffcc40d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:20:52 | INFO | \"  Saving to local: df_vect_comments/000000000005 | 551,542 Rows by 516 Cols\"\n",
      "19:21:03 | INFO | \"Processing: comments/top/2021-07-09/000000000006.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e255d7e48d474187836fdb9d800aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:24:43 | INFO | \"  Saving to local: df_vect_comments/000000000006 | 588,569 Rows by 516 Cols\"\n",
      "19:24:54 | INFO | \"Processing: comments/top/2021-07-09/000000000007.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cd631eccbc4f859c0dfb9790c0842e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:28:20 | INFO | \"  Saving to local: df_vect_comments/000000000007 | 549,341 Rows by 516 Cols\"\n",
      "19:28:32 | INFO | \"Processing: comments/top/2021-07-09/000000000008.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f476059bfbb140a5bbdc2f45042c4c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:31:19 | INFO | \"  Saving to local: df_vect_comments/000000000008 | 447,740 Rows by 516 Cols\"\n",
      "19:31:31 | INFO | \"Processing: comments/top/2021-07-09/000000000009.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daf73f13aa2432391a2292f471c4125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:34:57 | INFO | \"  Saving to local: df_vect_comments/000000000009 | 554,171 Rows by 516 Cols\"\n",
      "19:35:09 | INFO | \"Processing: comments/top/2021-07-09/000000000010.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f035ff2bc684aebbd6c214d4ce2eb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:38:08 | INFO | \"  Saving to local: df_vect_comments/000000000010 | 478,748 Rows by 516 Cols\"\n",
      "19:38:19 | INFO | \"Processing: comments/top/2021-07-09/000000000011.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d094c52a66409db867690eac9b509b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:41:27 | INFO | \"  Saving to local: df_vect_comments/000000000011 | 506,263 Rows by 516 Cols\"\n",
      "19:41:38 | INFO | \"Processing: comments/top/2021-07-09/000000000012.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777185ac74c64b558343a49e3327be3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:44:57 | INFO | \"  Saving to local: df_vect_comments/000000000012 | 538,231 Rows by 516 Cols\"\n",
      "19:45:09 | INFO | \"Processing: comments/top/2021-07-09/000000000013.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69855f7a246445d2b013bd344f096cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:48:23 | INFO | \"  Saving to local: df_vect_comments/000000000013 | 516,219 Rows by 516 Cols\"\n",
      "19:48:35 | INFO | \"Processing: comments/top/2021-07-09/000000000014.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f560264d35384e02879a3f9897947561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:51:58 | INFO | \"  Saving to local: df_vect_comments/000000000014 | 543,914 Rows by 516 Cols\"\n",
      "19:52:10 | INFO | \"Processing: comments/top/2021-07-09/000000000015.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841174e834d245d9b28b0707d911197e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:55:46 | INFO | \"  Saving to local: df_vect_comments/000000000015 | 583,580 Rows by 516 Cols\"\n",
      "19:55:59 | INFO | \"Processing: comments/top/2021-07-09/000000000016.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5d8bc513124c288a5c7557508fd6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:59:15 | INFO | \"  Saving to local: df_vect_comments/000000000016 | 533,066 Rows by 516 Cols\"\n",
      "19:59:27 | INFO | \"Processing: comments/top/2021-07-09/000000000017.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f65f86b7493464683333341eecf4896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:02:47 | INFO | \"  Saving to local: df_vect_comments/000000000017 | 536,540 Rows by 516 Cols\"\n",
      "20:02:58 | INFO | \"Processing: comments/top/2021-07-09/000000000018.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f43b10d96f34a9e9c7d720ead3e32e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:05:50 | INFO | \"  Saving to local: df_vect_comments/000000000018 | 455,375 Rows by 516 Cols\"\n",
      "20:06:02 | INFO | \"Processing: comments/top/2021-07-09/000000000019.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b85859175f94f05b22573509f8517cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:09:34 | INFO | \"  Saving to local: df_vect_comments/000000000019 | 570,490 Rows by 516 Cols\"\n",
      "20:09:45 | INFO | \"Processing: comments/top/2021-07-09/000000000020.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d87cdc78978491989919dfe64bf2484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:13:28 | INFO | \"  Saving to local: df_vect_comments/000000000020 | 600,022 Rows by 516 Cols\"\n",
      "20:13:40 | INFO | \"Processing: comments/top/2021-07-09/000000000021.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7b4b0acab4f349b5b8c88d7145f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:16:51 | INFO | \"  Saving to local: df_vect_comments/000000000021 | 504,174 Rows by 516 Cols\"\n",
      "20:17:03 | INFO | \"Processing: comments/top/2021-07-09/000000000022.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e3e29df38b4a0b84f48193f1f6b32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:20:02 | INFO | \"  Saving to local: df_vect_comments/000000000022 | 485,072 Rows by 516 Cols\"\n",
      "20:20:14 | INFO | \"Processing: comments/top/2021-07-09/000000000023.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21018fd2a75243c88353ffd2031bf9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:23:14 | INFO | \"  Saving to local: df_vect_comments/000000000023 | 482,067 Rows by 516 Cols\"\n",
      "20:23:25 | INFO | \"Processing: comments/top/2021-07-09/000000000024.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd0bf6082f142cfbd830002668018dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:26:21 | INFO | \"  Saving to local: df_vect_comments/000000000024 | 473,130 Rows by 516 Cols\"\n",
      "20:26:32 | INFO | \"Processing: comments/top/2021-07-09/000000000025.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45678526bbad40678ee78b944b0bc47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:30:01 | INFO | \"  Saving to local: df_vect_comments/000000000025 | 560,305 Rows by 516 Cols\"\n",
      "20:30:13 | INFO | \"Processing: comments/top/2021-07-09/000000000026.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195167441eef4663b78261b332c0a7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:33:21 | INFO | \"  Saving to local: df_vect_comments/000000000026 | 507,379 Rows by 516 Cols\"\n",
      "20:33:32 | INFO | \"Processing: comments/top/2021-07-09/000000000027.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2d00769f284ab885331708ed0153d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:36:48 | INFO | \"  Saving to local: df_vect_comments/000000000027 | 527,642 Rows by 516 Cols\"\n",
      "20:36:59 | INFO | \"Processing: comments/top/2021-07-09/000000000028.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcf8fdf77104f91952c5354c0642858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:07 | INFO | \"  Saving to local: df_vect_comments/000000000028 | 505,776 Rows by 516 Cols\"\n",
      "20:40:19 | INFO | \"Processing: comments/top/2021-07-09/000000000029.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc66f940d11417ab05c74e8b5682983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:43:22 | INFO | \"  Saving to local: df_vect_comments/000000000029 | 490,102 Rows by 516 Cols\"\n",
      "20:43:34 | INFO | \"Processing: comments/top/2021-07-09/000000000030.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ade99dfc8114542a0740180c43799fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:47:08 | INFO | \"  Saving to local: df_vect_comments/000000000030 | 572,696 Rows by 516 Cols\"\n",
      "20:47:20 | INFO | \"Processing: comments/top/2021-07-09/000000000031.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b745157947b14365bcf5dd3f39074bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:49:42 | INFO | \"  Saving to local: df_vect_comments/000000000031 | 375,509 Rows by 516 Cols\"\n",
      "20:49:53 | INFO | \"Processing: comments/top/2021-07-09/000000000032.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3cd4bb7d764cb692e4cf10a4b0d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:53:17 | INFO | \"  Saving to local: df_vect_comments/000000000032 | 539,335 Rows by 516 Cols\"\n",
      "20:53:29 | INFO | \"Processing: comments/top/2021-07-09/000000000033.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74abf71b64e94f219ccedd3db1134867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:56:23 | INFO | \"  Saving to local: df_vect_comments/000000000033 | 468,589 Rows by 516 Cols\"\n",
      "20:56:34 | INFO | \"Processing: comments/top/2021-07-09/000000000034.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2ed9fecac64df69421baffa6c635ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:59:46 | INFO | \"  Saving to local: df_vect_comments/000000000034 | 507,419 Rows by 516 Cols\"\n",
      "20:59:59 | INFO | \"Processing: comments/top/2021-07-09/000000000035.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b597c2a9ecc44f9c83661d10a79662b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:03:09 | INFO | \"  Saving to local: df_vect_comments/000000000035 | 504,183 Rows by 516 Cols\"\n",
      "21:03:21 | INFO | \"Processing: comments/top/2021-07-09/000000000036.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384e10151cfe4a88a473bc846a6f7dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:06:22 | INFO | \"  Saving to local: df_vect_comments/000000000036 | 484,596 Rows by 516 Cols\"\n",
      "21:12:57 | INFO | \"  2:13:09.495852 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"new_batch_fxn_{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "#     subreddits_path=subreddits_path,\n",
    "#     posts_path=posts_path,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=6100,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    n_sample_comments=None,\n",
    "#     n_sample_posts=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ea4241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54924"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1207f",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fe16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:02:33 | INFO | \"Start vectorize function\"\n",
      "23:02:33 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-29_230233\"\n",
      "23:02:33 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "23:02:34 | INFO | \"  Saving config to local path...\"\n",
      "23:02:34 | INFO | \"  Logging config to mlflow...\"\n",
      "23:02:34 | INFO | \"Loading model use_multilingual...\"\n",
      "23:02:37 | INFO | \"  0:00:03.026929 <- Load TF HUB model time elapsed\"\n",
      "23:02:37 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "23:02:37 | INFO | \"Load subreddits df...\"\n",
      "23:02:38 | INFO | \"  0:00:01.198338 <- df_subs loading time elapsed\"\n",
      "23:02:38 | INFO | \"  (3767, 4) <- df_subs shape\"\n",
      "23:02:38 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "23:02:39 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff282f052814565bae13389c88877f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:02:43 | INFO | \"  0:00:04.972073 <- df_subs vectorizing time elapsed\"\n",
      "23:02:43 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 3,767 Rows by 514 Cols\"\n",
      "23:02:44 | INFO | \"     7.8 MB <- Memory usage\"\n",
      "23:02:44 | INFO | \"       1\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "23:02:44 | INFO | \"  Logging to mlflow...\"\n",
      "23:02:45 | INFO | \"Loading df_posts...\n",
      "  gs://i18n-subreddit-clustering/posts/top/2021-07-16\"\n",
      "23:02:58 | INFO | \"  0:00:12.642326 <- df_post loading time elapsed\"\n",
      "23:02:58 | INFO | \"  (1649929, 6) <- df_posts.shape\"\n",
      "23:02:58 | INFO | \"Vectorizing POSTS...\"\n",
      "23:02:59 | INFO | \"Getting embeddings in batches of size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0cdca8992b4d9b88869cba9dff2b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:17:43 | INFO | \"  0:14:44.684010 <- df_posts vectorizing time elapsed\"\n",
      "23:17:45 | INFO | \"  Saving to local: df_vect_posts/df | 1,649,929 Rows by 515 Cols\"\n",
      "23:17:45 | INFO | \"  3,532.8 MB <- Memory usage\"\n",
      "23:17:45 | INFO | \"      48\t<- target Dask partitions\t   75.0 <- target MB partition size\"\n",
      "23:18:06 | INFO | \"  Logging to mlflow...\"\n",
      "23:18:52 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "23:18:52 | INFO | \"-- Loading & vectorizing COMMENTS in files: 37 --\n",
      "Expected batch size: 2000\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d50fae78924bbca288542c216ad164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:18:52 | INFO | \"Processing: comments/top/2021-07-09/000000000000.parquet\"\n",
      "23:18:55 | INFO | \"  (545879, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47acd2c6778c42998c88c795e21ab3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:23:07 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 545,879 Rows by 516 Cols\"\n",
      "23:23:18 | INFO | \"Processing: comments/top/2021-07-09/000000000001.parquet\"\n",
      "23:23:22 | INFO | \"  (581335, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d82bf68bba34c68a67e345ae090f965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:27:51 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 581,335 Rows by 516 Cols\"\n",
      "23:28:03 | INFO | \"Processing: comments/top/2021-07-09/000000000002.parquet\"\n",
      "23:28:07 | INFO | \"  (477712, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2412c4065d4e89b6baa1b9485dbab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:31:50 | INFO | \"  Saving to local: df_vect_comments/000000000002 | 477,712 Rows by 516 Cols\"\n",
      "23:32:01 | INFO | \"Processing: comments/top/2021-07-09/000000000003.parquet\"\n",
      "23:32:04 | INFO | \"  (466445, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90488d23f933425cb6ff3a034ef0a4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:35:38 | INFO | \"  Saving to local: df_vect_comments/000000000003 | 466,445 Rows by 516 Cols\"\n",
      "23:35:49 | INFO | \"Processing: comments/top/2021-07-09/000000000004.parquet\"\n",
      "23:35:52 | INFO | \"  (583289, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df8b82c67794695bc6004fce9997d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:40:18 | INFO | \"  Saving to local: df_vect_comments/000000000004 | 583,289 Rows by 516 Cols\"\n",
      "23:40:29 | INFO | \"Processing: comments/top/2021-07-09/000000000005.parquet\"\n",
      "23:40:32 | INFO | \"  (550569, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d5c5176408488190e1b6db212a7f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:18:13 | INFO | \"  Saving to local: df_vect_comments/000000000013 | 515,423 Rows by 516 Cols\"\n",
      "00:18:24 | INFO | \"Processing: comments/top/2021-07-09/000000000014.parquet\"\n",
      "00:18:28 | INFO | \"  (542892, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e078c23741d5453ebb5a59200c3a55b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:21:43 | INFO | \"  Saving to local: df_vect_comments/000000000028 | 505,036 Rows by 516 Cols\"\n",
      "01:21:55 | INFO | \"Processing: comments/top/2021-07-09/000000000029.parquet\"\n",
      "01:21:58 | INFO | \"  (489180, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29d474e748f43ccac47e60ebff9e4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:25:45 | INFO | \"  Saving to local: df_vect_comments/000000000029 | 489,180 Rows by 516 Cols\"\n",
      "01:25:56 | INFO | \"Processing: comments/top/2021-07-09/000000000030.parquet\"\n",
      "01:26:00 | INFO | \"  (571862, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58333c1554334dabac570b7380ffffcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:30:24 | INFO | \"  Saving to local: df_vect_comments/000000000030 | 571,862 Rows by 516 Cols\"\n",
      "01:30:35 | INFO | \"Processing: comments/top/2021-07-09/000000000031.parquet\"\n",
      "01:30:38 | INFO | \"  (374952, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900ed86759be4c7c9421fcb1b01939b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:33:29 | INFO | \"  Saving to local: df_vect_comments/000000000031 | 374,952 Rows by 516 Cols\"\n",
      "01:33:39 | INFO | \"Processing: comments/top/2021-07-09/000000000032.parquet\"\n",
      "01:33:44 | INFO | \"  (538452, 6) <- updated df_comments shape AFTER removing orphan comments (w/o post)\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bb3088fde0490788e85b170109c9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"new_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=posts_path,\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    tf_batch_inference_rows=2000,\n",
    "    tf_limit_first_n_chars=1000,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19c5345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f71bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d49aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb5744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a8486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef899fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12b4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a3136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521df32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fe602",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033469b",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8beff20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
