{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1db4ace",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Run Vectorizing for all NEW subreddits in v0.4.1 \n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5e7f7",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4526c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393a6219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72795c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d358e",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ffc573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a20bfc",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890d3935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>v0.4.0_use_multi_clustering_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/17</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>v0.4.0_use_multi_clustering</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/18</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>v0.4.1_mUSE_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/19</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>v0.4.1_mUSE_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/20</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>v0.4.1_mUSE_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/21</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>v0.4.1_mUSE_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/22</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>v0.4.1_mUSE_clustering_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/23</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>v0.4.1_mUSE_clustering</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/24</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active\n",
       "17            17     v0.4.0_use_multi_clustering_test  gs://i18n-subreddit-clustering/mlflow/mlruns/17          active\n",
       "18            18          v0.4.0_use_multi_clustering  gs://i18n-subreddit-clustering/mlflow/mlruns/18          active\n",
       "19            19           v0.4.1_mUSE_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/19          active\n",
       "20            20                v0.4.1_mUSE_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/20          active\n",
       "21            21          v0.4.1_mUSE_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/21          active\n",
       "22            22               v0.4.1_mUSE_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/22          active\n",
       "23            23          v0.4.1_mUSE_clustering_test  gs://i18n-subreddit-clustering/mlflow/mlruns/23          active\n",
       "24            24               v0.4.1_mUSE_clustering  gs://i18n-subreddit-clustering/mlflow/mlruns/24          active"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a2fb1",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdec8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280c9ca",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50424f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.1 inputs - 50k subreddits - Active Subreddits (no Geo) + Geo-relevant users_l28>=100 & posts_l28>=4',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-12-14',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-12-14',\n",
       " 'folder_comments_text_and_meta': 'comments/top/2021-12-14',\n",
       " 'folder_subreddits_text_and_meta_filter': 'subreddits/top/2021-09-24'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v041 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.1_2021_12',\n",
    ")\n",
    "config_data_v041.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7539a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.1_mUSE_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.1_mUSE_inference'\n",
    "\n",
    "bucket_name = config_data_v041.config_dict['bucket_name']\n",
    "subreddits_path = config_data_v041.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = config_data_v041.config_dict['folder_posts_text_and_meta']\n",
    "\n",
    "subreddits_path_exclude = config_data_v041.config_dict['folder_subreddits_text_and_meta_filter']\n",
    "# comments_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a4537",
   "metadata": {},
   "source": [
    "# Side bar - had to move files \n",
    "## from subfolder `2021-12-14` to: `2021-12-24`...\n",
    "dislexia strikes again\n",
    "\n",
    "```python\n",
    "# check list of files\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "l_comment_files_to_process = list(bucket.list_blobs(prefix=subreddits_path))\n",
    "total_comms_file_count = len(l_comment_files_to_process)\n",
    "print(total_comms_file_count)\n",
    "\n",
    "new_folder = \"gs://i18n-subreddit-clustering/posts/top/2021-12-14/\"\n",
    "for blob_ in list(bucket.list_blobs(prefix='posts/top/2021-12-24')):\n",
    "    current_name = f\"gs://i18n-subreddit-clustering/{blob_.name}\"\n",
    "    \n",
    "    `!gsutil mv $current_name   $new_folder`  # remove tick marks, in actual code, only used them to fix display issue\n",
    "    \n",
    "# > Copying gs://i18n-subreddit-clustering/posts/top/2021-12-24/000000000001.parquet [Content-Type=application/octet-stream]...\n",
    "# > Removing gs://i18n-subreddit-clustering/posts/top/2021-12-24/000000000001.parquet...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1a10a",
   "metadata": {},
   "source": [
    "# Run test on sample files\n",
    "Only one or two files so that we catch some errors before running on whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34d5ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:27:18 | INFO | \"Start vectorize function\"\n",
      "23:27:18 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-17_232718\"\n",
      "23:27:18 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "23:27:19 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "23:27:19 | INFO | \"  Saving config to local path...\"\n",
      "23:27:19 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "23:27:19 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "23:27:20 | INFO | \"Loading model use_multilingual...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 157 calls to <function recreate_function.<locals>.restored_function_body at 0x7f887ecf1050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:27:22 | INFO | \"  0:00:02.358327 <- Load TF HUB model time elapsed\"\n",
      "23:27:22 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "23:27:22 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "23:27:22 | INFO | \"-- Loading & vectorizing COMMENTS in files: 40 --\n",
      "Expected batch size: 2600\"\n",
      "23:27:22 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000000.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000001.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000002.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000003.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000004.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000005.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000006.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000007.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000008.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000009.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000010.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000011.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000012.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000013.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000014.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000015.parquet --\"\n",
      "23:27:22 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000016.parquet --\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 158 calls to <function recreate_function.<locals>.restored_function_body at 0x7f88d2bea710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:27:23 | INFO | \"Processing: posts/top/2021-12-14/000000000017.parquet\"\n",
      "23:27:25 | WARNING | \"Found duplicate IDs in col: post_id\"\n",
      "23:27:25 | INFO | \"Keeping only one row_per ID\"\n",
      "23:27:25 | INFO | \"  (350847, 5) <- df_comments.shape AFTER removing duplicates\"\n",
      "23:27:26 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "  0%|                                                   | 0/135 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 159 calls to <function recreate_function.<locals>.restored_function_body at 0x7f8884477320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################| 135/135 [02:03<00:00,  1.10it/s]\n",
      "23:29:30 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 350,847 Rows by 515 Cols\"\n",
      " 45%|####5     | 18/40 [02:18<02:49,  7.71s/it]23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000018.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000019.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000020.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000021.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000022.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000023.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000024.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000025.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000026.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000027.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000028.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000029.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000030.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000031.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000032.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000033.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000034.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000035.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000036.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000037.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000038.parquet --\"\n",
      "23:29:41 | INFO | \"    -- Skipping file: posts/top/2021-12-14/000000000039.parquet --\"\n",
      "100%|##########| 40/40 [02:18<00:00,  3.47s/it]\n",
      "23:29:41 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "23:29:47 | INFO | \"  0:02:28.889990 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    # subreddits_path_exclude=subreddits_path_exclude,  # New param to exclude embeddings in these subs\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    # Check 17 b/c it seems to have a duplicated post\n",
    "    n_comment_files_slice_start=17,\n",
    "    n_comment_files_slice_end=18,\n",
    "\n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,  # subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2600,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11011806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6091"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b392fb",
   "metadata": {},
   "source": [
    "# Run full with `BATCHING` - `lower_case=False`\n",
    "\n",
    "60GB of RAM wasn't good enough for 19Million comments _lol_... so let's just do batching by default.\n",
    "\n",
    "\n",
    "Error without batching (loading all parquet files into a single file):\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d9d9a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.4.1_mUSE_inference_test\n",
      "posts/top/2021-12-14\n"
     ]
    }
   ],
   "source": [
    "comments_path = None\n",
    "print(mlflow_experiment_test)\n",
    "print(posts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22beb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:56:27 | INFO | \"Start vectorize function\"\n",
      "23:56:27 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-17_235627\"\n",
      "23:56:27 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "23:56:27 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "23:56:27 | INFO | \"  Saving config to local path...\"\n",
      "23:56:27 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "23:56:28 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "23:56:28 | INFO | \"Loading model use_multilingual...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 1344 calls to <function recreate_function.<locals>.restored_function_body at 0x7f88d4381440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:56:30 | INFO | \"  0:00:02.280445 <- Load TF HUB model time elapsed\"\n",
      "23:56:30 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "23:56:30 | INFO | \"Load subreddits df...\"\n",
      "23:56:31 | INFO | \"  0:00:00.684703 <- df_subs_exclude loading time elapsed\"\n",
      "23:56:31 | INFO | \"  (19262, 4) <- df_subs_exclude shape\"\n",
      "23:56:31 | INFO | \"Load subreddits df...\"\n",
      "23:56:32 | INFO | \"  0:00:00.768617 <- df_subs loading time elapsed\"\n",
      "23:56:32 | INFO | \"  (49705, 4) <- df_subs shape\"\n",
      "23:56:32 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 1345 calls to <function recreate_function.<locals>.restored_function_body at 0x7f85a07499e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:56:33 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 1346 calls to <function recreate_function.<locals>.restored_function_body at 0x7f887e9413b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###########################################| 20/20 [00:28<00:00,  1.41s/it]\n",
      "23:57:01 | INFO | \"  0:00:29.216174 <- df_subs vectorizing time elapsed\"\n",
      "23:57:01 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 49,705 Rows by 514 Cols\"\n",
      "23:57:01 | INFO | \"Converting pandas to dask...\"\n",
      "23:57:01 | INFO | \"   103.4 MB <- Memory usage\"\n",
      "23:57:01 | INFO | \"       2\t<- target Dask partitions\t  100.0 <- target MB partition size\"\n",
      "23:57:02 | INFO | \"  Logging to mlflow...\"\n",
      "23:57:04 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "23:57:05 | INFO | \"-- Loading & vectorizing COMMENTS in files: 40 --\n",
      "Expected batch size: 2500\"\n",
      "23:57:05 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]23:57:05 | INFO | \"Processing: posts/top/2021-12-14/000000000000.parquet\"\n",
      "23:57:07 | WARNING | \"Found duplicate IDs in col: post_id\"\n",
      "23:57:07 | INFO | \"Keeping only one row_per ID\"\n",
      "23:57:07 | INFO | \"  (456040, 5) <- df_comments.shape AFTER removing duplicates\"\n",
      "23:57:08 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "23:57:08 | INFO | \"  (270338, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "23:57:08 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "100%|#########################################| 109/109 [01:25<00:00,  1.28it/s]\n",
      "23:58:34 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 270,338 Rows by 515 Cols\"\n",
      "  2%|2         | 1/40 [01:39<1:04:34, 99.34s/it]23:58:44 | INFO | \"Processing: posts/top/2021-12-14/000000000001.parquet\"\n",
      "23:58:46 | WARNING | \"Found duplicate IDs in col: post_id\"\n",
      "23:58:46 | INFO | \"Keeping only one row_per ID\"\n",
      "23:58:46 | INFO | \"  (405806, 5) <- df_comments.shape AFTER removing duplicates\"\n",
      "23:58:47 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "23:58:47 | INFO | \"  (208572, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "23:58:47 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "100%|###########################################| 84/84 [01:05<00:00,  1.28it/s]\n",
      "23:59:54 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 208,572 Rows by 515 Cols\"\n",
      "  5%|5         | 2/40 [02:57<54:56, 86.76s/it]  00:00:02 | INFO | \"Processing: posts/top/2021-12-14/000000000002.parquet\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_new_fxn_no_filters-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,  # subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "    subreddits_path_exclude=subreddits_path_exclude,  # New param to exclude embeddings in these subs\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2500,\n",
    "    tf_limit_first_n_chars=850,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b370352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c092a",
   "metadata": {},
   "source": [
    "## Run it on all posts, not just the posts for new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c579944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:48:53 | INFO | \"Start vectorize function\"\n",
      "04:48:53 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-18_044853\"\n",
      "04:48:54 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "04:48:54 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "04:48:54 | INFO | \"  Saving config to local path...\"\n",
      "04:48:54 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "04:48:55 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "04:48:55 | INFO | \"Loading model use_multilingual...\"\n",
      "04:48:57 | INFO | \"  0:00:02.248069 <- Load TF HUB model time elapsed\"\n",
      "04:48:57 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "04:48:57 | INFO | \"Load subreddits df...\"\n",
      "04:48:58 | INFO | \"  0:00:00.786374 <- df_subs loading time elapsed\"\n",
      "04:48:58 | INFO | \"  (49705, 4) <- df_subs shape\"\n",
      "04:48:58 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "04:48:59 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "04:49:10 | INFO | \"progress:  35%|###########8                      | 7/20 [00:11<00:21,  1.64s/it]\"\n",
      "04:49:22 | INFO | \"progress:  35%|###########8                      | 7/20 [00:23<00:21,  1.64s/it]\"\n",
      "04:49:22 | INFO | \"progress:  85%|############################     | 17/20 [00:23<00:03,  1.33s/it]\"\n",
      "04:49:25 | INFO | \"progress: 100%|#################################| 20/20 [00:25<00:00,  1.30s/it]\"\n",
      "\n",
      "04:49:25 | INFO | \"  0:00:26.914999 <- df_subs vectorizing time elapsed\"\n",
      "04:49:25 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 49,705 Rows by 514 Cols\"\n",
      "04:49:25 | INFO | \"Converting pandas to dask...\"\n",
      "04:49:25 | INFO | \"   103.4 MB <- Memory usage\"\n",
      "04:49:25 | INFO | \"       2\t<- target Dask partitions\t  100.0 <- target MB partition size\"\n",
      "04:49:26 | INFO | \"  Logging to mlflow...\"\n",
      "04:49:28 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "04:49:28 | INFO | \"-- Loading & vectorizing COMMENTS in files: 40 --\n",
      "Expected batch size: 2500\"\n",
      "04:49:28 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "04:49:28 | INFO | \"Processing: posts/top/2021-12-14/000000000000.parquet\"\n",
      "04:49:31 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "04:49:43 | INFO | \"progress:   8%|##6                             | 15/183 [00:11<02:10,  1.29it/s]\"\n",
      "04:49:54 | INFO | \"progress:  17%|#####4                          | 31/183 [00:23<01:52,  1.35it/s]\"\n",
      "04:50:05 | INFO | \"progress:  26%|########2                       | 47/183 [00:34<01:38,  1.39it/s]\"\n",
      "04:50:17 | INFO | \"progress:  34%|###########                     | 63/183 [00:45<01:25,  1.40it/s]\"\n",
      "04:50:28 | INFO | \"progress:  44%|#############9                  | 80/183 [00:57<01:12,  1.43it/s]\"\n",
      "04:50:40 | INFO | \"progress:  53%|################9               | 97/183 [01:08<00:59,  1.44it/s]\"\n",
      "04:50:52 | INFO | \"progress:  62%|###################3           | 114/183 [01:20<00:48,  1.43it/s]\"\n",
      "04:51:03 | INFO | \"progress:  62%|###################3           | 114/183 [01:31<00:48,  1.43it/s]\"\n",
      "04:51:03 | INFO | \"progress:  72%|######################1        | 131/183 [01:32<00:35,  1.46it/s]\"\n",
      "04:51:15 | INFO | \"progress:  81%|#########################2     | 149/183 [01:43<00:22,  1.48it/s]\"\n",
      "04:51:28 | INFO | \"progress:  91%|############################1  | 166/183 [01:57<00:12,  1.41it/s]\"\n",
      "04:51:39 | INFO | \"progress: 100%|###############################| 183/183 [02:08<00:00,  1.44it/s]\"\n",
      "04:51:39 | INFO | \"progress: 100%|###############################| 183/183 [02:08<00:00,  1.43it/s]\"\n",
      "\n",
      "04:51:41 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 456,040 Rows by 515 Cols\"\n",
      "04:51:52 | INFO | \"progress:   2%|4               | 1/40 [02:24<1:33:44, 144.22s/it]\"\n",
      "04:51:53 | INFO | \"Processing: posts/top/2021-12-14/000000000001.parquet\"\n",
      "04:51:55 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "04:52:07 | INFO | \"progress:  10%|###3                            | 17/163 [00:11<01:40,  1.46it/s]\"\n",
      "04:52:18 | INFO | \"progress:  21%|######6                         | 34/163 [00:23<01:29,  1.45it/s]\"\n",
      "04:52:30 | INFO | \"progress:  31%|##########                      | 51/163 [00:34<01:16,  1.47it/s]\"\n",
      "04:52:42 | INFO | \"progress:  42%|#############3                  | 68/163 [00:46<01:05,  1.45it/s]\"\n",
      "04:52:53 | INFO | \"progress:  52%|################6               | 85/163 [00:58<00:52,  1.47it/s]\"\n",
      "04:53:04 | INFO | \"progress:  52%|################6               | 85/163 [01:08<00:52,  1.47it/s]\"\n",
      "04:53:04 | INFO | \"progress:  63%|###################3           | 102/163 [01:09<00:41,  1.48it/s]\"\n",
      "04:53:17 | INFO | \"progress:  73%|######################6        | 119/163 [01:22<00:30,  1.43it/s]\"\n",
      "04:53:29 | INFO | \"progress:  83%|#########################6     | 135/163 [01:33<00:19,  1.42it/s]\"\n",
      "04:53:41 | INFO | \"progress:  93%|############################7  | 151/163 [01:45<00:08,  1.40it/s]\"\n",
      "04:53:49 | INFO | \"progress: 100%|###############################| 163/163 [01:53<00:00,  1.44it/s]\"\n",
      "\n",
      "04:53:50 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 405,806 Rows by 515 Cols\"\n",
      "04:54:01 | INFO | \"progress:   5%|8               | 2/40 [04:32<1:25:29, 134.98s/it]\"\n",
      "04:54:01 | INFO | \"Processing: posts/top/2021-12-14/000000000002.parquet\"\n",
      "04:54:04 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "04:54:15 | INFO | \"progress:   9%|##8                             | 15/170 [00:11<01:53,  1.36it/s]\"\n",
      "04:54:27 | INFO | \"progress:  18%|#####6                          | 30/170 [00:23<01:49,  1.28it/s]\"\n",
      "04:54:38 | INFO | \"progress:  27%|########6                       | 46/170 [00:34<01:31,  1.35it/s]\"\n",
      "04:54:50 | INFO | \"progress:  37%|###########8                    | 63/170 [00:45<01:16,  1.40it/s]\"\n",
      "04:55:02 | INFO | \"progress:  47%|###############                 | 80/170 [00:58<01:04,  1.39it/s]\"\n",
      "04:55:14 | INFO | \"progress:  56%|#################8              | 95/170 [01:09<00:55,  1.35it/s]\"\n",
      "04:55:25 | INFO | \"progress:  56%|#################8              | 95/170 [01:20<00:55,  1.35it/s]\"\n",
      "04:55:25 | INFO | \"progress:  65%|####################           | 110/170 [01:21<00:44,  1.35it/s]\"\n",
      "04:55:36 | INFO | \"progress:  74%|######################7        | 125/170 [01:32<00:33,  1.35it/s]\"\n",
      "04:55:48 | INFO | \"progress:  82%|#########################5     | 140/170 [01:44<00:22,  1.32it/s]\"\n",
      "04:56:00 | INFO | \"progress:  92%|############################4  | 156/170 [01:55<00:10,  1.33it/s]\"\n",
      "04:56:11 | INFO | \"progress: 100%|###############################| 170/170 [02:06<00:00,  1.34it/s]\"\n",
      "\n",
      "04:56:13 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 424,070 Rows by 515 Cols\"\n",
      "04:56:23 | INFO | \"progress:   8%|#2              | 3/40 [06:55<1:25:21, 138.43s/it]\"\n",
      "04:56:24 | INFO | \"Processing: posts/top/2021-12-14/000000000003.parquet\"\n",
      "04:56:27 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "04:56:38 | INFO | \"progress:   9%|##8                             | 18/202 [00:11<01:53,  1.62it/s]\"\n",
      "04:56:52 | INFO | \"progress:  18%|#####7                          | 36/202 [00:25<02:00,  1.38it/s]\"\n",
      "04:57:03 | INFO | \"progress:  27%|########5                       | 54/202 [00:36<01:40,  1.47it/s]\"\n",
      "04:57:15 | INFO | \"progress:  36%|###########5                    | 73/202 [00:48<01:23,  1.55it/s]\"\n",
      "04:57:25 | INFO | \"progress:  36%|###########5                    | 73/202 [00:58<01:23,  1.55it/s]\"\n",
      "04:57:26 | INFO | \"progress:  44%|##############                  | 89/202 [00:59<01:14,  1.51it/s]\"\n",
      "04:57:37 | INFO | \"progress:  52%|################1              | 105/202 [01:10<01:05,  1.49it/s]\"\n",
      "04:57:48 | INFO | \"progress:  61%|##################8            | 123/202 [01:21<00:52,  1.51it/s]\"\n",
      "04:58:00 | INFO | \"progress:  70%|#####################7         | 142/202 [01:33<00:38,  1.56it/s]\"\n",
      "04:58:12 | INFO | \"progress:  80%|########################7      | 161/202 [01:44<00:25,  1.58it/s]\"\n",
      "04:58:25 | INFO | \"progress:  89%|###########################4   | 179/202 [01:58<00:15,  1.51it/s]\"\n",
      "04:58:36 | INFO | \"progress:  89%|###########################4   | 179/202 [02:09<00:15,  1.51it/s]\"\n",
      "04:58:36 | INFO | \"progress:  97%|############################## | 196/202 [02:09<00:04,  1.49it/s]\"\n",
      "04:58:40 | INFO | \"progress: 100%|###############################| 202/202 [02:13<00:00,  1.51it/s]\"\n",
      "\n",
      "04:58:43 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 502,883 Rows by 515 Cols\"\n",
      "04:58:54 | INFO | \"progress:  10%|#6              | 4/40 [09:25<1:25:55, 143.21s/it]\"\n",
      "04:58:54 | INFO | \"Processing: posts/top/2021-12-14/000000000004.parquet\"\n",
      "04:58:57 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "04:59:09 | INFO | \"progress:  11%|###4                            | 20/185 [00:11<01:34,  1.75it/s]\"\n",
      "04:59:22 | INFO | \"progress:  22%|######9                         | 40/185 [00:24<01:31,  1.58it/s]\"\n",
      "04:59:34 | INFO | \"progress:  31%|##########                      | 58/185 [00:36<01:20,  1.58it/s]\"\n",
      "04:59:45 | INFO | \"progress:  41%|#############1                  | 76/185 [00:47<01:08,  1.59it/s]\"\n",
      "04:59:56 | INFO | \"progress:  41%|#############1                  | 76/185 [00:58<01:08,  1.59it/s]\"\n",
      "04:59:57 | INFO | \"progress:  51%|################2               | 94/185 [00:59<00:58,  1.56it/s]\"\n",
      "05:00:08 | INFO | \"progress:  61%|##################7            | 112/185 [01:10<00:46,  1.57it/s]\"\n",
      "05:00:19 | INFO | \"progress:  71%|#####################9         | 131/185 [01:21<00:33,  1.62it/s]\"\n",
      "05:00:32 | INFO | \"progress:  81%|#########################1     | 150/185 [01:34<00:22,  1.57it/s]\"\n",
      "05:00:44 | INFO | \"progress:  90%|###########################9   | 167/185 [01:46<00:11,  1.53it/s]\"\n",
      "05:00:55 | INFO | \"progress:  99%|##############################6| 183/185 [01:58<00:01,  1.48it/s]\"\n",
      "05:00:57 | INFO | \"progress: 100%|###############################| 185/185 [01:59<00:00,  1.55it/s]\"\n",
      "\n",
      "05:00:59 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 462,488 Rows by 515 Cols\"\n",
      "05:01:10 | INFO | \"progress:  12%|##              | 5/40 [11:42<1:22:05, 140.72s/it]\"\n",
      "05:01:10 | INFO | \"Processing: posts/top/2021-12-14/000000000005.parquet\"\n",
      "05:01:13 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "05:01:25 | INFO | \"progress:   9%|##9                             | 16/172 [00:11<01:55,  1.35it/s]\"\n",
      "05:01:36 | INFO | \"progress:  19%|######1                         | 33/172 [00:23<01:35,  1.45it/s]\"\n",
      "05:01:47 | INFO | \"progress:  19%|######1                         | 33/172 [00:33<01:35,  1.45it/s]\"\n",
      "05:01:47 | INFO | \"progress:  29%|#########3                      | 50/172 [00:34<01:22,  1.48it/s]\"\n",
      "05:01:59 | INFO | \"progress:  39%|############4                   | 67/172 [00:45<01:10,  1.48it/s]\"\n",
      "05:02:10 | INFO | \"progress:  49%|###############6                | 84/172 [00:56<00:59,  1.49it/s]\"\n",
      "05:02:22 | INFO | \"progress:  59%|##################2            | 101/172 [01:09<00:48,  1.45it/s]\"\n",
      "05:02:34 | INFO | \"progress:  69%|#####################4         | 119/172 [01:20<00:35,  1.50it/s]\"\n",
      "05:02:47 | INFO | \"progress:  80%|########################6      | 137/172 [01:33<00:24,  1.46it/s]\"\n",
      "05:02:57 | INFO | \"progress:  80%|########################6      | 137/172 [01:44<00:24,  1.46it/s]\"\n",
      "05:02:58 | INFO | \"progress:  89%|###########################5   | 153/172 [01:44<00:13,  1.46it/s]\"\n",
      "05:03:09 | INFO | \"progress:  99%|##############################6| 170/172 [01:56<00:01,  1.46it/s]\"\n",
      "05:03:11 | INFO | \"progress: 100%|###############################| 172/172 [01:57<00:00,  1.46it/s]\"\n",
      "\n",
      "05:03:13 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 427,762 Rows by 515 Cols\"\n",
      "05:03:24 | INFO | \"progress:  15%|##4             | 6/40 [13:55<1:18:23, 138.35s/it]\"\n",
      "05:03:24 | INFO | \"Processing: posts/top/2021-12-14/000000000006.parquet\"\n",
      "05:03:27 | INFO | \"Getting embeddings in batches of size: 2500\"\n",
      "05:03:38 | INFO | \"progress:   9%|##7                             | 15/176 [00:11<02:00,  1.34it/s]\"\n",
      "05:03:50 | INFO | \"progress:  18%|#####8                          | 32/176 [00:22<01:41,  1.43it/s]\"\n",
      "05:04:02 | INFO | \"progress:  28%|########9                       | 49/176 [00:34<01:29,  1.42it/s]\"\n",
      "05:04:13 | INFO | \"progress:  38%|############                    | 66/176 [00:45<01:15,  1.46it/s]\"\n",
      "05:04:25 | INFO | \"progress:  47%|###############                 | 83/176 [00:58<01:05,  1.42it/s]\"\n",
      "05:04:38 | INFO | \"progress:  56%|##################              | 99/176 [01:10<00:55,  1.39it/s]\"\n",
      "05:04:48 | INFO | \"progress:  56%|##################              | 99/176 [01:20<00:55,  1.39it/s]\"\n",
      "05:04:49 | INFO | \"progress:  66%|####################4          | 116/176 [01:21<00:42,  1.42it/s]\"\n",
      "05:05:02 | INFO | \"progress:  76%|#######################4       | 133/176 [01:34<00:30,  1.39it/s]\"\n",
      "05:05:13 | INFO | \"progress:  85%|##########################2    | 149/176 [01:45<00:19,  1.40it/s]\"\n",
      "05:05:25 | INFO | \"progress:  95%|#############################4 | 167/176 [01:57<00:06,  1.45it/s]\"\n",
      "05:05:31 | INFO | \"progress: 100%|###############################| 176/176 [02:03<00:00,  1.42it/s]\"\n",
      "\n",
      "05:05:33 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 439,951 Rows by 515 Cols\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_new_fxn_no_filters-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,  # subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2500,\n",
    "    tf_limit_first_n_chars=850,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d4c70",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589864d",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37306a44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:05:02 | INFO | \"Start vectorize function\"\n",
      "10:05:02 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_100502\"\n",
      "10:05:02 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "10:05:03 | INFO | \"  Saving config to local path...\"\n",
      "10:05:03 | INFO | \"  Logging config to mlflow...\"\n",
      "10:05:03 | INFO | \"Loading model use_multilingual...\"\n",
      "10:05:05 | INFO | \"  0:00:02.265257 <- Load TF HUB model time elapsed\"\n",
      "10:05:05 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "10:05:05 | INFO | \"Load subreddits df...\"\n",
      "10:05:06 | INFO | \"  0:00:00.683829 <- df_subs loading time elapsed\"\n",
      "10:05:06 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "10:05:06 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "10:05:06 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]10:05:18 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[568066,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_45494289]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:26<00:00,  3.33s/it]\n",
      "10:05:33 | INFO | \"  0:00:27.219040 <- df_subs vectorizing time elapsed\"\n",
      "10:05:33 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "10:05:33 | INFO | \"Converting pandas to dask...\"\n",
      "10:05:33 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "10:05:33 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "10:05:34 | INFO | \"  Logging to mlflow...\"\n",
      "10:05:36 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "10:05:36 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2600\"\n",
      "10:05:36 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]10:05:36 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      " 56%|#######################4                  | 63/113 [01:18<01:03,  1.27s/it]10:07:07 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[594158,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_45494289]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 113/113 [02:30<00:00,  1.33s/it]\n",
      "10:08:10 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:43<1:10:52, 163.57s/it]10:08:19 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "100%|#########################################| 158/158 [03:15<00:00,  1.24s/it]\n",
      "10:11:39 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:13<1:19:28, 190.76s/it]10:11:49 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "100%|#########################################| 127/127 [02:36<00:00,  1.23s/it]\n",
      "10:14:30 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [09:03<1:12:34, 181.44s/it]10:14:40 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "100%|###########################################| 98/98 [02:03<00:00,  1.26s/it]\n",
      "10:16:46 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [11:19<1:02:35, 163.27s/it]10:16:55 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "100%|#########################################| 106/106 [02:11<00:00,  1.24s/it]\n",
      "10:19:09 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [13:42<57:16, 156.19s/it]  10:19:19 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "100%|#########################################| 138/138 [02:56<00:00,  1.28s/it]\n",
      "10:22:19 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [16:53<58:46, 167.94s/it]10:22:29 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "100%|#########################################| 126/126 [02:46<00:00,  1.32s/it]\n",
      "10:25:20 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 326,821 Rows by 515 Cols\"\n",
      " 26%|##5       | 7/27 [19:53<57:18, 171.94s/it]10:25:30 | INFO | \"Processing: posts/top/2021-09-27/000000000007.parquet\"\n",
      "100%|#########################################| 107/107 [02:21<00:00,  1.32s/it]\n",
      "10:27:54 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 277,995 Rows by 515 Cols\"\n",
      " 30%|##9       | 8/27 [22:27<52:39, 166.31s/it]10:28:04 | INFO | \"Processing: posts/top/2021-09-27/000000000008.parquet\"\n",
      "100%|#########################################| 142/142 [03:12<00:00,  1.35s/it]\n",
      "10:31:20 | INFO | \"  Saving to local: df_vect_posts/000000000008 | 368,185 Rows by 515 Cols\"\n",
      " 33%|###3      | 9/27 [25:54<53:40, 178.90s/it]10:31:30 | INFO | \"Processing: posts/top/2021-09-27/000000000009.parquet\"\n",
      "100%|#########################################| 115/115 [02:36<00:00,  1.36s/it]\n",
      "10:34:10 | INFO | \"  Saving to local: df_vect_posts/000000000009 | 297,827 Rows by 515 Cols\"\n",
      " 37%|###7      | 10/27 [28:43<49:51, 175.98s/it]10:34:20 | INFO | \"Processing: posts/top/2021-09-27/000000000010.parquet\"\n",
      "100%|#########################################| 122/122 [02:44<00:00,  1.35s/it]\n",
      "10:37:09 | INFO | \"  Saving to local: df_vect_posts/000000000010 | 316,831 Rows by 515 Cols\"\n",
      " 41%|####      | 11/27 [31:43<47:10, 176.92s/it]10:37:19 | INFO | \"Processing: posts/top/2021-09-27/000000000011.parquet\"\n",
      "100%|#########################################| 117/117 [02:44<00:00,  1.40s/it]\n",
      "10:40:07 | INFO | \"  Saving to local: df_vect_posts/000000000011 | 303,271 Rows by 515 Cols\"\n",
      " 44%|####4     | 12/27 [34:40<44:18, 177.23s/it]10:40:17 | INFO | \"Processing: posts/top/2021-09-27/000000000012.parquet\"\n",
      "100%|#########################################| 113/113 [02:28<00:00,  1.31s/it]\n",
      "10:42:49 | INFO | \"  Saving to local: df_vect_posts/000000000012 | 291,892 Rows by 515 Cols\"\n",
      " 48%|####8     | 13/27 [37:23<40:18, 172.74s/it]10:42:59 | INFO | \"Processing: posts/top/2021-09-27/000000000013.parquet\"\n",
      "100%|#########################################| 117/117 [02:34<00:00,  1.32s/it]\n",
      "10:45:38 | INFO | \"  Saving to local: df_vect_posts/000000000013 | 302,591 Rows by 515 Cols\"\n",
      " 52%|#####1    | 14/27 [40:11<37:08, 171.42s/it]10:45:48 | INFO | \"Processing: posts/top/2021-09-27/000000000014.parquet\"\n",
      "100%|#########################################| 125/125 [02:53<00:00,  1.39s/it]\n",
      "10:48:45 | INFO | \"  Saving to local: df_vect_posts/000000000014 | 322,877 Rows by 515 Cols\"\n",
      " 56%|#####5    | 15/27 [43:19<35:15, 176.26s/it]10:48:55 | INFO | \"Processing: posts/top/2021-09-27/000000000015.parquet\"\n",
      "100%|#########################################| 124/124 [02:50<00:00,  1.38s/it]\n",
      "10:51:50 | INFO | \"  Saving to local: df_vect_posts/000000000015 | 322,124 Rows by 515 Cols\"\n",
      " 59%|#####9    | 16/27 [46:24<32:48, 178.91s/it]10:52:00 | INFO | \"Processing: posts/top/2021-09-27/000000000016.parquet\"\n",
      "100%|#########################################| 120/120 [02:41<00:00,  1.35s/it]\n",
      "10:54:46 | INFO | \"  Saving to local: df_vect_posts/000000000016 | 309,522 Rows by 515 Cols\"\n",
      " 63%|######2   | 17/27 [49:19<29:38, 177.87s/it]10:54:56 | INFO | \"Processing: posts/top/2021-09-27/000000000017.parquet\"\n",
      "100%|#########################################| 122/122 [02:50<00:00,  1.39s/it]\n",
      "10:57:50 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 315,380 Rows by 515 Cols\"\n",
      " 67%|######6   | 18/27 [52:23<26:57, 179.74s/it]10:58:00 | INFO | \"Processing: posts/top/2021-09-27/000000000018.parquet\"\n",
      "100%|#########################################| 116/116 [02:43<00:00,  1.41s/it]\n",
      "11:00:47 | INFO | \"  Saving to local: df_vect_posts/000000000018 | 300,659 Rows by 515 Cols\"\n",
      " 70%|#######   | 19/27 [55:21<23:52, 179.02s/it]11:00:57 | INFO | \"Processing: posts/top/2021-09-27/000000000019.parquet\"\n",
      "100%|###########################################| 96/96 [02:13<00:00,  1.39s/it]\n",
      "11:03:14 | INFO | \"  Saving to local: df_vect_posts/000000000019 | 248,516 Rows by 515 Cols\"\n",
      " 74%|#######4  | 20/27 [57:46<19:43, 169.01s/it]11:03:23 | INFO | \"Processing: posts/top/2021-09-27/000000000020.parquet\"\n",
      "100%|#########################################| 115/115 [02:39<00:00,  1.38s/it]\n",
      "11:06:05 | INFO | \"  Saving to local: df_vect_posts/000000000020 | 298,379 Rows by 515 Cols\"\n",
      " 78%|#######7  | 21/27 [1:00:39<17:00, 170.05s/it]11:06:15 | INFO | \"Processing: posts/top/2021-09-27/000000000021.parquet\"\n",
      "100%|#########################################| 129/129 [02:58<00:00,  1.39s/it]\n",
      "11:09:18 | INFO | \"  Saving to local: df_vect_posts/000000000021 | 334,647 Rows by 515 Cols\"\n",
      " 81%|########1 | 22/27 [1:03:51<14:43, 176.79s/it]11:09:28 | INFO | \"Processing: posts/top/2021-09-27/000000000022.parquet\"\n",
      "100%|#########################################| 127/127 [02:53<00:00,  1.36s/it]\n",
      "11:12:37 | INFO | \"  Saving to local: df_vect_posts/000000000022 | 329,376 Rows by 515 Cols\"\n",
      " 85%|########5 | 23/27 [1:07:10<12:13, 183.40s/it]11:12:46 | INFO | \"Processing: posts/top/2021-09-27/000000000023.parquet\"\n",
      " 42%|#################4                        | 52/125 [01:11<01:38,  1.35s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2600,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241cc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "692687df",
   "metadata": {},
   "source": [
    "# Debug - why are there duplicates in file 17?\n",
    "\n",
    "Looks like the reason for the duplicate is that there's at least one post where the OCR text is different... unclear why we have two separate rows, but maybe we can drop the dupe and keep the post wit the longest text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbe471e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "l_comment_files_to_process = list(bucket.list_blobs(prefix=posts_path))\n",
    "total_comms_file_count = len(l_comment_files_to_process)\n",
    "print(total_comms_file_count)\n",
    "\n",
    "new_folder = \"gs://i18n-subreddit-clustering/posts/top/2021-12-14/\"\n",
    "for blob_ in l_comment_files_to_process:\n",
    "    current_name = f\"gs://i18n-subreddit-clustering/{blob_.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a32a962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 335 ms, total: 1.76 s\n",
      "Wall time: 2.86 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(350848, 44)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "l_cols_ix_posts = [\n",
    "    'subreddit_name',\n",
    "    'subreddit_id',\n",
    "    'post_id',\n",
    "]\n",
    "l_cols_text = [\n",
    "    'text',\n",
    "    'weighted_language',\n",
    "    'weighted_language_probability',\n",
    "]\n",
    "\n",
    "df_17 = pd.read_parquet(\n",
    "    f\"gs://i18n-subreddit-clustering/posts/top/2021-12-14/000000000017.parquet\",\n",
    "    # columns=l_cols_ix_posts + l_cols_text,\n",
    ")\n",
    "df_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d543b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_5e7b5_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >dtype</th>        <th class=\"col_heading level0 col1\" >count</th>        <th class=\"col_heading level0 col2\" >unique</th>        <th class=\"col_heading level0 col3\" >unique-percent</th>        <th class=\"col_heading level0 col4\" >null-count</th>        <th class=\"col_heading level0 col5\" >null-percent</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_5e7b5_level0_row0\" class=\"row_heading level0 row0\" >subreddit_name</th>\n",
       "                        <td id=\"T_5e7b5_row0_col0\" class=\"data row0 col0\" >object</td>\n",
       "                        <td id=\"T_5e7b5_row0_col1\" class=\"data row0 col1\" >350,848</td>\n",
       "                        <td id=\"T_5e7b5_row0_col2\" class=\"data row0 col2\" >1,102</td>\n",
       "                        <td id=\"T_5e7b5_row0_col3\" class=\"data row0 col3\" >0.31%</td>\n",
       "                        <td id=\"T_5e7b5_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "                        <td id=\"T_5e7b5_row0_col5\" class=\"data row0 col5\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5e7b5_level0_row1\" class=\"row_heading level0 row1\" >subreddit_id</th>\n",
       "                        <td id=\"T_5e7b5_row1_col0\" class=\"data row1 col0\" >object</td>\n",
       "                        <td id=\"T_5e7b5_row1_col1\" class=\"data row1 col1\" >350,848</td>\n",
       "                        <td id=\"T_5e7b5_row1_col2\" class=\"data row1 col2\" >1,102</td>\n",
       "                        <td id=\"T_5e7b5_row1_col3\" class=\"data row1 col3\" >0.31%</td>\n",
       "                        <td id=\"T_5e7b5_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "                        <td id=\"T_5e7b5_row1_col5\" class=\"data row1 col5\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_5e7b5_level0_row2\" class=\"row_heading level0 row2\" >post_id</th>\n",
       "                        <td id=\"T_5e7b5_row2_col0\" class=\"data row2 col0\" >object</td>\n",
       "                        <td id=\"T_5e7b5_row2_col1\" class=\"data row2 col1\" >350,848</td>\n",
       "                        <td id=\"T_5e7b5_row2_col2\" class=\"data row2 col2\" >350,847</td>\n",
       "                        <td id=\"T_5e7b5_row2_col3\" class=\"data row2 col3\" >100.00%</td>\n",
       "                        <td id=\"T_5e7b5_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "                        <td id=\"T_5e7b5_row2_col5\" class=\"data row2 col5\" >0.00%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f88d34a31d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_describe(df_17[l_cols_ix_posts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b569d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_post_in_sub</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>submit_date</th>\n",
       "      <th>removed</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>upvotes_lookup</th>\n",
       "      <th>downvotes_lookup</th>\n",
       "      <th>net_upvotes_lookup</th>\n",
       "      <th>neutered</th>\n",
       "      <th>verdict</th>\n",
       "      <th>content_category</th>\n",
       "      <th>comments</th>\n",
       "      <th>successful</th>\n",
       "      <th>app_name</th>\n",
       "      <th>post_type</th>\n",
       "      <th>post_nsfw</th>\n",
       "      <th>post_url</th>\n",
       "      <th>geolocation_country_code</th>\n",
       "      <th>rating_short</th>\n",
       "      <th>rating_name</th>\n",
       "      <th>primary_topic</th>\n",
       "      <th>geo_relevant_countries</th>\n",
       "      <th>geo_relevant_country_codes</th>\n",
       "      <th>geo_relevant_subreddit</th>\n",
       "      <th>geo_relevant_subreddit_all</th>\n",
       "      <th>geo_relevant_subreddit_v04</th>\n",
       "      <th>ambassador_or_default_any</th>\n",
       "      <th>ambassador_or_default_sub_france</th>\n",
       "      <th>ambassador_or_default_sub_germany</th>\n",
       "      <th>weighted_language</th>\n",
       "      <th>weighted_language_probability</th>\n",
       "      <th>post_language_preference</th>\n",
       "      <th>ocr_images_in_post_count</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>ocr_text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>ocr_text_word_count</th>\n",
       "      <th>post_url_for_embeddings</th>\n",
       "      <th>text</th>\n",
       "      <th>ocr_inferred_text_agg_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62218</th>\n",
       "      <td>602</td>\n",
       "      <td>caguns</td>\n",
       "      <td>t5_31iwc</td>\n",
       "      <td>t3_rdma9j</td>\n",
       "      <td>t2_cugrenid</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>web2x</td>\n",
       "      <td>multi_media</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/CAguns/comments/rdma9j/the_journey_is_complete/</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Hobbies</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3971</td>\n",
       "      <td>37.0</td>\n",
       "      <td>745</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>The journey is complete! Right around the beginning of September, being a new gun owner in CA, I decided that I wanted a 1911 handgun. But not just any 1911 - it *had* to be a Springfield 1911 Loaded Marine Corps Operator. But even more...</td>\n",
       "      <td>SPRINGFIELD ARMORY  SI GENESEO IL USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62219</th>\n",
       "      <td>602</td>\n",
       "      <td>caguns</td>\n",
       "      <td>t5_31iwc</td>\n",
       "      <td>t3_rdma9j</td>\n",
       "      <td>t2_cugrenid</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>web2x</td>\n",
       "      <td>multi_media</td>\n",
       "      <td>False</td>\n",
       "      <td>/r/CAguns/comments/rdma9j/the_journey_is_complete/</td>\n",
       "      <td>us</td>\n",
       "      <td>E</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>Hobbies</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>3971</td>\n",
       "      <td>27.0</td>\n",
       "      <td>745</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>The journey is complete! Right around the beginning of September, being a new gun owner in CA, I decided that I wanted a 1911 handgun. But not just any 1911 - it *had* to be a Springfield 1911 Loaded Marine Corps Operator. But even more...</td>\n",
       "      <td>USA H  USAH LOADED OPERATOR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rank_post_in_sub subreddit_name subreddit_id    post_id      user_id submit_date  removed  upvotes  upvotes_lookup  downvotes_lookup  net_upvotes_lookup  neutered verdict content_category  comments  successful app_name    post_type post_nsfw  \\\n",
       "62218               602         caguns     t5_31iwc  t3_rdma9j  t2_cugrenid  2021-12-10        0        6               8                 3                   5     False    None             None        11           1    web2x  multi_media     False   \n",
       "62219               602         caguns     t5_31iwc  t3_rdma9j  t2_cugrenid  2021-12-10        0        6               8                 3                   5     False    None             None        11           1    web2x  multi_media     False   \n",
       "\n",
       "                                                 post_url geolocation_country_code rating_short rating_name primary_topic geo_relevant_countries geo_relevant_country_codes  geo_relevant_subreddit geo_relevant_subreddit_all  geo_relevant_subreddit_v04 ambassador_or_default_any  \\\n",
       "62218  /r/CAguns/comments/rdma9j/the_journey_is_complete/                       us            E    Everyone       Hobbies                   None                       None                   False                       None                       False                      None   \n",
       "62219  /r/CAguns/comments/rdma9j/the_journey_is_complete/                       us            E    Everyone       Hobbies                   None                       None                   False                       None                       False                      None   \n",
       "\n",
       "      ambassador_or_default_sub_france ambassador_or_default_sub_germany weighted_language  weighted_language_probability post_language_preference  ocr_images_in_post_count flair_text  text_len  ocr_text_len  text_word_count  ocr_text_word_count post_url_for_embeddings  \\\n",
       "62218                             None                              None                en                       0.999996                       en                       1.0       None      3971          37.0              745                  6.0                    None   \n",
       "62219                             None                              None                en                       0.999996                       en                       1.0       None      3971          27.0              745                  5.0                    None   \n",
       "\n",
       "                                                                                                                                                                                                                                                  text            ocr_inferred_text_agg_clean  \n",
       "62218  The journey is complete! Right around the beginning of September, being a new gun owner in CA, I decided that I wanted a 1911 handgun. But not just any 1911 - it *had* to be a Springfield 1911 Loaded Marine Corps Operator. But even more...  SPRINGFIELD ARMORY  SI GENESEO IL USA  \n",
       "62219  The journey is complete! Right around the beginning of September, being a new gun owner in CA, I decided that I wanted a 1911 handgun. But not just any 1911 - it *had* to be a Springfield 1911 Loaded Marine Corps Operator. But even more...            USA H  USAH LOADED OPERATOR  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_17[df_17.duplicated(subset=l_cols_ix_posts, keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "364fdd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_post_in_sub</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>submit_date</th>\n",
       "      <th>removed</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>upvotes_lookup</th>\n",
       "      <th>downvotes_lookup</th>\n",
       "      <th>net_upvotes_lookup</th>\n",
       "      <th>neutered</th>\n",
       "      <th>verdict</th>\n",
       "      <th>content_category</th>\n",
       "      <th>comments</th>\n",
       "      <th>successful</th>\n",
       "      <th>app_name</th>\n",
       "      <th>post_type</th>\n",
       "      <th>post_nsfw</th>\n",
       "      <th>post_url</th>\n",
       "      <th>geolocation_country_code</th>\n",
       "      <th>rating_short</th>\n",
       "      <th>rating_name</th>\n",
       "      <th>primary_topic</th>\n",
       "      <th>geo_relevant_countries</th>\n",
       "      <th>geo_relevant_country_codes</th>\n",
       "      <th>geo_relevant_subreddit</th>\n",
       "      <th>geo_relevant_subreddit_all</th>\n",
       "      <th>geo_relevant_subreddit_v04</th>\n",
       "      <th>ambassador_or_default_any</th>\n",
       "      <th>ambassador_or_default_sub_france</th>\n",
       "      <th>ambassador_or_default_sub_germany</th>\n",
       "      <th>weighted_language</th>\n",
       "      <th>weighted_language_probability</th>\n",
       "      <th>post_language_preference</th>\n",
       "      <th>ocr_images_in_post_count</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>ocr_text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>ocr_text_word_count</th>\n",
       "      <th>post_url_for_embeddings</th>\n",
       "      <th>text</th>\n",
       "      <th>ocr_inferred_text_agg_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [rank_post_in_sub, subreddit_name, subreddit_id, post_id, user_id, submit_date, removed, upvotes, upvotes_lookup, downvotes_lookup, net_upvotes_lookup, neutered, verdict, content_category, comments, successful, app_name, post_type, post_nsfw, post_url, geolocation_country_code, rating_short, rating_name, primary_topic, geo_relevant_countries, geo_relevant_country_codes, geo_relevant_subreddit, geo_relevant_subreddit_all, geo_relevant_subreddit_v04, ambassador_or_default_any, ambassador_or_default_sub_france, ambassador_or_default_sub_germany, weighted_language, weighted_language_probability, post_language_preference, ocr_images_in_post_count, flair_text, text_len, ocr_text_len, text_word_count, ocr_text_word_count, post_url_for_embeddings, text, ocr_inferred_text_agg_clean]\n",
       "Index: []"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_17[df_17.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d92add",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb3092",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbe4aa02",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
