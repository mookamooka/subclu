{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ac98ae",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Run Vectorizing for all NEW subreddits in v0.4.1 \n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or comment level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61da0b",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0104015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b5e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6591f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b90e6",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734c59ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec44431",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c054e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Default</td>\n",
       "      <td>./mlruns/0</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fse_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/1</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fse_vectorize_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/2</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>subreddit_description_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/3</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fse_vectorize_v1.1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/4</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>use_multilingual_v0.1_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/5</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>use_multilingual_v1</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/6</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>use_multilingual_v1_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/7</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>use_multilingual_v1_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/8</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>v0.3.2_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/9</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>v0.3.2_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/10</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>v0.3.2_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/11</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>v0.3.2_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/12</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>v0.4.0_use_multi_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/13</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>v0.4.0_use_multi_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/14</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>v0.4.0_use_multi_clustering_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/17</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>v0.4.0_use_multi_clustering</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/18</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>v0.4.1_mUSE_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/19</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>v0.4.1_mUSE_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/20</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>v0.4.1_mUSE_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/21</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>v0.4.1_mUSE_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/22</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>v0.4.1_mUSE_clustering_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/23</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>v0.4.1_mUSE_clustering</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/24</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                                 name                                artifact_location lifecycle_stage\n",
       "0              0                              Default                                       ./mlruns/0          active\n",
       "1              1                               fse_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/1          active\n",
       "2              2                     fse_vectorize_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/2          active\n",
       "3              3             subreddit_description_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/3          active\n",
       "4              4                   fse_vectorize_v1.1   gs://i18n-subreddit-clustering/mlflow/mlruns/4          active\n",
       "5              5           use_multilingual_v0.1_test   gs://i18n-subreddit-clustering/mlflow/mlruns/5          active\n",
       "6              6                  use_multilingual_v1   gs://i18n-subreddit-clustering/mlflow/mlruns/6          active\n",
       "7              7  use_multilingual_v1_aggregates_test   gs://i18n-subreddit-clustering/mlflow/mlruns/7          active\n",
       "8              8       use_multilingual_v1_aggregates   gs://i18n-subreddit-clustering/mlflow/mlruns/8          active\n",
       "9              9      v0.3.2_use_multi_inference_test   gs://i18n-subreddit-clustering/mlflow/mlruns/9          active\n",
       "10            10           v0.3.2_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/10          active\n",
       "11            11     v0.3.2_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/11          active\n",
       "12            12          v0.3.2_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/12          active\n",
       "13            13      v0.4.0_use_multi_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/13          active\n",
       "14            14           v0.4.0_use_multi_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/14          active\n",
       "15            15     v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16          v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active\n",
       "17            17     v0.4.0_use_multi_clustering_test  gs://i18n-subreddit-clustering/mlflow/mlruns/17          active\n",
       "18            18          v0.4.0_use_multi_clustering  gs://i18n-subreddit-clustering/mlflow/mlruns/18          active\n",
       "19            19           v0.4.1_mUSE_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/19          active\n",
       "20            20                v0.4.1_mUSE_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/20          active\n",
       "21            21          v0.4.1_mUSE_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/21          active\n",
       "22            22               v0.4.1_mUSE_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/22          active\n",
       "23            23          v0.4.1_mUSE_clustering_test  gs://i18n-subreddit-clustering/mlflow/mlruns/23          active\n",
       "24            24               v0.4.1_mUSE_clustering  gs://i18n-subreddit-clustering/mlflow/mlruns/24          active"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97eaa92",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b913b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f410857",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f9cd633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.1 inputs - 50k subreddits - Active Subreddits (no Geo) + Geo-relevant users_l28>=100 & posts_l28>=4',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-12-14',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-12-14',\n",
       " 'folder_comments_text_and_meta': 'comments/top/2021-12-14',\n",
       " 'folder_subreddits_text_and_meta_filter': 'subreddits/top/2021-09-24'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v041 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.1_2021_12',\n",
    "#     config_name='top_subreddits_2021_07_16',\n",
    ")\n",
    "config_data_v041.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0bb15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.1_mUSE_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.1_mUSE_inference'\n",
    "\n",
    "bucket_name = config_data_v041.config_dict['bucket_name']\n",
    "subreddits_path = config_data_v041.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = config_data_v041.config_dict['folder_posts_text_and_meta']\n",
    "\n",
    "subreddits_path_exclude = config_data_v041.config_dict['folder_subreddits_text_and_meta_filter']\n",
    "# comments_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783c777",
   "metadata": {},
   "source": [
    "# Side bar - had to move files \n",
    "## from subfolder `2021-12-14` to: `2021-12-24`...\n",
    "dislexia strikes again\n",
    "\n",
    "```python\n",
    "# check list of files\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "l_comment_files_to_process = list(bucket.list_blobs(prefix=subreddits_path))\n",
    "total_comms_file_count = len(l_comment_files_to_process)\n",
    "print(total_comms_file_count)\n",
    "\n",
    "new_folder = \"gs://i18n-subreddit-clustering/posts/top/2021-12-14/\"\n",
    "for blob_ in list(bucket.list_blobs(prefix='posts/top/2021-12-24')):\n",
    "    current_name = f\"gs://i18n-subreddit-clustering/{blob_.name}\"\n",
    "    \n",
    "    `!gsutil mv $current_name   $new_folder`  # remove tick marks, in actual code, only used them to fix display issue\n",
    "    \n",
    "# > Copying gs://i18n-subreddit-clustering/posts/top/2021-12-24/000000000001.parquet [Content-Type=application/octet-stream]...\n",
    "# > Removing gs://i18n-subreddit-clustering/posts/top/2021-12-24/000000000001.parquet...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635c9a3",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=False`\n",
    "Let's see if the current refactor is good enough or if I really need to manually batch files...\n",
    "\n",
    "**answer**: no it wasn't good enough -- 60GB of RAM wasn't good enough for 19Million comments _lol_.\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cbb41bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.4.1_mUSE_inference_test\n",
      "posts/top/2021-12-14\n"
     ]
    }
   ],
   "source": [
    "comments_path = None\n",
    "print(mlflow_experiment_test)\n",
    "print(posts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a136d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2458e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_new_fxn{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,  # subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "    subreddits_path_exclude=subreddits_path_exclude,  # New param to exclude embeddings in these subs\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2500,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=2,\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    # try slicing later files?\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e190d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d88796",
   "metadata": {},
   "source": [
    "## Test - Re-do comments with new batching logic\n",
    "Trying to do all 19 million comments at once broke, sigh, so need to batch one file at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a5a50",
   "metadata": {},
   "source": [
    "### Re-run comments and log to non-test mlflow experiment\n",
    "\n",
    "\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf40da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:58:41 | INFO | \"Start vectorize function\"\n",
      "20:58:41 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-17_205841\"\n",
      "20:58:41 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "20:58:44 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "20:58:44 | INFO | \"  Saving config to local path...\"\n",
      "20:58:44 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "20:58:44 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "20:58:45 | INFO | \"Loading model use_multilingual...\"\n",
      "20:58:47 | INFO | \"  0:00:02.292857 <- Load TF HUB model time elapsed\"\n",
      "20:58:47 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "20:58:47 | INFO | \"Load subreddits df...\"\n",
      "20:58:48 | INFO | \"  0:00:00.790027 <- df_subs_exclude loading time elapsed\"\n",
      "20:58:48 | INFO | \"  (19262, 4) <- df_subs_exclude shape\"\n",
      "20:58:48 | INFO | \"Load subreddits df...\"\n",
      "20:58:49 | INFO | \"  0:00:01.292345 <- df_subs loading time elapsed\"\n",
      "20:58:49 | INFO | \"  (49705, 4) <- df_subs shape\"\n",
      "20:58:49 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "20:58:50 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|###########################################| 20/20 [00:27<00:00,  1.35s/it]\n",
      "20:59:17 | INFO | \"  0:00:28.085073 <- df_subs vectorizing time elapsed\"\n",
      "20:59:17 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 49,705 Rows by 514 Cols\"\n",
      "20:59:17 | INFO | \"Converting pandas to dask...\"\n",
      "20:59:17 | INFO | \"   103.4 MB <- Memory usage\"\n",
      "20:59:17 | INFO | \"       2\t<- target Dask partitions\t  100.0 <- target MB partition size\"\n",
      "20:59:18 | INFO | \"  Logging to mlflow...\"\n",
      "20:59:21 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "20:59:21 | INFO | \"-- Loading & vectorizing COMMENTS in files: 40 --\n",
      "Expected batch size: 2600\"\n",
      "20:59:21 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]20:59:21 | INFO | \"Processing: posts/top/2021-12-14/000000000000.parquet\"\n",
      "20:59:24 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "20:59:24 | INFO | \"  (270338, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "20:59:24 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|#########################################| 104/104 [01:17<00:00,  1.34it/s]\n",
      "21:00:43 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 270,338 Rows by 515 Cols\"\n",
      "  2%|2         | 1/40 [01:31<59:31, 91.57s/it]21:00:53 | INFO | \"Processing: posts/top/2021-12-14/000000000001.parquet\"\n",
      "21:00:55 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:00:55 | INFO | \"  (208572, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:00:55 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|###########################################| 81/81 [01:00<00:00,  1.35it/s]\n",
      "21:01:56 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 208,572 Rows by 515 Cols\"\n",
      "  5%|5         | 2/40 [02:43<50:43, 80.09s/it]21:02:05 | INFO | \"Processing: posts/top/2021-12-14/000000000002.parquet\"\n",
      "21:02:08 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:02:08 | INFO | \"  (190100, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:02:08 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|###########################################| 74/74 [00:59<00:00,  1.25it/s]\n",
      "21:03:08 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 190,100 Rows by 515 Cols\"\n",
      "  8%|7         | 3/40 [03:55<47:00, 76.24s/it]21:03:16 | INFO | \"Processing: posts/top/2021-12-14/000000000003.parquet\"\n",
      "21:03:20 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:03:20 | INFO | \"  (490046, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:03:20 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|#########################################| 189/189 [02:22<00:00,  1.32it/s]\n",
      "21:05:46 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 490,046 Rows by 515 Cols\"\n",
      " 10%|#         | 4/40 [06:35<1:05:43, 109.54s/it]21:05:57 | INFO | \"Processing: posts/top/2021-12-14/000000000004.parquet\"\n",
      "21:05:59 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:05:59 | INFO | \"  (379968, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:06:00 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|#########################################| 147/147 [01:48<00:00,  1.36it/s]\n",
      "21:07:50 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 379,968 Rows by 515 Cols\"\n",
      " 12%|#2        | 5/40 [08:40<1:06:59, 114.85s/it]21:08:01 | INFO | \"Processing: posts/top/2021-12-14/000000000005.parquet\"\n",
      "21:08:04 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:08:04 | INFO | \"  (338492, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:08:05 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|#########################################| 131/131 [01:40<00:00,  1.31it/s]\n",
      "21:09:47 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 338,492 Rows by 515 Cols\"\n",
      " 15%|#5        | 6/40 [10:36<1:05:24, 115.44s/it]21:09:58 | INFO | \"Processing: posts/top/2021-12-14/000000000006.parquet\"\n",
      "21:10:01 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:10:01 | INFO | \"  (324549, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:10:02 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|#########################################| 125/125 [01:36<00:00,  1.30it/s]\n",
      "21:11:40 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 324,549 Rows by 515 Cols\"\n",
      " 18%|#7        | 7/40 [12:29<1:03:02, 114.63s/it]21:11:51 | INFO | \"Processing: posts/top/2021-12-14/000000000007.parquet\"\n",
      "21:11:53 | INFO | \"  Excluding posts for subs to exclude...\"\n",
      "21:11:53 | INFO | \"  (256604, 5) <- df_comments.shape AFTER excluding subreddits\"\n",
      "21:11:54 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "100%|###########################################| 99/99 [01:15<00:00,  1.31it/s]\n",
      "21:13:10 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 256,604 Rows by 515 Cols\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path_exclude=subreddits_path_exclude,  # New param to exclude embeddings in these subs\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2600,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ca1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01980999",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36588b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:05:02 | INFO | \"Start vectorize function\"\n",
      "10:05:02 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-09-28_100502\"\n",
      "10:05:02 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "10:05:03 | INFO | \"  Saving config to local path...\"\n",
      "10:05:03 | INFO | \"  Logging config to mlflow...\"\n",
      "10:05:03 | INFO | \"Loading model use_multilingual...\"\n",
      "10:05:05 | INFO | \"  0:00:02.265257 <- Load TF HUB model time elapsed\"\n",
      "10:05:05 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "10:05:05 | INFO | \"Load subreddits df...\"\n",
      "10:05:06 | INFO | \"  0:00:00.683829 <- df_subs loading time elapsed\"\n",
      "10:05:06 | INFO | \"  (19262, 4) <- df_subs shape\"\n",
      "10:05:06 | INFO | \"Vectorizing subreddit descriptions...\"\n",
      "10:05:06 | INFO | \"Getting embeddings in batches of size: 2600\"\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]10:05:18 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[568066,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_45494289]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#############################################| 8/8 [00:26<00:00,  3.33s/it]\n",
      "10:05:33 | INFO | \"  0:00:27.219040 <- df_subs vectorizing time elapsed\"\n",
      "10:05:33 | INFO | \"  Saving to local: df_vect_subreddits_description/df | 19,262 Rows by 514 Cols\"\n",
      "10:05:33 | INFO | \"Converting pandas to dask...\"\n",
      "10:05:33 | INFO | \"    40.1 MB <- Memory usage\"\n",
      "10:05:33 | INFO | \"       2\t<- target Dask partitions\t   30.0 <- target MB partition size\"\n",
      "10:05:34 | INFO | \"  Logging to mlflow...\"\n",
      "10:05:36 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "10:05:36 | INFO | \"-- Loading & vectorizing COMMENTS in files: 27 --\n",
      "Expected batch size: 2600\"\n",
      "10:05:36 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]10:05:36 | INFO | \"Processing: posts/top/2021-09-27/000000000000.parquet\"\n",
      " 56%|#######################4                  | 63/113 [01:18<01:03,  1.27s/it]10:07:07 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[594158,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_45494289]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "100%|#########################################| 113/113 [02:30<00:00,  1.33s/it]\n",
      "10:08:10 | INFO | \"  Saving to local: df_vect_posts/000000000000 | 292,752 Rows by 515 Cols\"\n",
      "  4%|3         | 1/27 [02:43<1:10:52, 163.57s/it]10:08:19 | INFO | \"Processing: posts/top/2021-09-27/000000000001.parquet\"\n",
      "100%|#########################################| 158/158 [03:15<00:00,  1.24s/it]\n",
      "10:11:39 | INFO | \"  Saving to local: df_vect_posts/000000000001 | 409,991 Rows by 515 Cols\"\n",
      "  7%|7         | 2/27 [06:13<1:19:28, 190.76s/it]10:11:49 | INFO | \"Processing: posts/top/2021-09-27/000000000002.parquet\"\n",
      "100%|#########################################| 127/127 [02:36<00:00,  1.23s/it]\n",
      "10:14:30 | INFO | \"  Saving to local: df_vect_posts/000000000002 | 328,363 Rows by 515 Cols\"\n",
      " 11%|#1        | 3/27 [09:03<1:12:34, 181.44s/it]10:14:40 | INFO | \"Processing: posts/top/2021-09-27/000000000003.parquet\"\n",
      "100%|###########################################| 98/98 [02:03<00:00,  1.26s/it]\n",
      "10:16:46 | INFO | \"  Saving to local: df_vect_posts/000000000003 | 254,659 Rows by 515 Cols\"\n",
      " 15%|#4        | 4/27 [11:19<1:02:35, 163.27s/it]10:16:55 | INFO | \"Processing: posts/top/2021-09-27/000000000004.parquet\"\n",
      "100%|#########################################| 106/106 [02:11<00:00,  1.24s/it]\n",
      "10:19:09 | INFO | \"  Saving to local: df_vect_posts/000000000004 | 275,211 Rows by 515 Cols\"\n",
      " 19%|#8        | 5/27 [13:42<57:16, 156.19s/it]  10:19:19 | INFO | \"Processing: posts/top/2021-09-27/000000000005.parquet\"\n",
      "100%|#########################################| 138/138 [02:56<00:00,  1.28s/it]\n",
      "10:22:19 | INFO | \"  Saving to local: df_vect_posts/000000000005 | 357,460 Rows by 515 Cols\"\n",
      " 22%|##2       | 6/27 [16:53<58:46, 167.94s/it]10:22:29 | INFO | \"Processing: posts/top/2021-09-27/000000000006.parquet\"\n",
      "100%|#########################################| 126/126 [02:46<00:00,  1.32s/it]\n",
      "10:25:20 | INFO | \"  Saving to local: df_vect_posts/000000000006 | 326,821 Rows by 515 Cols\"\n",
      " 26%|##5       | 7/27 [19:53<57:18, 171.94s/it]10:25:30 | INFO | \"Processing: posts/top/2021-09-27/000000000007.parquet\"\n",
      "100%|#########################################| 107/107 [02:21<00:00,  1.32s/it]\n",
      "10:27:54 | INFO | \"  Saving to local: df_vect_posts/000000000007 | 277,995 Rows by 515 Cols\"\n",
      " 30%|##9       | 8/27 [22:27<52:39, 166.31s/it]10:28:04 | INFO | \"Processing: posts/top/2021-09-27/000000000008.parquet\"\n",
      "100%|#########################################| 142/142 [03:12<00:00,  1.35s/it]\n",
      "10:31:20 | INFO | \"  Saving to local: df_vect_posts/000000000008 | 368,185 Rows by 515 Cols\"\n",
      " 33%|###3      | 9/27 [25:54<53:40, 178.90s/it]10:31:30 | INFO | \"Processing: posts/top/2021-09-27/000000000009.parquet\"\n",
      "100%|#########################################| 115/115 [02:36<00:00,  1.36s/it]\n",
      "10:34:10 | INFO | \"  Saving to local: df_vect_posts/000000000009 | 297,827 Rows by 515 Cols\"\n",
      " 37%|###7      | 10/27 [28:43<49:51, 175.98s/it]10:34:20 | INFO | \"Processing: posts/top/2021-09-27/000000000010.parquet\"\n",
      "100%|#########################################| 122/122 [02:44<00:00,  1.35s/it]\n",
      "10:37:09 | INFO | \"  Saving to local: df_vect_posts/000000000010 | 316,831 Rows by 515 Cols\"\n",
      " 41%|####      | 11/27 [31:43<47:10, 176.92s/it]10:37:19 | INFO | \"Processing: posts/top/2021-09-27/000000000011.parquet\"\n",
      "100%|#########################################| 117/117 [02:44<00:00,  1.40s/it]\n",
      "10:40:07 | INFO | \"  Saving to local: df_vect_posts/000000000011 | 303,271 Rows by 515 Cols\"\n",
      " 44%|####4     | 12/27 [34:40<44:18, 177.23s/it]10:40:17 | INFO | \"Processing: posts/top/2021-09-27/000000000012.parquet\"\n",
      "100%|#########################################| 113/113 [02:28<00:00,  1.31s/it]\n",
      "10:42:49 | INFO | \"  Saving to local: df_vect_posts/000000000012 | 291,892 Rows by 515 Cols\"\n",
      " 48%|####8     | 13/27 [37:23<40:18, 172.74s/it]10:42:59 | INFO | \"Processing: posts/top/2021-09-27/000000000013.parquet\"\n",
      "100%|#########################################| 117/117 [02:34<00:00,  1.32s/it]\n",
      "10:45:38 | INFO | \"  Saving to local: df_vect_posts/000000000013 | 302,591 Rows by 515 Cols\"\n",
      " 52%|#####1    | 14/27 [40:11<37:08, 171.42s/it]10:45:48 | INFO | \"Processing: posts/top/2021-09-27/000000000014.parquet\"\n",
      "100%|#########################################| 125/125 [02:53<00:00,  1.39s/it]\n",
      "10:48:45 | INFO | \"  Saving to local: df_vect_posts/000000000014 | 322,877 Rows by 515 Cols\"\n",
      " 56%|#####5    | 15/27 [43:19<35:15, 176.26s/it]10:48:55 | INFO | \"Processing: posts/top/2021-09-27/000000000015.parquet\"\n",
      "100%|#########################################| 124/124 [02:50<00:00,  1.38s/it]\n",
      "10:51:50 | INFO | \"  Saving to local: df_vect_posts/000000000015 | 322,124 Rows by 515 Cols\"\n",
      " 59%|#####9    | 16/27 [46:24<32:48, 178.91s/it]10:52:00 | INFO | \"Processing: posts/top/2021-09-27/000000000016.parquet\"\n",
      "100%|#########################################| 120/120 [02:41<00:00,  1.35s/it]\n",
      "10:54:46 | INFO | \"  Saving to local: df_vect_posts/000000000016 | 309,522 Rows by 515 Cols\"\n",
      " 63%|######2   | 17/27 [49:19<29:38, 177.87s/it]10:54:56 | INFO | \"Processing: posts/top/2021-09-27/000000000017.parquet\"\n",
      "100%|#########################################| 122/122 [02:50<00:00,  1.39s/it]\n",
      "10:57:50 | INFO | \"  Saving to local: df_vect_posts/000000000017 | 315,380 Rows by 515 Cols\"\n",
      " 67%|######6   | 18/27 [52:23<26:57, 179.74s/it]10:58:00 | INFO | \"Processing: posts/top/2021-09-27/000000000018.parquet\"\n",
      "100%|#########################################| 116/116 [02:43<00:00,  1.41s/it]\n",
      "11:00:47 | INFO | \"  Saving to local: df_vect_posts/000000000018 | 300,659 Rows by 515 Cols\"\n",
      " 70%|#######   | 19/27 [55:21<23:52, 179.02s/it]11:00:57 | INFO | \"Processing: posts/top/2021-09-27/000000000019.parquet\"\n",
      "100%|###########################################| 96/96 [02:13<00:00,  1.39s/it]\n",
      "11:03:14 | INFO | \"  Saving to local: df_vect_posts/000000000019 | 248,516 Rows by 515 Cols\"\n",
      " 74%|#######4  | 20/27 [57:46<19:43, 169.01s/it]11:03:23 | INFO | \"Processing: posts/top/2021-09-27/000000000020.parquet\"\n",
      "100%|#########################################| 115/115 [02:39<00:00,  1.38s/it]\n",
      "11:06:05 | INFO | \"  Saving to local: df_vect_posts/000000000020 | 298,379 Rows by 515 Cols\"\n",
      " 78%|#######7  | 21/27 [1:00:39<17:00, 170.05s/it]11:06:15 | INFO | \"Processing: posts/top/2021-09-27/000000000021.parquet\"\n",
      "100%|#########################################| 129/129 [02:58<00:00,  1.39s/it]\n",
      "11:09:18 | INFO | \"  Saving to local: df_vect_posts/000000000021 | 334,647 Rows by 515 Cols\"\n",
      " 81%|########1 | 22/27 [1:03:51<14:43, 176.79s/it]11:09:28 | INFO | \"Processing: posts/top/2021-09-27/000000000022.parquet\"\n",
      "100%|#########################################| 127/127 [02:53<00:00,  1.36s/it]\n",
      "11:12:37 | INFO | \"  Saving to local: df_vect_posts/000000000022 | 329,376 Rows by 515 Cols\"\n",
      " 85%|########5 | 23/27 [1:07:10<12:13, 183.40s/it]11:12:46 | INFO | \"Processing: posts/top/2021-09-27/000000000023.parquet\"\n",
      " 42%|#################4                        | 52/125 [01:11<01:38,  1.35s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_batch_fxn-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=subreddits_path,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=posts_path,\n",
    "\n",
    "    # Hack: Rename cols so that I can process posts as a batch of comments\n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts',\n",
    "    \n",
    "    tf_batch_inference_rows=2600,\n",
    "    tf_limit_first_n_chars=850,\n",
    "    \n",
    "    n_sample_comment_files=None,\n",
    "    \n",
    "#     n_sample_posts=9500,\n",
    "#     n_sample_comments=19100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf987e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd12d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b41af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5807c3",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8524464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
