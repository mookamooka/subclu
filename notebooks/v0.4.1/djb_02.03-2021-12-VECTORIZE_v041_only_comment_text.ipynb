{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9e89ae",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "We have a new batch of COMMENTS for v0.4.1 that we need to vectorize.\n",
    "\n",
    "Diff from before: Instead of trying to process all comments in a single job, split up into 3 batches so that it's eaiser to recover in case any of the jobs fails.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook runs the `vectorize_text_to_embeddings` function to:\n",
    "- loading USE-multilingual model\n",
    "- load post & comment text\n",
    "- convert the text into embeddings (at post or level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6bf61",
   "metadata": {},
   "source": [
    "# Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66287fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c413f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\t\tv 3.7.10\n",
      "===\n",
      "mlflow\t\tv: 1.16.0\n",
      "numpy\t\tv: 1.18.5\n",
      "mlflow\t\tv: 1.16.0\n",
      "pandas\t\tv: 1.2.5\n",
      "tensorflow_text\tv: 2.3.0\n",
      "tensorflow\tv: 2.3.3\n",
      "subclu\t\tv: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gc\n",
    "# from functools import partial\n",
    "# import os\n",
    "import logging\n",
    "# from pathlib import Path\n",
    "# from pprint import pprint\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# TF libraries... I've been getting errors when these aren't loaded\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "\n",
    "import subclu\n",
    "from subclu.utils.hydra_config_loader import LoadHydraConfig\n",
    "from subclu.models.vectorize_text import (\n",
    "    vectorize_text_to_embeddings,\n",
    ")\n",
    "from subclu.models import vectorize_text_tf\n",
    "\n",
    "from subclu.utils import set_working_directory\n",
    "from subclu.utils.mlflow_logger import MlflowLogger\n",
    "from subclu.utils.eda import (\n",
    "    setup_logging, counts_describe, value_counts_and_pcts,\n",
    "    notebook_display_config, print_lib_versions,\n",
    "    style_df_numeric\n",
    ")\n",
    "\n",
    "\n",
    "print_lib_versions([mlflow, np, mlflow, pd, tensorflow_text, tf, subclu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2168c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('default')\n",
    "\n",
    "setup_logging()\n",
    "notebook_display_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c1e86",
   "metadata": {},
   "source": [
    "# Initialize mlflow logging with sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889dfcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use new class to initialize mlflow\n",
    "mlf = MlflowLogger(tracking_uri='sqlite')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc613d5",
   "metadata": {},
   "source": [
    "## Get list of experiments with new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e3534e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artifact_location</th>\n",
       "      <th>lifecycle_stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>v0.4.0_use_multi_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/15</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>v0.4.0_use_multi_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/16</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>v0.4.0_use_multi_clustering_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/17</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>v0.4.0_use_multi_clustering</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/18</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>v0.4.1_mUSE_inference_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/19</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>v0.4.1_mUSE_inference</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/20</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>v0.4.1_mUSE_aggregates_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/21</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>v0.4.1_mUSE_aggregates</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/22</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>v0.4.1_mUSE_clustering_test</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/23</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>v0.4.1_mUSE_clustering</td>\n",
       "      <td>gs://i18n-subreddit-clustering/mlflow/mlruns/24</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_id                              name                                artifact_location lifecycle_stage\n",
       "15            15  v0.4.0_use_multi_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/15          active\n",
       "16            16       v0.4.0_use_multi_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/16          active\n",
       "17            17  v0.4.0_use_multi_clustering_test  gs://i18n-subreddit-clustering/mlflow/mlruns/17          active\n",
       "18            18       v0.4.0_use_multi_clustering  gs://i18n-subreddit-clustering/mlflow/mlruns/18          active\n",
       "19            19        v0.4.1_mUSE_inference_test  gs://i18n-subreddit-clustering/mlflow/mlruns/19          active\n",
       "20            20             v0.4.1_mUSE_inference  gs://i18n-subreddit-clustering/mlflow/mlruns/20          active\n",
       "21            21       v0.4.1_mUSE_aggregates_test  gs://i18n-subreddit-clustering/mlflow/mlruns/21          active\n",
       "22            22            v0.4.1_mUSE_aggregates  gs://i18n-subreddit-clustering/mlflow/mlruns/22          active\n",
       "23            23       v0.4.1_mUSE_clustering_test  gs://i18n-subreddit-clustering/mlflow/mlruns/23          active\n",
       "24            24            v0.4.1_mUSE_clustering  gs://i18n-subreddit-clustering/mlflow/mlruns/24          active"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlf.list_experiment_meta(output_format='pandas').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a7f85",
   "metadata": {},
   "source": [
    "# Check whether we have access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6ed8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built with CUDA? True\n",
      "GPUs\n",
      "===\n",
      "Num GPUs Available: 1\n",
      "GPU details:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "l_phys_gpus = tf.config.list_physical_devices('GPU')\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\n",
    "    f\"\\nBuilt with CUDA? {tf.test.is_built_with_cuda()}\"\n",
    "    f\"\\nGPUs\\n===\"\n",
    "    f\"\\nNum GPUs Available: {len(l_phys_gpus)}\"\n",
    "    f\"\\nGPU details:\\n{l_phys_gpus}\"\n",
    "#     f\"\\n\\nAll devices:\\n===\\n\"\n",
    "#     f\"{device_lib.list_local_devices()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536b5a7",
   "metadata": {},
   "source": [
    "# Load config with data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7acdeca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'v0.4.1 inputs - 50k subreddits - Active Subreddits (no Geo) + Geo-relevant users_l28>=100 & posts_l28>=4',\n",
       " 'bucket_name': 'i18n-subreddit-clustering',\n",
       " 'folder_subreddits_text_and_meta': 'subreddits/top/2021-12-14',\n",
       " 'folder_posts_text_and_meta': 'posts/top/2021-12-14',\n",
       " 'folder_comments_text_and_meta': 'comments/top/2021-12-14',\n",
       " 'folder_subreddits_text_and_meta_filter': 'subreddits/top/2021-09-24'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data_v041 = LoadHydraConfig(\n",
    "    config_path=\"../config/data_text_and_metadata\",\n",
    "    config_name='v0.4.1_2021_12',\n",
    ")\n",
    "config_data_v041.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adcc4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_experiment_test = 'v0.4.1_mUSE_inference_test'\n",
    "mlflow_experiment_full = 'v0.4.1_mUSE_inference'\n",
    "\n",
    "# Add or over-ride configs values\n",
    "bucket_name = config_data_v041.config_dict['bucket_name']\n",
    "subreddits_path = None  # config_data_v041.config_dict['folder_subreddits_text_and_meta']\n",
    "posts_path = None  # config_data_v041.config_dict['folder_posts_text_and_meta']\n",
    "\n",
    "comments_path = config_data_v041.config_dict['folder_comments_text_and_meta']\n",
    "\n",
    "# subreddits_path_exclude = config_data_v041.config_dict['folder_subreddits_text_and_meta_filter']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef17ea9",
   "metadata": {},
   "source": [
    "### list total number of comment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef48c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_full_path = f\"gs://{bucket_name}/{comments_path}\"\n",
    "# !gsutil ls $comments_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e222e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "# check list of files\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "l_comment_files_to_process = list(bucket.list_blobs(prefix=comments_path))\n",
    "total_comms_file_count = len(l_comment_files_to_process)\n",
    "print(total_comms_file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15592dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 19), (19, 38), (38, 57), (57, 76)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_subsets = 4\n",
    "slice_base = math.ceil(total_comms_file_count / 4)\n",
    "slice_indeces = list()\n",
    "\n",
    "for i in range(4):\n",
    "    slice_indeces.append((i * slice_base, (i + 1) * slice_base))\n",
    "\n",
    "slice_indeces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a1ce9",
   "metadata": {},
   "source": [
    "# Test with batching function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b410a0",
   "metadata": {},
   "source": [
    "### New function (with batching)\n",
    "Most inputs will be the same.\n",
    "However, some things will change:\n",
    "- Added new parameter to sample only first N files (we'll process each file individually)\n",
    "\n",
    "For subreddit only, we can expand to more than 1,500 characters.\n",
    "\n",
    "HOWEVER - when scoring posts &/or comments, we're better off trimming to first ~1,000 characters to speed things up. We can increase the character len if results aren't great... this could be a hyperparameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c9462",
   "metadata": {},
   "source": [
    "### Notes on batch & characters:\n",
    "\n",
    "Comments tend to be shorter, so we can usually run larger batches. A batch of `6,000` can still result in `OOM` errors, so go lower than that.\n",
    "```python\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=5000,\n",
    "    tf_limit_first_n_chars=900,\n",
    "```\n",
    "\n",
    "Posts tend to be longer, so we're better off running smaller batches:\n",
    "```python\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=2400,\n",
    "    tf_limit_first_n_chars=900,\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6838e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:06:41 | INFO | \"Start vectorize function\"\n",
      "07:06:41 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-21_070641\"\n",
      "07:06:41 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "07:06:42 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "07:06:42 | INFO | \"  Saving config to local path...\"\n",
      "07:06:42 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "07:06:43 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "07:06:43 | INFO | \"Loading model use_multilingual...\"\n",
      "07:06:45 | INFO | \"  0:00:02.223100 <- Load TF HUB model time elapsed\"\n",
      "07:06:45 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "07:06:45 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "07:06:45 | INFO | \"-- Loading & vectorizing COMMENTS in files: 2 --\n",
      "Expected batch size: 6000\"\n",
      "07:06:45 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "07:06:45 | INFO | \"Processing: comments/top/2021-12-14/000000000000.parquet\"\n",
      "07:06:49 | INFO | \"  Sampling COMMENTS down to: 209,100     Samples PER FILE: 104,551\"\n",
      "07:06:49 | INFO | \"  (104551, 6) <- df_comments.shape AFTER sampling\"\n",
      "07:06:49 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id', 'comment_id']\"\n",
      "07:06:49 | INFO | \"col_text: comment_body_text\"\n",
      "07:06:49 | INFO | \"lowercase_text: False\"\n",
      "07:06:49 | INFO | \"limit_first_n_chars: 900\"\n",
      "07:06:49 | INFO | \"limit_first_n_chars_retry: 600\"\n",
      "07:06:49 | INFO | \"Getting embeddings in batches of size: 6000\"\n",
      "07:07:01 | INFO | \"progress:  61%|####################1            | 11/18 [00:11<00:07,  1.09s/it]\"\n",
      "07:07:08 | INFO | \"progress: 100%|#################################| 18/18 [00:18<00:00,  1.04s/it]\"\n",
      "\n",
      "07:07:08 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 104,551 Rows by 516 Cols\"\n",
      "07:07:12 | INFO | \"progress:  50%|##########          | 1/2 [00:26<00:26, 26.76s/it]\"\n",
      "07:07:12 | INFO | \"Processing: comments/top/2021-12-14/000000000001.parquet\"\n",
      "07:07:16 | INFO | \"  Sampling COMMENTS down to: 209,100     Samples PER FILE: 104,551\"\n",
      "07:07:16 | INFO | \"  (104551, 6) <- df_comments.shape AFTER sampling\"\n",
      "07:07:16 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id', 'comment_id']\"\n",
      "07:07:16 | INFO | \"col_text: comment_body_text\"\n",
      "07:07:16 | INFO | \"lowercase_text: False\"\n",
      "07:07:16 | INFO | \"limit_first_n_chars: 900\"\n",
      "07:07:16 | INFO | \"limit_first_n_chars_retry: 600\"\n",
      "07:07:17 | INFO | \"Getting embeddings in batches of size: 6000\"\n",
      "07:07:28 | INFO | \"progress:  56%|##################3              | 10/18 [00:11<00:08,  1.11s/it]\"\n",
      "07:07:36 | INFO | \"progress: 100%|#################################| 18/18 [00:19<00:00,  1.09s/it]\"\n",
      "\n",
      "07:07:37 | INFO | \"  Saving to local: df_vect_comments/000000000001 | 104,551 Rows by 516 Cols\"\n",
      "07:07:40 | INFO | \"progress: 100%|####################| 2/2 [00:55<00:00, 27.84s/it]\"\n",
      "07:07:40 | INFO | \"progress: 100%|####################| 2/2 [00:55<00:00, 27.68s/it]\"\n",
      "\n",
      "07:07:41 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "07:07:45 | INFO | \"  0:01:03.601440 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"posts_as_comments_full_text-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=6000,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    # Sampling/batching files or rows\n",
    "    n_sample_comment_files=2,\n",
    "    n_sample_comments=209100,\n",
    "    # n_sample_posts=9500,\n",
    "    get_embeddings_verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e86c12",
   "metadata": {},
   "source": [
    "### Test on a slice of files\n",
    "\n",
    "The previous `n_sample_comment_files` would always sample the first N files, but we didn't check whether file list was sorted.\n",
    "\n",
    "```\n",
    "# # TODO(djb): blobs can't be sorted, but I can sort and check the file name, so use that instead.\n",
    "```\n",
    "With new refactoring: \n",
    "- I sort list to ensure consistency on each run \n",
    "- add slice start & end parameters to pick arbitrary files in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d10cad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:42:09 | INFO | \"Start vectorize function\"\n",
      "07:42:09 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-21_074209\"\n",
      "07:42:10 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "07:42:10 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "07:42:10 | INFO | \"  Saving config to local path...\"\n",
      "07:42:10 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "07:42:11 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "07:42:11 | INFO | \"Loading model use_multilingual...\"\n",
      "07:42:13 | INFO | \"  0:00:02.271232 <- Load TF HUB model time elapsed\"\n",
      "07:42:13 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "07:42:13 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "07:42:13 | INFO | \"-- Loading & vectorizing COMMENTS in files: 2 --\n",
      "Expected batch size: 6000\"\n",
      "07:42:13 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function recreate_function.<locals>.restored_function_body at 0x7f696480a4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:42:14 | INFO | \"Processing: comments/top/2021-12-14/000000000002.parquet\"\n",
      "07:42:17 | INFO | \"  Sampling COMMENTS down to: 39,100     Samples PER FILE: 19,551\"\n",
      "07:42:17 | INFO | \"  (19551, 6) <- df_comments.shape AFTER sampling\"\n",
      "07:42:17 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id', 'comment_id']\"\n",
      "07:42:17 | INFO | \"col_text: comment_body_text\"\n",
      "07:42:17 | INFO | \"lowercase_text: False\"\n",
      "07:42:17 | INFO | \"limit_first_n_chars: 900\"\n",
      "07:42:17 | INFO | \"limit_first_n_chars_retry: 600\"\n",
      "07:42:17 | INFO | \"Getting embeddings in batches of size: 6000\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function recreate_function.<locals>.restored_function_body at 0x7f62353328c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:42:22 | INFO | \"progress: 100%|###################################| 4/4 [00:04<00:00,  1.25s/it]\"\n",
      "\n",
      "07:42:22 | INFO | \"  Saving to local: df_vect_comments/000000000002 | 19,551 Rows by 516 Cols\"\n",
      "07:42:24 | INFO | \"Processing: comments/top/2021-12-14/000000000003.parquet\"\n",
      "07:42:26 | INFO | \"  Sampling COMMENTS down to: 39,100     Samples PER FILE: 19,551\"\n",
      "07:42:26 | INFO | \"  (19551, 6) <- df_comments.shape AFTER sampling\"\n",
      "07:42:26 | INFO | \"cols_index: ['subreddit_name', 'subreddit_id', 'post_id', 'comment_id']\"\n",
      "07:42:26 | INFO | \"col_text: comment_body_text\"\n",
      "07:42:26 | INFO | \"lowercase_text: False\"\n",
      "07:42:26 | INFO | \"limit_first_n_chars: 900\"\n",
      "07:42:26 | INFO | \"limit_first_n_chars_retry: 600\"\n",
      "07:42:27 | INFO | \"Getting embeddings in batches of size: 6000\"\n",
      "07:42:31 | INFO | \"progress: 100%|###################################| 4/4 [00:04<00:00,  1.01s/it]\"\n",
      "\n",
      "07:42:31 | INFO | \"  Saving to local: df_vect_comments/000000000003 | 19,551 Rows by 516 Cols\"\n",
      "07:42:32 | INFO | \"progress: 100%|####################| 2/2 [00:18<00:00,  9.22s/it]\"\n",
      "\n",
      "07:42:32 | INFO | \"Logging COMMENT files as mlflow artifact (to GCS)...\"\n",
      "07:42:33 | INFO | \"  0:00:24.020200 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"comments_slice-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_test,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    batch_comment_files=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=6000,\n",
    "    tf_limit_first_n_chars=900,\n",
    "\n",
    "    # slicing FILES \n",
    "    n_comment_files_slice_start=2,\n",
    "    n_comment_files_slice_end=4,\n",
    "\n",
    "    # Sampling FILES \n",
    "    #   NOTE: DON'T USE n_sample_*_files and slices! they may create unexpected results!\n",
    "    # n_sample_comment_files=2, \n",
    "     \n",
    "    # Sampling rows\n",
    "    n_sample_comments=89100,\n",
    "    # n_sample_posts=9500,\n",
    "    get_embeddings_verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9349981",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f4661",
   "metadata": {},
   "source": [
    "# Run all in slices + using new batching logic\n",
    "\n",
    "By splitting into 4 jobs (using slices), we decrease the chance of a single job failing and ruining all the jobs. Which should make it easier to re-run a single job if it fails.\n",
    "\n",
    "### impact of batch size\n",
    "by increasing batch size from 3,300 to 3,900 we reduce processing time per file by 15 seconds.\n",
    "\n",
    "```bash\n",
    "[07:56:14 | INFO | \"Processing: comments/top/2021-12-14/000000000000.parquet\"]\n",
    "07:50:24 | INFO | \"Getting embeddings in batches of size: 3300\"\n",
    "07:50:35 | INFO | \"progress:   5%|#6                              | 13/245 [00:11<03:22,  1.15it/s]\"\n",
    "07:50:46 | INFO | \"progress:  12%|###7                            | 29/245 [00:22<02:43,  1.32it/s]\"\n",
    "07:50:58 | INFO | \"progress:  18%|#####8                          | 45/245 [00:34<02:30,  1.33it/s]\"\n",
    "07:51:09 | INFO | \"progress:  24%|#######8                        | 60/245 [00:45<02:18,  1.34it/s]\"\n",
    "07:51:22 | INFO | \"progress:  31%|#########7                      | 75/245 [00:58<02:14,  1.26it/s]\"\n",
    "07:51:34 | INFO | \"progress:  36%|###########4                    | 88/245 [01:09<02:07,  1.23it/s]\"\n",
    "07:51:45 | INFO | \"progress:  36%|###########4                    | 88/245 [01:21<02:07,  1.23it/s]\"\n",
    "07:51:45 | INFO | \"progress:  42%|#############                  | 103/245 [01:21<01:52,  1.26it/s]\"\n",
    "07:51:55 | INFO | \"progress:  42%|#############                  | 103/245 [01:31<01:52,  1.26it/s]\"\n",
    "07:51:56 | INFO | \"progress:  48%|##############9                | 118/245 [01:32<01:38,  1.28it/s]\"\n",
    "07:52:08 | INFO | \"progress:  54%|################8              | 133/245 [01:44<01:27,  1.28it/s]\"\n",
    "07:52:19 | INFO | \"progress:  60%|##################5            | 147/245 [01:55<01:17,  1.27it/s]\"\n",
    "07:52:31 | INFO | \"progress:  66%|####################3          | 161/245 [02:06<01:07,  1.25it/s]\"\n",
    "07:52:44 | INFO | \"progress:  71%|######################1        | 175/245 [02:19<00:58,  1.20it/s]\"\n",
    "07:52:55 | INFO | \"progress:  77%|#######################9       | 189/245 [02:31<00:46,  1.20it/s]\"\n",
    "07:53:06 | INFO | \"progress:  77%|#######################9       | 189/245 [02:41<00:46,  1.20it/s]\"\n",
    "07:53:06 | INFO | \"progress:  82%|#########################5     | 202/245 [02:42<00:36,  1.19it/s]\"\n",
    "07:53:18 | INFO | \"progress:  88%|###########################3   | 216/245 [02:54<00:24,  1.19it/s]\"\n",
    "07:53:30 | INFO | \"progress:  94%|#############################1 | 230/245 [03:05<00:12,  1.20it/s]\"\n",
    "07:53:41 | INFO | \"progress: 100%|##############################8| 244/245 [03:16<00:00,  1.21it/s]\"\n",
    "07:53:41 | INFO | \"progress: 100%|###############################| 245/245 [03:17<00:00,  1.24it/s]\"\n",
    "\n",
    "\n",
    "# higher batch size = faster\n",
    "07:56:14 | INFO | \"Processing: comments/top/2021-12-14/000000000000.parquet\"\n",
    "07:56:17 | INFO | \"Getting embeddings in batches of size: 3900\"\n",
    "07:56:29 | INFO | \"progress:   6%|#8                              | 12/207 [00:11<03:04,  1.06it/s]\"\n",
    "07:56:40 | INFO | \"progress:  13%|####1                           | 27/207 [00:22<02:29,  1.20it/s]\"\n",
    "07:56:52 | INFO | \"progress:  20%|######4                         | 42/207 [00:35<02:15,  1.21it/s]\"\n",
    "07:57:04 | INFO | \"progress:  27%|########6                       | 56/207 [00:46<02:04,  1.21it/s]\"\n",
    "07:57:17 | INFO | \"progress:  34%|##########8                     | 70/207 [00:59<01:58,  1.16it/s]\"\n",
    "07:57:28 | INFO | \"progress:  34%|##########8                     | 70/207 [01:10<01:58,  1.16it/s]\"\n",
    "07:57:28 | INFO | \"progress:  40%|############8                   | 83/207 [01:11<01:47,  1.15it/s]\"\n",
    "07:57:40 | INFO | \"progress:  47%|##############9                 | 97/207 [01:22<01:34,  1.17it/s]\"\n",
    "07:57:52 | INFO | \"progress:  54%|################6              | 111/207 [01:34<01:22,  1.17it/s]\"\n",
    "07:58:04 | INFO | \"progress:  60%|##################5            | 124/207 [01:46<01:11,  1.16it/s]\"\n",
    "07:58:15 | INFO | \"progress:  66%|####################5          | 137/207 [01:58<01:01,  1.14it/s]\"\n",
    "07:58:28 | INFO | \"progress:  66%|####################5          | 137/207 [02:11<01:01,  1.14it/s]\"\n",
    "07:58:28 | INFO | \"progress:  72%|######################4        | 150/207 [02:11<00:52,  1.10it/s]\"\n",
    "07:58:38 | INFO | \"progress:  72%|######################4        | 150/207 [02:21<00:52,  1.10it/s]\"\n",
    "07:58:40 | INFO | \"progress:  78%|########################2      | 162/207 [02:22<00:41,  1.08it/s]\"\n",
    "07:58:52 | INFO | \"progress:  85%|##########################2    | 175/207 [02:34<00:29,  1.09it/s]\"\n",
    "07:59:04 | INFO | \"progress:  91%|############################1  | 188/207 [02:46<00:17,  1.08it/s]\"\n",
    "07:59:15 | INFO | \"progress:  97%|##############################1| 201/207 [02:58<00:05,  1.09it/s]\"\n",
    "07:59:21 | INFO | \"progress: 100%|###############################| 207/207 [03:03<00:00,  1.13it/s]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df0b2564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 19), (19, 38), (38, 57), (57, 76)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19f9c83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 38), (38, 57), (57, 76)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_indeces[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57c89a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 19): 0, (19, 38): 1, (38, 57): 2, (57, 76): 3}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_slice_nums = {s_: i for i, s_ in enumerate(slice_indeces)}\n",
    "d_slice_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a1f09",
   "metadata": {},
   "source": [
    "## Re-run comments and log to \"full\" (non-test) mlflow experiment\n",
    "\n",
    "batch of 3,800 is too large - 3 of 4 jobs failed, reduce down to 3,200 or lower b/c it's better to take a while and save than keep getting stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0ef2186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2470"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(3800 * .65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb9db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:08:27 | INFO | \"Start vectorize function\"\n",
      "10:08:27 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-12-21_100827\"\n",
      "10:08:28 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "10:08:28 | INFO | \"host_name: djb-subclu-inference-tf-2-3-20210630\"\n",
      "10:08:28 | INFO | \"  Saving config to local path...\"\n",
      "10:08:28 | INFO | \"  Logging config to mlflow with joblib...\"\n",
      "10:08:29 | INFO | \"  Logging config to mlflow with YAML...\"\n",
      "10:08:29 | INFO | \"Loading model use_multilingual...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 513 calls to <function recreate_function.<locals>.restored_function_body at 0x7f629e52b050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:08:31 | INFO | \"  0:00:02.250651 <- Load TF HUB model time elapsed\"\n",
      "10:08:31 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "10:08:31 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "10:08:31 | INFO | \"-- Loading & vectorizing COMMENTS in files: 19 --\n",
      "Expected batch size: 3600\"\n",
      "10:08:31 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 514 calls to <function recreate_function.<locals>.restored_function_body at 0x7f6396737d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:08:32 | INFO | \"Processing: comments/top/2021-12-14/000000000019.parquet\"\n",
      "10:08:36 | INFO | \"Getting embeddings in batches of size: 3600\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 515 calls to <function recreate_function.<locals>.restored_function_body at 0x7f629d1994d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:08:47 | INFO | \"progress:   5%|#7                              | 12/221 [00:11<03:14,  1.07it/s]\"\n",
      "10:08:59 | INFO | \"progress:  11%|###6                            | 25/221 [00:22<02:57,  1.11it/s]\"\n",
      "10:09:11 | INFO | \"progress:  17%|#####5                          | 38/221 [00:34<02:48,  1.09it/s]\"\n",
      "10:09:22 | INFO | \"progress:  23%|#######3                        | 51/221 [00:46<02:33,  1.11it/s]\"\n",
      "10:09:33 | INFO | \"progress:  23%|#######3                        | 51/221 [00:57<02:33,  1.11it/s]\"\n",
      "10:09:34 | INFO | \"progress:  29%|#########4                      | 65/221 [00:57<02:15,  1.15it/s]\"\n",
      "10:09:45 | INFO | \"progress:  36%|###########4                    | 79/221 [01:08<01:59,  1.18it/s]\"\n",
      "10:09:58 | INFO | \"progress:  42%|#############4                  | 93/221 [01:21<01:50,  1.16it/s]\"\n",
      "10:10:09 | INFO | \"progress:  48%|###############                | 107/221 [01:33<01:37,  1.17it/s]\"\n",
      "10:10:24 | INFO | \"progress:  48%|###############                | 107/221 [01:47<01:37,  1.17it/s]\"\n",
      "10:10:24 | WARNING | \"\n",
      "ResourceExhausted, lowering character limit\n",
      " OOM when allocating tensor with shape[622949,1280] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/EncoderDNN/CNN_layers/ngram_order_5/Ngram-5-Conv/concat_2}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_restored_function_body_273574]\n",
      "\n",
      "Function call stack:\n",
      "restored_function_body\n",
      "\n",
      "\"\n",
      "10:10:26 | INFO | \"progress:  51%|###############7               | 112/221 [01:49<02:11,  1.20s/it]\"\n",
      "10:10:38 | INFO | \"progress:  57%|#################6             | 126/221 [02:01<01:42,  1.08s/it]\"\n",
      "10:10:50 | INFO | \"progress:  63%|###################4           | 139/221 [02:13<01:23,  1.02s/it]\"\n",
      "10:11:01 | INFO | \"progress:  69%|#####################4         | 153/221 [02:25<01:05,  1.04it/s]\"\n",
      "10:11:14 | INFO | \"progress:  76%|#######################4       | 167/221 [02:37<00:50,  1.07it/s]\"\n",
      "10:11:24 | INFO | \"progress:  76%|#######################4       | 167/221 [02:48<00:50,  1.07it/s]\"\n",
      "10:11:25 | INFO | \"progress:  82%|#########################3     | 181/221 [02:49<00:35,  1.11it/s]\"\n",
      "10:11:37 | INFO | \"progress:  89%|###########################4   | 196/221 [03:00<00:21,  1.17it/s]\"\n",
      "10:11:49 | INFO | \"progress:  95%|#############################5 | 211/221 [03:13<00:08,  1.17it/s]\"\n",
      "10:11:58 | INFO | \"progress: 100%|###############################| 221/221 [03:21<00:00,  1.09it/s]\"\n",
      "\n",
      "10:12:02 | INFO | \"  Saving to local: df_vect_comments/000000000019 | 792,281 Rows by 516 Cols\"\n",
      "10:12:14 | INFO | \"progress:   5%|8               | 1/19 [03:43<1:06:54, 223.01s/it]\"\n",
      "10:12:14 | INFO | \"Processing: comments/top/2021-12-14/000000000020.parquet\"\n",
      "10:12:18 | INFO | \"Getting embeddings in batches of size: 3600\"\n",
      "10:12:29 | INFO | \"progress:   7%|##1                             | 13/191 [00:11<02:33,  1.16it/s]\"\n",
      "10:12:43 | INFO | \"progress:  14%|####3                           | 26/191 [00:24<02:39,  1.03it/s]\"\n",
      "10:12:54 | INFO | \"progress:  19%|######1                         | 37/191 [00:36<02:34,  1.00s/it]\"\n",
      "10:13:05 | INFO | \"progress:  19%|######1                         | 37/191 [00:47<02:34,  1.00s/it]\"\n",
      "10:13:06 | INFO | \"progress:  25%|########                        | 48/191 [00:47<02:24,  1.01s/it]\"\n",
      "10:13:17 | INFO | \"progress:  31%|##########                      | 60/191 [00:58<02:09,  1.01it/s]\"\n",
      "10:13:29 | INFO | \"progress:  39%|############3                   | 74/191 [01:10<01:48,  1.07it/s]\"\n",
      "10:13:40 | INFO | \"progress:  46%|##############7                 | 88/191 [01:22<01:32,  1.12it/s]\"\n",
      "10:13:54 | INFO | \"progress:  53%|################5              | 102/191 [01:35<01:21,  1.09it/s]\"\n",
      "10:14:05 | INFO | \"progress:  61%|##################9            | 117/191 [01:47<01:04,  1.15it/s]\"\n",
      "10:14:16 | INFO | \"progress:  61%|##################9            | 117/191 [01:57<01:04,  1.15it/s]\"\n",
      "10:14:16 | INFO | \"progress:  68%|#####################          | 130/191 [01:58<00:52,  1.16it/s]\"\n",
      "10:14:28 | INFO | \"progress:  75%|#######################2       | 143/191 [02:09<00:41,  1.16it/s]\"\n",
      "10:14:39 | INFO | \"progress:  82%|#########################3     | 156/191 [02:21<00:30,  1.14it/s]\"\n",
      "10:14:51 | INFO | \"progress:  89%|###########################5   | 170/191 [02:33<00:18,  1.16it/s]\"\n",
      "10:15:03 | INFO | \"progress:  96%|#############################8 | 184/191 [02:44<00:05,  1.18it/s]\"\n",
      "10:15:08 | INFO | \"progress: 100%|###############################| 191/191 [02:50<00:00,  1.12it/s]\"\n",
      "\n",
      "10:15:12 | INFO | \"  Saving to local: df_vect_comments/000000000020 | 684,954 Rows by 516 Cols\"\n",
      "10:15:23 | INFO | \"progress:  11%|#8                | 2/19 [06:52<57:33, 203.12s/it]\"\n",
      "10:15:24 | INFO | \"Processing: comments/top/2021-12-14/000000000021.parquet\"\n",
      "10:15:29 | INFO | \"Getting embeddings in batches of size: 3600\"\n",
      "10:15:41 | INFO | \"progress:   7%|##1                             | 14/210 [00:11<02:45,  1.19it/s]\"\n",
      "10:15:53 | INFO | \"progress:  13%|####2                           | 28/210 [00:23<02:35,  1.17it/s]\"\n",
      "10:16:05 | INFO | \"progress:  20%|######2                         | 41/210 [00:35<02:29,  1.13it/s]\"\n",
      "10:16:16 | INFO | \"progress:  26%|########2                       | 54/210 [00:47<02:16,  1.14it/s]\"\n",
      "10:16:27 | INFO | \"progress:  26%|########2                       | 54/210 [00:57<02:16,  1.14it/s]\"\n",
      "10:16:28 | INFO | \"progress:  32%|##########3                     | 68/210 [00:58<02:02,  1.16it/s]\"\n",
      "10:16:40 | INFO | \"progress:  39%|############4                   | 82/210 [01:11<01:51,  1.15it/s]\"\n",
      "10:16:52 | INFO | \"progress:  46%|##############6                 | 96/210 [01:22<01:36,  1.18it/s]\"\n",
      "10:27:19 | INFO | \"progress:  67%|####################8          | 136/202 [01:55<00:53,  1.22it/s]\"\n",
      "11:04:38 | INFO | \"progress:  85%|##########################4    | 168/197 [02:32<00:24,  1.19it/s]\"\n",
      "11:04:39 | INFO | \"progress:  92%|############################6  | 182/197 [02:33<00:12,  1.19it/s]\"\n",
      "11:35:57 | INFO | \"progress:  81%|#########################1     | 147/181 [02:10<00:29,  1.15it/s]\"\n",
      "12:55:53 | INFO | \"progress:  61%|##################9            | 166/271 [02:20<01:30,  1.16it/s]\"\n",
      "13:02:13 | INFO | \"progress:  37%|###########7                    | 67/182 [00:58<01:40,  1.15it/s]\"\n"
     ]
    }
   ],
   "source": [
    "for slice_ in slice_indeces[1:]:\n",
    "    try:\n",
    "        mlflow.end_run(status='KILLED')\n",
    "\n",
    "        vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "            model_name='use_multilingual',\n",
    "            run_name=f\"comments_slice_{d_slice_nums[slice_]}-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "            mlflow_experiment=mlflow_experiment_full,\n",
    "\n",
    "            tokenize_lowercase=False,\n",
    "            batch_comment_files=True,\n",
    "\n",
    "            bucket_name=bucket_name,\n",
    "            subreddits_path=None,\n",
    "            posts_path=None,\n",
    "            comments_path=comments_path,\n",
    "\n",
    "            # TF batches\n",
    "            tf_batch_inference_rows=3600,\n",
    "            tf_limit_first_n_chars=900,\n",
    "\n",
    "            # slicing FILES \n",
    "            n_comment_files_slice_start=slice_[0],\n",
    "            n_comment_files_slice_end=slice_[1],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc69b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6c9fb",
   "metadata": {},
   "source": [
    "# Run full with `lower_case=True`\n",
    "\n",
    "This one is expected to be a little slower because it'll call `.str.lower()` on each batch of text.\n",
    "\n",
    "---\n",
    "\n",
    "TODO: unsure if it's worth running this job in parallel while I do work on a separate VM... might be a big pain to manually sync the rows from metrics & params happening at the same time in two different VMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18979f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:39:08 | INFO | \"Start vectorize function\"\n",
      "06:39:08 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-10-05_063908\"\n",
      "06:39:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/subreddit_clustering_i18n/mlflow_sync/djb-subclu-inference-tf-2-3-20210630/mlruns.db\"\n",
      "06:39:08 | INFO | \"  Saving config to local path...\"\n",
      "06:39:08 | INFO | \"  Logging config to mlflow...\"\n",
      "06:39:09 | INFO | \"Loading model use_multilingual...\"\n",
      "06:39:11 | INFO | \"  0:00:02.147629 <- Load TF HUB model time elapsed\"\n",
      "06:39:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "06:39:11 | INFO | \"** Procesing Comments files one at a time ***\"\n",
      "06:39:11 | INFO | \"-- Loading & vectorizing COMMENTS in files: 59 --\n",
      "Expected batch size: 3200\"\n",
      "06:39:11 | WARNING | \"df_posts missing, so we can't filter comments without a post...\n",
      "local variable 'df_posts' referenced before assignment\"\n",
      "  0%|          | 0/59 [00:00<?, ?it/s]06:39:11 | INFO | \"Processing: comments/top/2021-10-04/000000000000.parquet\"\n",
      "06:39:15 | INFO | \"Getting embeddings in batches of size: 3200\"\n",
      "100%|#########################################| 217/217 [02:44<00:00,  1.32it/s]\n",
      "06:42:03 | INFO | \"  Saving to local: df_vect_comments/000000000000 | 692,592 Rows by 516 Cols\"\n",
      "  0%|          | 0/59 [03:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'count_files_processed' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2125ec3fbd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# TF batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtf_batch_inference_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtf_limit_first_n_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Sampling FILES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david.bermejo/repos/subreddit_clustering_i18n/subclu/models/vectorize_text_tf.py\u001b[0m in \u001b[0;36mvectorize_text_to_embeddings\u001b[0;34m(mlflow_experiment, model_name, run_name, tokenize_lowercase, bucket_name, subreddits_path, posts_path, comments_path, preprocess_text_folder, col_text_post, col_text_post_word_count, col_text_post_url, col_post_id, col_comment_id, col_text_comment, col_text_comment_word_count, cols_index_comment, local_comms_subfolder_relative, mlflow_comments_folder, cols_comment_text_to_concat, col_comment_text_to_concat, col_subreddit_id, col_text_subreddit_description, col_text_subreddit_word_count, tf_batch_inference_rows, tf_limit_first_n_chars, n_sample_post_files, n_sample_comment_files, n_comment_files_slice_start, n_comment_files_slice_end, batch_comment_files, n_sample_posts, n_sample_comments, get_embeddings_verbose)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;31m# Log partial metrics to mlflow so it's easier to know whether a job is still alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;31m#  or dead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                 \u001b[0mcount_files_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_files_processed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 mlflow.log_metrics(\n\u001b[1;32m    414\u001b[0m                     {\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'count_files_processed' referenced before assignment"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "vectorize_text_tf.vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name=f\"comments_lower_case-{datetime.utcnow().strftime('%Y-%m-%d_%H%M%S')}\",\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=True,\n",
    "    \n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,\n",
    "    comments_path=comments_path,\n",
    "\n",
    "    # TF batches\n",
    "    tf_batch_inference_rows=3200,\n",
    "    tf_limit_first_n_chars=900,\n",
    "    \n",
    "    # Sampling FILES\n",
    "    # n_sample_comment_files=15,\n",
    "    # n_comment_files_slice_start=20,\n",
    "    # n_comment_files_slice_end=62,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e6956c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run(status='KILLED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23313933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd09365e",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4200de",
   "metadata": {},
   "source": [
    "### Notes on previous function (all in memory):\n",
    "- 60GB of RAM wasn't good enough for 19Million comments _lol_ (also might've run into memory leaks in the GPU)\n",
    "\n",
    "```\n",
    "...\n",
    "12:02:14 | INFO | \"  (19168154, 6) <- updated df_comments shape\"\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "<__array_function__ internals> in concatenate(*args, **kwargs)\n",
    "\n",
    "MemoryError: Unable to allocate 36.6 GiB for an array with shape (512, 19168154) and data type float32\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae388e",
   "metadata": {},
   "source": [
    "### New batching fxn\n",
    "Besides file-batching, this job increased the row-batches from 2,000 to 6,100... unclear if this is having a negative impact. Maybe smaller batches are somehow more efficient?\n",
    "Now that I'm reading one file at a time, it looks like speed is taking a big hit\n",
    "\n",
    "Baseline when running it all in memory. It took `1:32:26`, but it ran out of memory (RAM).\n",
    "The current ETA is around `2 hours`\n",
    "\n",
    "```\n",
    "# singe file, all in memory (results in OOM)\n",
    "12:02:14 | INFO | \"Vectorizing COMMENTS...\"\n",
    "12:02:14 | INFO | \"Getting embeddings in batches of size: 2100\"\n",
    "100%\n",
    "9128/9128 [1:32:26<00:00, 1.97it/s]\n",
    "\n",
    "\n",
    "# one file at a time... slower, but we get results one file at a time...\n",
    "16%\n",
    "6/37 [21:11<1:49:46, 212.45s/it]\n",
    "```\n",
    "\n",
    "\n",
    "Notes on new fxn to batch posts as if they're comments. (Because batching logic is only implemented for comments)\n",
    "\n",
    "```python\n",
    "    # Hack: Rename cols so that I can process `posts` as a batch of comments\n",
    "    bucket_name=bucket_name,\n",
    "    subreddits_path=None,\n",
    "    posts_path=None,  # posts_path\n",
    "    comments_path=comments_path,\n",
    "    \n",
    "    col_post_id=None,\n",
    "    col_comment_id='post_id',\n",
    "    col_text_comment='text',\n",
    "    col_text_comment_word_count='text_word_count',\n",
    "    cols_index_comment=['subreddit_name', 'subreddit_id', 'post_id'],\n",
    "    local_comms_subfolder_relative='df_vect_posts',\n",
    "    mlflow_comments_folder='df_vect_posts_extra_text',\n",
    "    cols_comment_text_to_concat=['flair_text', 'post_url_for_embeddings', 'text', 'ocr_inferred_text_agg_clean'],\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e208f8f",
   "metadata": {},
   "source": [
    "# Run full with lower_case=False (legacy fse/fasttext)\n",
    "\n",
    "Time on CPU, only comments + subs:\n",
    "```\n",
    "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
    "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
    "\n",
    "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3164d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:28:50 | INFO | \"Start vectorize function\"\n",
      "13:28:50 | INFO | \"  Local model saving directory: /home/jupyter/subreddit_clustering_i18n/data/models/use_multilingual/2021-07-01_1328\"\n",
      "13:28:50 | INFO | \"Load comments df...\"\n",
      "13:29:07 | INFO | \"  (1108757, 6) <- df_comments shape\"\n",
      "13:29:07 | INFO | \"Keep only comments that match posts IDs in df_posts...\"\n",
      "13:29:07 | INFO | \"df_posts missing, so we can't filter comments...\"\n",
      "13:29:07 | INFO | \"Load subreddits df...\"\n",
      "13:29:08 | INFO | \"  (629, 4) <- df_subs shape\"\n",
      "13:29:08 | INFO | \"MLflow tracking URI: sqlite:////home/jupyter/mlflow/mlruns.db\"\n",
      "13:29:09 | INFO | \"Loading model use_multilingual...\n",
      "  with kwargs: None\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 770 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc1c7200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:11 | INFO | \"  0:00:02.282361 <- Load TF HUB model time elapsed\"\n",
      "13:29:11 | WARNING | \"For TF-HUB models, the only preprocessing applied is lowercase()\"\n",
      "13:29:11 | INFO | \"Vectorizing subreddit descriptions...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 771 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7ecc27c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:13 | INFO | \"  Saving to local... df_vect_subreddits_description...\"\n",
      "13:29:13 | INFO | \"  Logging to mlflow...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 772 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7fb3f1dd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:29:14 | INFO | \"Vectorizing COMMENTS...\"\n",
      "13:29:14 | INFO | \"Getting embeddings in batches of size: 1500\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d7faaaa3c242e4bef7a38d489afafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:44:30 | INFO | \"  Saving to local... df_vect_comments...\"\n",
      "13:44:49 | INFO | \"  Logging to mlflow...\"\n",
      "13:45:11 | INFO | \"  0:16:21.475036 <- Total vectorize fxn time elapsed\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run(status='KILLED')\n",
    "\n",
    "model, df_vect, df_vect_comments, df_vect_subs = vectorize_text_to_embeddings(\n",
    "    model_name='use_multilingual',\n",
    "    run_name='full_data-lowercase_false',\n",
    "    mlflow_experiment=mlflow_experiment_full,\n",
    "    \n",
    "    tokenize_lowercase=False,\n",
    "    subreddits_path='subreddits/de/2021-06-16',\n",
    "    posts_path=None,  # 'posts/de/2021-06-16',\n",
    "    comments_path='comments/de/2021-06-16',\n",
    "    tf_batch_inference_rows=1500,\n",
    "    tf_limit_first_n_chars=1100,\n",
    "    n_sample_posts=None,\n",
    "    n_sample_comments=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
